---
title: Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max!  Use pandas.DataFrame.shift() more!
author: "Daniel Claborne"
date: "2022-11-28"
lastmod: "2022-12-04"
categories: [Data Science, Code]
bibliography: "../../bibs/ml.bib"
image: "/www/gumbel-matrix.png"
twitter-card:
    image: "/www/gumbel-matrix.png"
format: 
  html:
    fig-width: 8
    fig-height: 6
---

Honestly, what a disaster of a title.  I don't know if either part in isolation would be more likely to get someone to read this, but I just wanted to make a post.  Maybe I should have click-baited with something completely unrelated, oh well.

I'm currently taking Machine Learning for Trading from the Georgia Tech online CS masters program as part of my plan to motivate myself to self-study by paying them money.  While much of the ML parts of the class are review for me, it has been fun to learn things about trading, as well as do some numpy/pandas exercises.[$^{[1]}$](#ref1)  

The class is heavy on vectorizing your code so its fast (speed is money in trading), as well working with time series.  I'll go over two things I've found neat/useful.  One is vectorizing sampling from a categorical distribution where the rows are logits.  The other is using the .shift() method of pandas DataFrames and Series.

## Vectorized sampling from a categorical distribution

Okay, so the setup is you have an array that looks like this:

```{python}
import numpy as np
np.random.seed(23496)

# The ML people got to me and I now call stuff that sums to 1 'logits'
logits = np.random.uniform(size = (1000, 10))
logits = logits/logits.sum(axis = 1)[:,None]

logits[:5,:]
```

Each row can be seen as the bin probabilities of a categorical distribution.  Now suppose we want to sample from each of those distributions.  One way you might do it is by leveraging `apply_along_axis`:

```{python}
samples = np.apply_along_axis(
    lambda x: np.random.choice(range(len(x)), p=x), 
    1, 
    logits
)

samples[:10], samples.shape
```

Hm, okay this works, but it is basically running a for loop over the rows of the array.  Generally, `apply_along_axis` is not what you want to be doing if speed is a concern.

So how do we vectorize this?  The answer I provide here takes advantage of the [Gumbel-max trick](https://en.wikipedia.org/wiki/Gumbel_distribution#Gumbel_reparametrization_tricks) for sampling from a categorical distribution.  Essentially, given probabilities $\pi_i, i \in {0,1,...,K}, \sum_i \pi_i = 1$ if you add [Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution) noise to the log of the probabilites and then take the max, it is equivalent to sampling from a categorical distribution.

Again, take the log of the probabilities, add Gumbel noise, then take the arg-max of the result.

```{python}
samples = (
    np.log(logits) + \
    np.random.gumbel(size = logits.shape)
    ).argmax(axis = 1)  

samples[:10], samples.shape
```

Lets test if this is actually faster:

```{python}
%%timeit
(np.log(logits) + np.random.gumbel(size = logits.shape)).argmax(axis = 1)  
```

```{python}
%%timeit
np.apply_along_axis(lambda x: np.random.choice(range(len(x)), p=x), 1, logits)  
```

Yea, so a couple orders of magnitude faster with vectorization.  We should probably also check that it produces a similar distribution across many samples (and also put a plot in here to break up the wall of text).  I'll verify by doing barplots for the distribution of 1000 values drawn from 4 of the probability distributions.  Brb, going down the stackoverflow wormhole because no one knows how to make plots, no one.

...

Ok I'm back, here is a way to make grouped barplots with seaborn:


```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# do 1000 draws from all distributions using gumbel-max
gumbel_draws = []

for i in range(1000):
    samples = (
        np.log(logits) + np.random.gumbel(size = logits.shape)
    ).argmax(axis = 1) 

    gumbel_draws.append(samples) 

gumbel_arr = np.array(gumbel_draws)

# ...and 1000 using apply_along_axis + np.random.choice
apply_func_draws = []

for i in range(1000):
    samples = np.apply_along_axis(
        lambda x: np.random.choice(range(len(x)), p=x), 
        1, 
        logits
    )
    apply_func_draws.append(samples)

apply_func_arr = np.array(apply_func_draws)
```

In the above, if you ran the two for loops separately, you would get a better sense of how much faster the vectorized code is.  Now well munge these arrays into dataframes, pivot, and feed them to seaborn.

```{python}
#| label: dist-comparison
#| fig-cap: "The distribution of drawn values should be roughly the same."

gumbel_df = pd.DataFrame(gumbel_arr[:,:4])
apply_func_df = pd.DataFrame(apply_func_arr[:,:4])

gumbel_df = pd.melt(gumbel_df, var_name = "distribution")
apply_func_df = pd.melt(apply_func_df, var_name = "distribution")

fig, axs = plt.subplots(1, 2, figsize = (14, 8))

p = sns.countplot(data = gumbel_df, x="distribution", hue="value", ax = axs[0])
p.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')
axs[0].set_title("Using Gumbel-max")

p = sns.countplot(data = apply_func_df, x="distribution", hue="value", ax = axs[1])
p.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')
axs[1].set_title("Using apply_along_axis + np.random.choice")

fig.tight_layout()
```

Eyeballing these, they look similar enough that I feel confident I've not messed up somewhere.

Finally, of note is that a modification of this trick is used a lot in training deep learning models that want to sample from a categorical distribution (Wav2vec[@baevski_wav2vec_2020] and Dall-E[@ramesh_zero-shot_2021] use this).  I'll go over it in another post, but tl;dr, the network learns the probabilities and max is changed to softmax to allow backpropagation.

## pandas.DataFrame.shift()

You could probably just go read the docs on this function, but I'll try to explain *why* its useful.  We often had to compute lagged differences or ratios for trading data indexed by date.  To start I'll give some solutions that don't work or are bad for some reason, but might seem like reasonable starts.  Lets make our dataframe with a date index to play around with:

```{python}
import pandas as pd

mydf = pd.DataFrame(
    {"col1":np.random.uniform(size=100), 
    "col2":np.random.uniform(size=100)}, 
    index = pd.date_range(start = "11/29/2022", periods=100)
)

mydf.head()
```

Now, suppose we want to compute the lag 1 difference.  Specifically, make a new series $s$ where $s[t] = col1[t] - col2[t-1]: t > 0$, $s[0] =$ NaN. Naive first attempt:

```{python}
mydf["col1"][1:] - mydf["col2"][:-1]
```

Uh, so what happened here?  Well, pandas does subtraction by *index*, like a join, so we just subtracted the values at the same dates, but the first and last dates were missing from col1 and col2 respectively, so we get NaN at those dates.  Clearly this is not what we want.

Another option converts to numpy, this is essentially just a way to move to element-wise addition:

```{python}
lag1_arr = np.array(mydf["col1"][1:]) - np.array(mydf["col2"][:-1])
lag1_arr[:5], lag1_arr.shape
```

Of course, this is not the same length as the series, so we have to do some finagling to get it to look right.

```{python}
# prepend a NaN
lag1_arr = np.insert(lag1_arr, 0, np.nan)
lag1_arr[:5], lag1_arr.shape
```

Ok, its the same length and has the right values so we can put it back in the dataframe as a column or create a new series (and add the index again)

```{python}
# make a new series
lag1_series = pd.Series(lag1_arr, index=mydf.index)

# or make a new column
# mydf["col3"] = lag1_arr
```

Alright, but this looks kinda ugly, we can do the same thing much more cleanly with the `.shift()` method of pandas DataFrames and Series.  `.shift(N)` does what it sounds like, it shifts the values N places forward (or backward for negative values), but keeps the indices of the series/dataframe fixed. 

```{python}
mydf["col1"].shift(3)
```

With this we can easily compute the lag 1 difference, keeping the indices and such.

```{python}
# difference
mydf["col1"] - mydf["col2"].shift(1)

# lag 3 ratio
mydf["col1"]/mydf["col2"].shift(3)
```

This lag-N difference or ratio is extremely common and honestly I can't believe I hadn't been using `.shift()` more.

****

$^{[1]}$I am not affiliated with GA-Tech beyond the new washing machine and jacuzzi they gave me to advertise their OMSCS program<a name="ref1"></a>
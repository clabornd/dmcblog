[
  {
    "objectID": "posts/hello-word/index.html",
    "href": "posts/hello-word/index.html",
    "title": "The blog lives. Blog blog blog.",
    "section": "",
    "text": "A test of the new website, deployed on github pages. Errr, I guess this post can be about how I did it? Look, just follow this tutorial okay.\nHere’s some python code I guess?\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter([1,2], [1,2])\nfig.show()"
  },
  {
    "objectID": "posts/update-2024/index.html",
    "href": "posts/update-2024/index.html",
    "title": "Personal Updates, Flipping Coins (Bobgate)",
    "section": "",
    "text": "Havent posted in a while, this is gonna be mostly a personal post to describe what I’ve been up to and practice regularly updating this blog so it doesn’t go stale. If you shockingly don’t care about the details of some random internet person’s life and just wanna read about an interesting puzzle, skip to Bobgate.\n\nPhD Applications\nI applied to about a dozen computer science PhD programs for Fall 2024, focusing on programs with good artificial intelligence labs preferably doing RL research. I aimed high and was rejected across the board. I was somewhat expecting it, given the competitiveness of the programs I applied to and the current hype around AI. There is a lot of doomerism about CS PhD admissions on Reddit, with people saying there’s high schoolers with AI/ML publications in top conferences, so the competition is incredibly strong. No doubt some of this is just the bias of the most worried people posting on Reddit, but probably there is some truth to it. Of course I’m still a bit disappointed, but am trying to shift focus back to work and finishing my masters in OMSCS, which I’m now two classes away from completing. For anyone interested, my list of schools and general application profile were:\n\nSchools: UW, UMD, UMass Amherst, Northeaster, Northwester, Oregon State, Penn, Columbia, GaTech, Duke, Chicago, USC.\nEducation: Oregon State: Undergrad in Economics, 2.88 GPA, Masters in Statistics, 3.67 GPA, OMSCS currently 4.0 GPA.\nResearch: Some publications, a couple in ML/AI but not at top venues, one first author.\nExperience: 6 years as a data scientist at Pacific Northwest National Laboratory.\nRecommendations: 3 from colleagues with PhD’s who I’ve co-authored with.\n\n\n\nOMSCS Worries\nI took machine learning (CS7641) this Spring and tried to basically wing it with my current knowledge of ML. I wouldn’t recommend this as the class is quite challenging even if you’re familiar with the material. They expect you to really engage with the content and the requirements of the assignments are somewhat ambiguous. In typical fashion, I was lamenting that I might not get a B, that I would have to take it again (B required for ‘core’ classes, ML being one of them), and of course in the end did well on the final and got an A. Along with an A in the second class I took this term, my 4.0 at OMSCS lives on. This probably won’t be the last time I’m worried for nothing.\n\n\nBobgate\nThere was recently a question posed by @littmath on Twitter that stumped some people as far as explaining the answer. The best answer I found can be seen here: Reddit.\nThe problem, briefly: 100 coins are flipped. For every HH that appears, Alice gets a point. For every HT that appears, Bob gets a point. Who is more likely to win? That is, who is more likely to have more points at the end of the 100 flips? Doesn’t matter by how much they win, just who is more likely to win.\nThere are reasonable arguments to say Alice will win (HHH is 2 points!), or that it is a tie (something about equal probability), however the answer is Bob. The easiest way to see this is by simply simulating a bunch of flips and seeing who wins more often, or by enumerating all possible outcomes for a smaller number of flips and counting who wins more:\nlibrary(dplyr)\nlibrary(purrr)\n\n# all combos of 10 flips\nbinary_list = lapply(1:10, function(i) c(0,1))\nall_combos = do.call(expand.grid, binary_list)\n\nscore_flip &lt;- function(...) {\n  x = list(...)\n  score_bob = 0\n  score_alice = 0\n  for (i in 1:(length(x)-1)) {\n    if ((x[i] == 1) && (x[i+1] == 0)) {\n      score_bob = score_bob + 1\n    }\n    \n    if ((x[i] == 1) && (x[i+1] == 1)) {\n      score_alice = score_alice + 1\n    }\n  }\n  \n  return(list('alice' = score_alice, 'bob' = score_bob))\n}\n\nresult = all_combos |&gt;  mutate(scores = purrr::pmap(all_combos, .f = score_flip))\n\nscore_alice = purrr::map(result$scores, ~.x$alice) |&gt; unlist()\nscore_bob = purrr::map(result$scores, ~.x$bob) |&gt; unlist()\n\nprint(c(mean(score_alice &gt; score_bob), mean(score_alice &lt; score_bob)))\n[1] 0.3623047 0.4531250\nWe see that Bob is winning more often in this example of 10 flips. The question asked for 100 flips though, which does matter, and indeed simulating it shows that Alice catches up a bit but loses more often. The intuition in the reddit post is that scoring happens only when there is one or more heads. This could be a single H, or a sequence of H’s, both terminated by a T or reaching the 100th flip. We can ask, for any such sequence, what is the expected points for Bob or Alice? Lets consider Bob first. They are almost always going to get 1 point, only in the case where there are H’s until the 100th flip will there not be 1 point which is usually very unlikely, so lets approximate the expected points for Bob as 1 for now.\nNow for Alice. We consider that HT makes up half of all sequences of H’s: Half the time, a T is flipped after the first head, ending the sequence with zero points. Then we consider that HHT happens 1/4 of the time with one point, HHHT 1/8 of the time with 2 points, etc. Assuming there are 99 possible flips to go after the initial H, we can write out this sequence of expected points for Alice as:\n\\[\n\\sum_{i=1}^{100} \\frac{(i-1)}{2^i} \\lt \\sum_{i=1}^{\\infty} \\frac{(i-1)}{2^i}\n\\]\nYes the first term is zero but I’m starting there to simplify things later. Eh…okay I’m going to rewrite this and then use the power of stackexchange! to finally get the answer.\n\\[\\begin{align*}\n\\sum_{i=1}^{\\infty} \\frac{(i-1)}{2^i} &= \\sum_{i=1}^{\\infty} \\frac{i}{2^i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^i} \\\\\n&= \\sum_{i=1}^{\\infty} \\frac{i}{2^i} - 1 \\\\\n\\end{align*}\\]\nWhere the last step uses the definition of a geometric series. Now we have to solve for the final infinite summation. The solution is here, which I’ll repeat here in case my blog outlives stackexchange (likely). We start by stating the partial sum of the series (with a general \\(r\\) instead of our \\(r=\\frac{1}{2}\\)) as:\n\\[\nS_m = \\sum_{i=1}^{m} i r^i\n\\]\nNow notice the following:\n\\[\\begin{align*}\nS_m - r S_m &= \\sum_{i=1}^{m} r^i - mr^{m+1} \\\\\n&= \\frac{r - r^{m+1}}{1 - r} - mr^{m+1} \\\\\n&= \\frac{r - (m+1)r^{m+1} + mr^{m+2}}{1 - r}\n\\end{align*}\\]\nAnd since \\(S_m - r S_m = (1 - r) S_m\\), we have:\n\\[\nS_m = \\frac{r - (m+1)r^{m+1} + mr^{m+2}}{(1 - r)^2}\n\\]\nWith our \\(r=\\frac{1}{2}\\), this sucker is going to 2 as \\(m \\rightarrow \\infty\\) and is basically already there at \\(m=100\\). So the expected points for Alice is 1, but this is only true in the limit. If we begin our sequence with, say, only 5 flips left, then we actually have:\n\\[\nS_5 - \\sum_{i=1}^{5} \\frac{1}{2^i} = 1.78125 - 0.96875 = 0.8125\n\\]\nversus Bob who is \\(1-\\frac{1}{2^5} = 0.96875\\), a serious edge. This appears to be true for all values of \\(m\\), so that Bob’s expected score for a series of H’s is always greater. I’m not sure if there is a straightforward way to compute the expected score over the entire sequence of flips without enumerating each outcome as I did previously.\n\n\nComing Soon!\nI’ve been trying to train an agent to play fighting games with diambra. It has been a bit tricky since I’m trying to run it in Colab so I can share it, but that requires spinning up docker on another system (EC2) and pinging them from the Colab notebook. I’ve got this mostly sorted, so hopefully I can show something soon."
  },
  {
    "objectID": "posts/vqvae/vqvae.html",
    "href": "posts/vqvae/vqvae.html",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "",
    "text": "I’ve written before about both the gumbel-max trick and variational autoencoders. The world demanded a post that combined the two, so here it is. I mostly follow the repo for the DALL-E paper Ramesh et al. (2021). They also use the attrs package, which was nice to learn about, kinda neat, but a bit opaque.\nAs usual, I hope to entertain you with some of my mistakes and general troubles. If you are a sick, sick individual and came here to hear about taking the log of 0, I will happily indulge you. Here’s a Colab notebook if you’re on a similar learning journey and just need to see some code. Mega-disclaimer that I am just some dude, use at your own risk."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#categorical-vaes",
    "href": "posts/vqvae/vqvae.html#categorical-vaes",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "Categorical VAE’s",
    "text": "Categorical VAE’s\nThe general idea of categorical VAE’s is that our encoder learns the probabilities of a categorical distribution with \\(K\\) categories. These probabilities are used to sample from one of \\(K\\) vectors in a codebook (a collection of vectors). These sampled codebook vectors are then fed into a decoder to try to reconstruct the input.\nOptimization is done in the usual way, by maximizing the evidence lower bound (ELBO, see my VAE post or other resources for details): \\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\n\\end{align}\n\\]\nThe first term on the RHS (the reconstruction objective) is usually defined as a Gaussian or Laplace distribution, and can be maximized with the \\(l2\\) or \\(l1\\) loss. In the paper, they use a different distribution called the logit-laplace distribution, with the reasoning that it models the distribution of pixel values better, specifically that they lie in a bounded range. The logit-laplace distribution is defined as:\n\\[\nf(x, \\mu, b) = \\frac{1}{2bx(1-x)}\\exp\\left(-\\frac{|logit(x) - \\mu|}{b}\\right)\n\\]\nThey use the log of the RHS as the reconstruction objective, with the decoder outputting 6 channels per pixel location (3 \\(\\mu\\)’s and 3 \\(b\\)’s for each pixel). I couldn’t tell you why this is better than assuming the inputs (between 0 and 1) are the probabilites of Bernoulli distributions, perhaps it is more flexible? But their concern of a bounded range is just as well taken care of by outputting values in the range \\([0, 1]\\) and using cross entopy with logits against the input image. In fact, I do this against MNIST (yes, its MNIST again, gimme a break ok).\nThe second (KL) term of the ELBO in the case of categorical VAE’s is comparing our encoder’s outputted distribution with a uniform categorical distribution over the \\(K\\) classes, which is easily calculated as:\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = \\sum_{k=1}^K q(k\\vert x;\\phi) \\log\\left(\\frac{q(k\\vert x;\\phi)}{1/K}\\right)\n\\]\nsince we assume \\(p(z) = 1/K\\) for all \\(z\\)."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#the-gumbel-softmax-trick",
    "href": "posts/vqvae/vqvae.html#the-gumbel-softmax-trick",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "The Gumbel-(soft)Max Trick",
    "text": "The Gumbel-(soft)Max Trick\nOk, if you remember in VAE’s we have to deal with this whole non-differentiable thing, since our process goes:\n\nEncode our input to the probabilities of a categorical distribution\nSample from that distribution and use the sample to select a vector from a codebook.\nDecode the sample to try and match the input\n\nand we cant backpropagate through 2. This is handled by using a relaxation of the Gumbel-max trick which if you’ll recall, is a way to sample from a categorical distribution by taking the arg-max of the log of the probabilities plus noise from a Gumbel(0,1) distribution. Arg-max isn’t differentiable, so we use softmax (which is more accurately described as soft-arg-max) as a differentiable approximation. We can adjust the temperature of the softmax operation to more closely approximate the arg-max operation.1\nprobs = self.encoder(x) # B x K x H x W\ngumbel_noise = gumbel_sample(probs.shape).to(probs.device)\n\n# apply softmax to log(probs) + gumbel noise divided by temperature tau.\n# very small tau essentialy makes this one-hot\nz = torch.nn.functional.softmax((probs.log() + gumbel_noise)/tau, dim=1) \n\n# 'soft-samples' from the vector quantized embeddings\nz = torch.einsum(\"bchw,cv -&gt; bvhw\", z, self.embedding)\n\n# reconstruct to B x C x H x W\nx_reconstr = self.decoder(z)\nNotice that the probabilities are a B x vocab_size x H x W feature map. So we are sampling a vector from the codebook at each location in this feature map, or rather we are approximating sampling from it by taking a weighted (where the weights sum to one) combination of the codebook vectors, where one weight is very large is the rest are very small (due to the softmax operation). Once we have the reconstructed image and the probabilities of the categorical distribution, we can calculate the reconstruction loss and the KL divergence. The reconstruction loss is just:\nreconstr_loss = nn.functional.binary_cross_entropy_with_logits(xrecon, x, reduction=\"mean\")\nOr it would be the logit-laplace loss if I had implemented that:\ndef laplace_loss(x, mu, b):    \n    loss = -torch.log(2*b*x*(1-x)) - torch.abs(torch.logit(x) - mu)/b\n\n    return loss\nThe KL divergence is as defined above, and can be calculated like so:\ndef KL_loss(logits, vocab_size=4096):\n  loss = logits * (logits.log() - torch.log(torch.ones_like(logits)/vocab_size))\n\n  # B x C x H x W\n  return loss\n…well, really I should be summing over the channel axis (KL definition has a sum), I do it in the notebook outside the function. As we’ll see later though, whether you sum or average is kinda arbitrary, because it seems empirical results suggests multiplying the KL loss by some constant is a good idea.\n\nMistakes were made\n1) My first attempt was on the STL-10 dataset, which I leave code for in the notebook, but I couldn’t get the model to output good reconstructions. An example of one of the reconstructions:\n\n\n\n\n\n\nFigure 1: Left: Hi I’m a bird. Right: alks3lkj3olkdfffff\n\n\n\n2) In the paper, they describe dividing the pixel values by 255, when performing preprocessing steps necessary to prevent divide-by-zeros in the logit-Laplace loss, which I gladly replicated without checking if my images were already 0-1 normalized. Funilly enough, it seemed to learn better than the non-normalized version, perhaps this is a clue to why I can’t get good quality samples from STL-10.\n3) I had switched to cross-entropy loss, and was doing okay, until I got divide-by-zero errors. You see, I am calling softmax twice, once to do the Gumbel-softmax sampling, but also once before, in the encoder, to form the probabilities that go into the gumbel-softmax sampling. Remember we take the log of those values and then add Gumbel noise, so they all better be positive. Well, you might think that the softmax equation:\n\\[\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n\\]\nwould always produce positive values, but there’s this thing called numerical underflow which is a real pain in the ass, the ass of Daniel Claborne trying to take the logarithm of things, that is. Well, I just add a small constant and divide by the appropriate constant to make them still sum to 1 along the channel axis:\n# forward method of the encoder\ndef forward(self, x):\n    x = self.blocks(x)\n    x = torch.nn.functional.softmax(x, dim = 1)\n\n    x = (x + 1e-6)/(1 + self.vocab_size*1e-6)\n\n    return x\n4) I underestimated how much I would need to increase the importance of the KL term. In the paper, they multiply the KL loss term by \\(\\beta\\) which is increased from 0 to 6.6 over some number of iteration. My initial attempts that stopped at \\(\\beta = 2\\) produced poor sample quality, so I just tried their suggested \\(\\beta\\)\n\n\nSome success with MNIST\nOk, I was defeated by STL-10 (I’ll try again once Colab gives me some more compute credits), but good ol easy-mode MNIST gave me some nice results. I load my best test loss model from https://wandb.ai/clabornd/VQVAE/runs/9bb6w3ru?workspace=user-clabornd and use it to generate some samples.s\nFirst, we see if it can reconstruct an image that is passed to the encoder. Intuitively the encoder should map the image \\(X\\) to a latent representation \\(z\\) that is likely to produce something similar to \\(X\\) in the output, and so it does as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Left: Input. Right: decoded output\n\n\n\nBut remember we not only wanted to be able to produce an image when providing an image, but be able to produce images when sampling from random noise. This is the point of the KL loss term, making the latent representation close to a uniform categorical distribution. We should then be able to sample from a uniform categorical (for each pixel location in a latent feature map), pass this sample to our decoder, and get things that look like our training data. And so we do (sorta):\n\n\n\n\n\n\nFigure 3: Output of the decoder when fed feature maps of codebook vectors sampled from a uniform categorical distribution\n\n\n\nWow, what a bunch o beauts’. Hopefully I can get you some pictures of slightly less blurry birds/airplanes soon."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#footnotes",
    "href": "posts/vqvae/vqvae.html#footnotes",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the paper they start with temperature \\(tau = 1\\) and reduce it to \\(\\frac{1}{16}\\) over some number of iterations - I do as well in the notebook.↩︎"
  },
  {
    "objectID": "posts/minigpt/minigpt.html",
    "href": "posts/minigpt/minigpt.html",
    "title": "I Follow a GPT Tutorial",
    "section": "",
    "text": "ChatGPT is cool and popular. GPT-3 is the starting point for ChatGPT. Lets write something about me training a little version of GPT-3.\nGPT-3 (Brown et al. 2020) has been popular for a while now, and has strangely become it’s own nested version of deep learning as a whole: Everyone was applying deep learning to everything, now everyone is applying GPT-3 to everything, and —related— everyone is applying transformers to everything (vision, audio, point clouds….bleh). I’m reminded of this ‘The Onion’ short where the Large Hadron Collider scientists get bored and are throwing random things into it: ‘…last week we threw a bird in there…’. 10% of machine learning research now seems to be “what if we trained a transformer like this?!”.\nStrangely, GPT-3’s architecture is not all that complicated, or rather not much more than the original transformer architecture, and reading the paper and having it explained (“its just the decoder part…”) made me think that there must be more to it. Well, of course there is y’know all that engineering to train on 1000+ GPUs and such, but anyhow…\nI wanted to actually run the code to see if there was an ‘ah, its a bit more complicated’ moment. I stumbled across Andrej Karpathy’s tutorial video about training a mini version of GPT-3 so I decided to watch and see if I could replicate the Shakespeare-sounding gobbledeygook coming out of the final model, and get some more practice with weights-and-biases along the way.\nOk, so obviously you should probably just go watch his video?…but…uh…I dunno maybe stick around for my sharp wit? I didn’t follow his build-the-transformer-from-scratch bits, which are good, but I’ve gone through all that already. Really I just wanted to do the things that make GPT-3 what it is:\nThe above three things plus scale are indeed almost all of what makes GPT-3 special. Here’s a link to the colab notebook for training this thing up. For fun I did some hyperparameter tuning as well.\nOk so I’ll briefly cover 1 - 3, but just go watch the video, it’s great.\n(Aside: Andrej Karpathy’s video also convinced me to get Github Copilot, it auto suggested ‘just go watch the video, it’s great’ at the end of the last sentence. I’m not sure if I should be impressed or scared. I’m going to go with impressed. &lt;- These two sentences also generated by copilot. This entire blog is written by copilot. Haha, just kidding, or am I? What is real? This is a blog post about GPT model partially written by a GPT-based model aaaaaaaaaaaaaaaaaaaaaaa!)\n1. Basically, specify some maximum context length \\(N_{max}\\), grab \\(n + 1: n &lt; N_{max}\\) ‘tokens’ (words, characters, subword tokens, etc), and make your input: tokens 1 to \\(n\\), and your targets tokens 2 to \\(n + 1\\). Something like this:\n2. For masked self attention, we just want to pass an attention mask that makes it so every token can only attend to itself and tokens before it. The format is a bit strange, but it’s just a boolean tensor where the upper triangle is True and the lower triangle including diagonal is False (See the docs for further options). Here’s a quick way to make one:\n3. For generating text, we just need to start with some prompt, feed it in and get a prediction of the next token for every input token, then feed in the prompt + the predicted token, and get the next prediction, and so on."
  },
  {
    "objectID": "posts/minigpt/minigpt.html#hyperparameter-tuning",
    "href": "posts/minigpt/minigpt.html#hyperparameter-tuning",
    "title": "I Follow a GPT Tutorial",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nI sweep over hidden embedding dimension, learning rate, and batch size. The results of one of the sweeps can be seen here. There doesn’t seem to be much of a correlation between the hyperparameters and the validation loss. If I was doing another sweep I’d probably try varying dropout or max sequence length to see if I could recreate the validation loss.\n\n\n\n\n\n\nFigure 1: Hyperparameter Combinations and Corresponding Validation Loss\n\n\n\n\nGenerated Text\nI use the model from the run Radiant Rabbit to generate some text. The text is generated as described in 3 above, with a prompt of “LEONTES:”, a max length of 1000, and a block size of 128. Notice below I am passing an increasingly large mask and feeding a maximum of 128 tokens at a time. The model has not seen sequences longer than 128 tokens, and would begin to produce nonsense (or rather, even more nonsensical nonsense) beyond that length.\nidx = torch.tensor(encode(\"LEONTES:\\n\")).unsqueeze(0).to(device)\nblock_size = 128\n\nfor i in range(1000):\n    attn_mask = torch.triu(torch.ones(idx.size(1), idx.size(1)), diagonal=1).bool()\n    attn_mask = attn_mask[:block_size, :block_size].to(device)\n\n    logits = m(idx[:, -block_size:], attn_mask = attn_mask)\n    logits = logits[:, -1, :]\n    probs = F.softmax(logits, dim=-1)\n    idx_next = torch.multinomial(probs, 1)\n\n    idx = torch.cat([idx, idx_next], dim=1)\nI promised some funny text, here’s a snippet of the decoded output:\nLEONTES:\nTrue!\nO worse of this men it.\n\nDUKE VINCENTIO:\nGentle king him!\n\nProvost:\nNo more of ourself in a say 'nointed,' silent visit,\nIn carrion choose with ever of person;\nHe hath had made her heads, that nature's away:\nTherefore, had I in my vain sweet prevent\nTo see her friends did close his minds.\n\nProvost:\nO though this, if I do wish a sword knowledge\nIn wanton common can my blate to some have\nbased with said, but that it bloody billows,\nRatter lieuted with a moler'd and enmity\nMay have utter'd my heart\nBy from the testy-moning hour, whom\nMore comes not thus? beitterly od married.\n\nMAMILLIUS:\nLear out?\n\nLEONTES:\nNay, God help.\nServing Servant:\nHe's sometime now:\nThe judge hate I, being in become moads,\n'gainst enjoying warps it, and venture,\nThese stocks, tears; and it may be feen gone,\nbut the master of his old return in\nAnd bear me those his grace, that knows be call'd\nThe queen amorous of burthen and walking at him.\nDear most respect this prince hour? If then\npenter this member what h\nDANIEL:\nDaniel out?"
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html",
    "href": "posts/mlflow-hydra/mlflow-hydra.html",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "",
    "text": "PROGRAMMING NOTE: My post on running diambra reinforcement learning training from Colab is still coming, but might be posted elsewhere.\nUPDATE: This post discusses running the storage (postgres and minio) in docker containers and the mlflow UI from the command line. The post references a github repo at this commit. I’ve since edited the associated github repo to run the mlflow UI in a docker container as well from docker-compose, inspired by this tds post. The below should still work, but take a look at that post and the repo for the full docker setup."
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html#extra-options-with-hydra",
    "href": "posts/mlflow-hydra/mlflow-hydra.html#extra-options-with-hydra",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "Extra options with hydra",
    "text": "Extra options with hydra\nHydra allows us to manage configuration options in a more organized way. In this simple example, I have under cfg a config.yaml file that specifies the parameters for the training script. One is the experiment name to be displayed under the mlflow UI, and the other is the config group ‘model’, which allows me to specify what kind of model I want to train. The parameters of these models are stored in further yaml files under cfg/model. They contain the actual sklearn class or ‘target’ that I want to use to construct the model object as well as the parameters to pass to the constructor. For example, cfg/model/randomforest.yaml looks like this:\n_target_: sklearn.ensemble.RandomForestClassifier\nn_estimators: 100\nmax_depth: 10\nrandom_state: 42\nUnder the ‘top level’ config.yaml file, this is the default:\ndefaults:\n  - model: randomforest\n\nexperiment_name: mlflow-practice\nNote that I do not include the file type in the model parameter. See the hydra docs for more about the defaults list.\nSuppose I wanted to train an SVM instead of a random forest, I have another config for SVM in cfg/model/svm.yaml:\n_target_: sklearn.svm.SVC\nC: 1.0\ngamma: scale\nkernel: rbf\nI can specify the model to use with the model parameter in the command line:\npython src/basic-example.py model=svm\nOr change the value of one of the model parameters:\npython src/basic-example.py model=svm model.C=0.1\n\n# random forest\n\npython src/basic-example.py model=randomforest model.n_estimators=50\nOr even add a parameter that is not in the default config:\n# add the degree parameter to the svm model\npython src/basic-example.py model=svm model.C=0.1 model.kernel=poly +model.degree=4"
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html#hyperparameter-sweeps",
    "href": "posts/mlflow-hydra/mlflow-hydra.html#hyperparameter-sweeps",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "Hyperparameter sweeps",
    "text": "Hyperparameter sweeps\nYes yes, this is an sklearn example and you should just be using GridSearchCV or RandomizedSearchCV (in fact, with mlflow.autolog() mlflow will automatically log child runs of the main run) but I wanted to show how you can use hydra to perform multiple runs.\nFirst the easy way with the -m flag. Here we sweep over 3 values of the C parameter for the SVM model:\npython src/basic-example.py -m model=svm model.C=0.1,1,10 model.kernel=rbf\nYou can also specify a sweep config under the main config.yaml file. For example, I can specify a sweep over the C parameter for the SVM model like this in the top level config:\ndefaults:\n  - model: randomforest\n\nexperiment_name: mlflow-practice\n\nhydra:\n  sweeper:\n    params:\n      model.C: 0.1,1,10\nAnd then run the script without the -m flag:\npython src/basic-example.py -m model=svm\nThis of course hard-cores the sweep configuration, we might like to have modular sweep configs. We can put further yaml files under cfg/sweep that specify the parameters to sweep over. For example, cfg/sweep/svm.yaml:\n# @package _global_\nhydra:\n  sweeper:\n    params:\n      model: svm\n      model.C: 0.1,1,11\nThe hydra/sweeper/params group is a special hydra group for performing sweeps. Here we specify the @package directive to notify hydra that we’re editing the config at the top level. We can then run the script with the sweep config like this:\npython src/basic-example.py -m +sweep=svm\nIf you go to the mlflow or minio UI after all this you should see all these runs populated, and sortable by parameters and performance metrics:"
  },
  {
    "objectID": "posts/variational-autoencoder/index.html",
    "href": "posts/variational-autoencoder/index.html",
    "title": "Yet Another Explainer on Variational Autoencoders",
    "section": "",
    "text": "For a while, I’ve been struggling to understand variational autoencoders (VAE’s) at a satisfactory level. An initial pass produced a bit of understanding, but I got sucked back in when I tried to understand Dalle-E, which led me to try to understand diffusion models, which directed me back to going over the techniques used in variational autoencoders again. Some day I will write a Johnny-come-lately post about the different components of Dall-E, but today is about VAE’s.\nBefore I jump into things, my default disclaimer that I am probably not someone who should be writing authoritative-sounding articles about VAE’s - this is an exercise in understanding through explanation. Also, I think its just generally useful to have as many angles at explaining something available as possible – the sources I used to understand VAE’s had great diversity in the ways they explained recurring topics and how much to delve into certain pieces of the puzzle. Here are some resources that I have found useful and are probably more trustworthy (maybe mine is more humorous?):\n\nPaper by Kingma and Welling\\(^{[1]}\\)\nThis tutorial by some people at UC-Berkeley and Carnegie Mellon\\(^{[2]}\\)\nThis blog post\\(^{[3]}\\)\nThis other blog post\\(^{[4]}\\)\n\nMuch of the material here is going to be a re-hash of what has been presented in these. If you notice something wrong, please submit a pull request or open an issue in the git repo for this blog to correct my mistake.\n\n\nSetup\nAlright, so variational autoencoders are cool, they can produce synthetic samples (images) that are realistic and serve as great material for articles and Reddit posts about how ‘none of these faces are real!’ and how we will soon all live in an identity-less, AI-controlled dystopian nightmare.\n\n\nThis person doesn’t exist\n\nHow do VAE’s help us get there? One thing that [1] and [2] do is to initially put any sort of deep learning architectures aside and just focus on the general variational inference problem. At first I found this very annoying (get to the point!), but now think it is probably useful (yea yea, the people who are way smarter than me were right and I was wrong, who would have thought).\nThe setup is that we have some real observations, \\(X_{obs} = \\{x^{(1)}, x^{(2)}, ... x^{(N)}\\}\\) (When I talk about some arbitrary sample from the observed data, i’ll drop the superscript and just say \\(x\\)) that we assume are generated from some process that goes like:\n\nA random variable \\(z \\in \\mathcal{Z}\\) is drawn from some distribution \\(p(z\\vert\\psi)\\) with parameters \\(\\psi\\).\n\\(x^{(i)} \\in \\mathcal{X}\\) are generated through a conditional distribution \\(p(x\\vert z;\\theta)\\) with parameters \\(\\theta\\)\n\nThe \\(x^{(i)}\\) could be anything from single values to something very high dimensional like an image. We seek to maximize the likelihood of our data under the entire generative process:\n\\[p(x) = \\int p(x\\vert z; \\theta)p(z\\vert\\psi)dz\\]\nHowever there are some issues, the true values of \\(\\theta\\) and \\(\\psi\\) are not necessarily known to us. We also cannot assume that the posterior \\(p(z\\vert x;\\phi)\\) is tractable, which we will see is important later. Here we can note that we have the two pieces that correspond to the ‘encoder’ and ‘decoder’ pieces of the problem. \\(p(x\\vert z;\\theta)\\) is like a decoder, taking the hidden \\(z\\) and turning it into the observed \\(x\\). \\(p(z\\vert x;\\phi)\\) is like a decoder, taking some input \\(x\\) and produces a hidden representation \\(z\\) that was likely given that we observed that \\(x\\). A graph of the whole process including the ‘encoder’ and ‘decoder’ pieces in shown below.\n\n\n\n“Plate diagram” with solid arrows representing \\(p(x\\vert z; \\theta)\\) and dashed arrows representing \\(p(z\\vert x;\\phi)\\). From Kingma and Welling (2014)\n\n\nOk now I’ll try to make the jump to neural networks (you can stop scrolling). This was one of the hardest parts of understanding vae’s for me – it was hard to get used to the idea of networks that produced distributions. The notation of using \\(p(...)\\) to refer to both a distribution under the variational Bayes framework and a neural network that parametrizes that distribution can be a bit confusing, but try to get comfortable switching between the two views. The important part is that \\(p(x\\vert z;\\theta)\\) can be approximated by a neural network. We’ll see soon that \\(p(z\\vert x;\\phi)\\) can be as well, and I’ll explain why we need it to be.\n\nLets start with \\(p(x\\vert z;\\theta)\\). First some assumptions are made:\n\n\\(p(x\\vert z;\\theta)\\) comes from a distribution such that it can be approximated/parametrized by a differentiable function \\(f(z; \\theta)\\)\n\\(z\\) is drawn from some probability distribution, often an isotropic Gaussian \\(p(z) = N(0, I)\\)\n\nThe first assumption is simply so we can perform gradient descent given a sampled \\(z\\) and optimize the likelihood of \\(x\\) given that \\(z\\). Here is where we have our neural network that produces a distribution. One common way is for \\(f(z; \\theta)\\) to take the form of encoding the mean of an isotropic normal: \\[p(x\\vert z; \\theta) = \\mathcal{N}(X\\vert f(z;\\theta), \\sigma^2_x*I)\\] Ok, I usually have to stop here…how does this help us? Well, we now have a network that will output a distribution from which we can calculate a likelihood for any given \\(x\\), and it is differentiable, such that we can edit the parameters \\(\\theta\\) through gradient descent to maximize the likelihood of all \\(x \\in X_{obs}\\). Having this ‘decoder’ network represent a distribution is also necessary when fitting it into the objective function later. From now on when I refer to \\(p(x\\vert z; \\theta)\\), I’ll be simultaneously talking about the distribution, and the network that produces that distribution.\nI think it is worthwhile to consider what maximizing this likelihood looks like in a real example. Suppose the \\(x^{(i)}\\) are greyscale images, what should \\(f(z; \\theta)\\) output to maximize the likelihood of a given \\(x\\)? Intuitively (and mathematically) it should output a mean vector with each element corresponding to a pixel in \\(x\\) and having the same value as that pixel – that is, the multivariate Gaussian it outputs should be directly centered over the multivariate representation of the image. One can also consider other forms of \\(x\\) and output distributions that make sense, such as if \\(x\\) is from a multivariate bernoulli, and \\(f(z; \\theta)\\) would then output the probability vector \\(p\\), maximizing the likelihood by having elements of \\(p\\) as close to 1 as possible for corresponding 1’s in \\(x\\) (and close to zero for zeros).\nThe second assumption is a bit weirder, and took me a while to get comfortable with. Essentially it is very dubious to try to handcraft a distribution for \\(z\\) that represents some informative latent representation of the data. Better to let \\(f(z; \\theta)\\) sort out the random noise and construct informative features through gradient descent. Later, when we get to the encoder/objective, we’ll also see that having \\(z\\) be \\(N(0, I)\\) is convenient when computing a component of the objective.\n\nAt this point, we could go ahead and try to train our decoder to maximize the likelihood of our observed data. We can estimate \\(p(x)\\) by drawing a whole bunch of \\(z\\) and then computing \\(\\frac{1}{N}\\sum_{i=1}^{N} p(x\\vert z_i;\\theta)\\) for each \\(x^{(i)}\\) and maximizing \\(log(p(X_{obs})) = \\sum_i log(p(x^{(i)}))\\) through gradient descent. However as mentioned in [2] we may need an unreasonable number of samples from \\(z\\) to get a good estimate of any \\(p(x)\\). This seems to be a problem with the complexity of the observed data, and how reasonable it is that \\(x\\) arises from \\(N(0, I)\\) random noise being passed through a function approximator:\n\nUs: Hey! we need you to turn this random noise in \\(\\mathbb{R}^2\\) into more of like….a diagonal line.\nModel: Yea sure I can kinda learn to move stuff in the second and fourth quadrants to the first and third quadrants.\n…\nUs: Hi, good job on the diagonal thing, now we need you to make some pictures of numbers from the noise.\nModel: What, like….images? Of digits? Christ man thats really high dimensional, how do I even…I mean i’ll give it a try but this is sort of a stretch.\nUs: Go get em!\n…\nUs: Hi champ, back again. We need you to recreate the diversity of pictures of human faces from the same random noi…\nModel: *explodes*\n\nAnother intuition I have for why this does not work, is that how does \\(f(z; \\theta)\\) decide to what distributions to map certain regions of the latent space \\(\\mathcal{Z}\\)? In the example of generating images of digits, it has to balance maximizing the probability of all digits. However if there is a group of digits (say, all the twos) that are very different from the rest of the digits than the rest of the digits are from each other, then maximizing \\(p(X_{obs})\\) might involve simply not mapping any region of \\(\\mathcal{Z}\\) to a distribution that odd digit is likely under – not a model we want to end up with. It helps then to define, for a given \\(x\\), regions of \\(\\mathcal{Z}\\) that were likely given we observed that \\(x\\).\n\n\nMaking things easier with an encoder\nI mentioned we would need to consider the posterior \\(p(z\\vert x;\\phi)\\) – it directly addresses the problem I just brought up: that we need to know what \\(z\\) values are likely given we observed a particular \\(x\\). We now develop our ‘encoder’, which again will map \\(x\\) to distributions over \\(\\mathcal{Z}\\).\nOne reasonable question is why do we want to map to a distribution? Why don’t we just map to specific points in \\(\\mathcal{Z}\\) that are most likely to produce \\(x\\). Well, remember that we want to be able to generate examples by sampling from \\(\\mathcal{Z}\\), and it is highly unlikely that a regular old encoder will map \\(x\\)’s to \\(z\\)’s in such a way that is ‘smooth’ or ‘regular’ enough such that we can do so.\nConsider some \\(z^{(i)}\\) that is likely to have produced a \\(4\\). The encoder is under no constraint to make some other \\(z^{(j)}\\) that is very close to \\(z^{(i)}\\) also likely to produce a \\(4\\). Similarly, given two regions of \\(\\mathcal{Z}\\) that produce, say, \\(1\\)’s and \\(7\\)’s, as we move from one region to the other, the model is under no obligation to smoothly change from decoding to \\(1\\)’s to decoding to \\(7\\)’s. [4] explains this concept very well. Another way to think about it is that this does not really match our initial definition of the generative process of \\(z\\)’s randomly occurring and producing \\(x\\)’s from some conditional distribution \\(p(x\\vert z)\\). We could make it fit, but \\(p(z)\\) would be some very wacky distribution with completely unknown form. So, it makes sense to encode the \\(x\\) to well behaved distributions over \\(z\\) as well as consider penalizing the encoder for producing sets of these conditional distributions \\(p(z\\vert x)\\) which are far apart, so that we can sample the latent space and reasonably get a high-quality example of an \\(x\\). We’ll see how this is done in the next section.\nTo wrap up this section, \\(p(z\\vert x)\\) will be approximated in a very similar way as \\(p(x\\vert z;\\theta)\\) – it’s parameters are determined by a neural network that takes in \\(x\\) and outputs the parameters of a distribution, in this case, the mean and variance vectors of an isotropic normal distribution:\n\\[q(z\\vert x; \\phi) = \\mathcal{N}(Z \\vert \\mu(x;\\phi), \\mathbf{\\Sigma}(x;\\phi))\\]\nWhere \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) are the mean vector and covariance matrix for the isotropic normal distribution output by our encoder network with parameters \\(\\phi\\) when fed \\(x\\) as input (in practice the network just outputs the diagonal elements for \\(\\mathbf{\\Sigma}(x;\\phi))\\), since the off-diagonals are forced to be zero). When referring to \\(q(z\\vert x; \\phi)\\) I’ll be talking both about the distribution induced by the network that outputs \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) and the network itself.\n\n\nThe objective\nSo, we have an encoder \\(q(z\\vert x; \\phi)\\) and decoder \\(p(x\\vert z;\\theta)\\), now all that is left is to train them. So we have to find an objective that incorporates them, as well as satisfies our goal of maximizing \\(p(X_{obs})\\) under a generative process with a \\(z\\) we can reasonably sample from and produce realistic examples.\nThere is a lot of variation (man…I think I missed the pun on this one earlier) in tutorials about how they arrive at the loss function. I’m working backwards from the definition in Kingma and Welling (2014) for the reason that the starting point is well motivated: We want to maximize \\(p(x)\\) under the generative process; can we find a differentiable form related to \\(p(x)\\) that includes our encoder and decoder structures?\nAnother common approach starts with the motivation of trying to optimize \\(q(z\\vert x; \\phi)\\) to match the intractable posterior \\(p(z\\vert x)\\), but I found it hard to make the logical leaps as to how someone could reasonably start here. That way is maybe algebraically easier to get to what we want, but I like how starting with \\(p(x)\\) and rewriting it feels more intuitively motivated.\nThe way I’ll get our encoder and decoder into the definition of \\(p(x)\\) is by doing a bit of the ol add zero trick:\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(z \\vert x))] + log(p(x))\\]\nYes, I am just taking the expectation of zero in there. I’m taking the log of \\(p(x)\\) because we’ll need log-everything on the right side and maximizing \\(log(p(x))\\) will also maximize \\(p(x)\\). As a bonus, we have the form of our encoder in the equation, great! (GREAT!) Notice that the expectation is over \\(z\\)’s drawn from \\(q(z\\vert x;\\phi)\\), that is, to approximate this expectation we would sample from \\(q(z\\vert x;\\phi)\\) by providing an \\(x\\) and sampling from the conditional distribution. Ok, now we need to get the decoder in there. I’ll move the \\(log(p(x))\\) inside the expectation (legal since it doesn’t depend on \\(z\\)) and then rewrite using the equality \\(log(p(z\\vert x)p(x)) = log(p(x,z)) = log(p(x\\vert z)p(z))\\):\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(x \\vert z;\\theta)) + log(p(z))]\\]\nYay! Theres our decoder! (Dont doubt me, I really am that excited as I write this). Now what we can do is collapse things into Kullback-Leibler divergences - specifically any +/- pair of log probabilities dependent on \\(z\\) can be rewritten as a KL divergence:\n\\[\nlog(p(x)) = KL(q(z\\vert x;\\phi) \\vert\\vert p(z \\vert x)) - KL(q(z\\vert x;\\phi)\\vert\\vert p(z)) + E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\label{eq1}\\tag{eqn-1}\n\\]\nHrm, we have that pesky intractable \\(p(z \\vert x)\\) in there. Thankfully, we can focus on the other terms and simply rewrite/rearrange using the fact that KL-divergence is non-negative:\n\\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\label{eq2}\\tag{eqn-2}\n\\end{align}\n\\]\nThe right hand side is known as the evidence lower bound (ELBO), and various derivations of it and the above inequality can be found all over the internet if you found the above unsettling, disturbing, offensive.\nIt is useful to stare a bit at (\\(\\ref{eq1}\\)) and (\\(\\ref{eq2}\\)). First, if our \\(q(z\\vert x;\\phi)\\) eventually ends up matching \\(p(z \\vert x)\\), then the first KL divergence in (\\(\\ref{eq1}\\)) will be zero, and maximizing the RHS of (\\(\\ref{eq2}\\)) will be like directly maximizing \\(p(x)\\) [2].\nThe other notable thing is that (\\(\\ref{eq2}\\)) has a nice interpretation. \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\) can be though of as the reconstruction loss - how close are our reconstructions to the data. \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\) is like a regularization term telling our encoder: “You must try to be like the prior distribution \\(p(z)\\)”. This regularization term achieves the goal of making sure the conditional distributions across the \\(x\\) are nice and compact around the prior distribution - so if we sample from a \\(\\mathcal{N}(0,I)\\), we are likely to get a \\(z\\) that is likely to produce one of our \\(x\\)’s.\n\n\nMaximizing the objective through gradient descent\nSo, uhm…is (\\(\\ref{eq2}\\)) differentiable? Nope. Tutorial over, you’ve been had, AHAHAHAHA!\nJoking aside it actually isn’t currently amenable to backpropagation but we will get around that in a second. So remember we have two things we need to compute:\n\nThe regularization term \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\)\nThe reconstruction term \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\)\n\nFor 1, when the two distributions in the KL divergence are Gaussian, then there is a nice closed form solution that reduces nicely when, as in our case, the second distribution is \\(\\mathcal{N}(0, I)\\):\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = KL(\\mathcal{N}(Z \\vert \\mu(x;\\phi), \\Sigma(x;\\phi))\\vert\\vert \\mathcal{N}(0,I))\n\\] \\[\n= \\frac{1}{2}(tr(\\Sigma(x;\\phi)) + \\mu(x;\\phi)^T\\mu(x;\\phi) - k - log(det(\\Sigma(x;\\phi))))\n\\]\nWhere remember \\(\\mu(x;\\phi)\\) and \\(\\Sigma(x;\\phi)\\) are our mean vector and covariance matrix computed by our encoder. Nice, this is a value, it is differentiable with respect to the parameters \\(\\phi\\), great.\nFor 2, note that when we compute gradients, we move the gradient inside the expectation, and are just computing a gradient from a single example of \\(z\\), drawn from \\(q(z\\vert x;\\phi)\\) given our input \\(x\\) at train time (and over many gradient computations, we have well approximated maximizing the expectation over \\(z\\sim q(z\\vert x;\\phi)\\)). However we have a bit of a problem. Again notice the expectation is over \\(z\\) sampled from \\(q(z\\vert x;\\phi)\\), therefore maximizing the expression inside the expectation depends not only on updating \\(p(x \\vert z;\\theta)\\) to perform reconstruction well, but on updating \\(q(z\\vert x;\\phi)\\) to produce distributions that output \\(z\\)’s that \\(p(x \\vert z;\\theta)\\) finds easy to decode.\nOk, so no big deal? Just update both networks based on the reconstruction loss? This won’t immediately work because our training looks like:\n\nSend \\(x\\) through \\(q(z\\vert x;\\phi)\\) to get a distribution over \\(z\\)’s\nSample a \\(z\\) from that distribution\nSend \\(z\\) through \\(p(x \\vert z;\\theta)\\) to produce \\(x\\) and compute the reconstruction loss\n\nThe problem is that we cannot backpropagate through step 2 (‘sample from a distribution’ is not a math operation we can take the gradient of). The solution is what is known as the reparametrization trick. Instead of sampling directly from the distribution inferred by the output of our encoder, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and then multiply/add… :\n\\[\\epsilon*\\sqrt{\\Sigma(x;\\phi)} + \\mu(x;\\phi)\\]\n…to mimic sampling from the implied distribution. The difference being that in this case we are just adding and multiplying by constants, things which backpropogation is fine with (even though those constants were obtained from sampling).\nNow we can compute gradients for batches of \\(x\\) and average the gradients in the normal fashion to train the model."
  },
  {
    "objectID": "posts/ppo/index.html",
    "href": "posts/ppo/index.html",
    "title": "Lunar Lander with PPO",
    "section": "",
    "text": "So, PPO….it seems like the transformer of reinforcement learning: “What should we do?”, “Eh, shove it through PPO and see what happens”. So again, this is just me replicating PPO and imploring anyone who got lost on the internet and is reading this to just go read the paper instead: Schulman et al. (2017).\nIf you are just here for some code, here is a colab notebook that runs hyperparameter tuning with wandb and ray-tune on my PPO implementation. At the end you can grab a trained model from wandb, run an episode, and download a gif of the episode."
  },
  {
    "objectID": "posts/ppo/index.html#the-objective",
    "href": "posts/ppo/index.html#the-objective",
    "title": "Lunar Lander with PPO",
    "section": "The Objective",
    "text": "The Objective\nMmmm, ok again….the paper….but I’ll go over the main pieces presented therein. PPO is, as it’s name suggests, a policy gradient algorithm, in the sense that it tries to maximize expected performance (total reward) by directly leveraging the gradient of the performance with respect to the policy parameters. That is, we want to maximize \\(J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\) and can derive an expression for the gradient \\(\\nabla_{\\theta} J(\\pi_{\\theta})\\) that we can approximate by sampling episodes and computing gradients. For a good intro to policy optimization, I like this intro by OpenAI.\nPPO is different from ‘vanilla’ policy optimization in that it maximizes some weird surrogate objective function:\n\\[\nL^{CPI}(\\theta) = E_{t \\sim \\pi_{\\theta_{old}}}\\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A(s_t, a_t)\\right]\n\\]\nwhere \\(A(s_t, a_t)\\) is the advantage function, which is the difference between the value function at time step \\(t\\) and some bootstrapped version of the value function at time step \\(t\\), which I’ll show later. The subscripts \\(\\theta\\) and \\(\\theta_{old}\\) indicate the parameters that will be updated, and a ‘frozen’ version of the parameters that will be used to compute the ratio of the new policy to the old policy. In words, the part of the objective inside the expectation says if the action we took at time step \\(t\\) was better than we currently expect (positive advantage), increase the probability of that action. If it was worse than we expect (negative advantage) decrease the probability of that action. Each example inside the expectation is sampled according a policy with the old parameters, \\(\\pi_{\\theta_{old}}\\).\nOkay, so if you stare at that loss there, you might notice that the algorithm will probably want to make very large updates to \\(\\theta\\). PPO addresses this by ‘clipping’ the probability ratio to prevent these large updates, like so:\n\\[\nL^{CLIP}(\\theta) = E_{t \\sim \\pi_{\\theta_{old}}}\\left[\\text{min}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A(s_t, a_t), \\text{clip}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)A(s_t, a_t)\\right)\\right]\n\\]\nWhere clip\\((x, a, b)\\) is a function that clips \\(x\\) to be between \\(a\\) and \\(b\\) and \\(\\epsilon\\) is a hyperparameter that controls the size of the clipping.\nOne thing that was weird to me, is that this loss does not have the log probability in the expectation there, as is seen in the derivation of the vanilla policy gradient. Well, actually, it sort of does, but it’s just easier to write it the way they do, see this blog post for an explanation."
  },
  {
    "objectID": "posts/ppo/index.html#computing-as_t-a_t",
    "href": "posts/ppo/index.html#computing-as_t-a_t",
    "title": "Lunar Lander with PPO",
    "section": "Computing \\(A(s_t, a_t)\\)",
    "text": "Computing \\(A(s_t, a_t)\\)\nIn the paper they mention two ways of computing the advantage. One is to simply compute the difference between the value function at time step \\(t\\) and the rewards-to-go from time step \\(t\\) plus the value function at the last time step:\n\\[\n\\hat{A}(s_t, a_t) = -V(s_t) + \\sum_{t'=t}^{T}r_{t'} + V(s_T)\n\\]\nAs mentioned in the paper, we can compute a weighted version of this return Schulman et al. (2018) Sutton and Barto (2020) as so:\n\\[\n\\hat{A}(s_t, a_t) = \\sum_{t'=t}^{T}(\\gamma \\lambda)^{t'-t}\\delta_{t'}\n\\]\nWhere \\(\\delta_{t'} = r_{t'} + \\gamma V(s_{t'+1}) - V(s_{t'})\\) is the TD error at time step \\(t'\\) and \\(\\lambda\\) is a hyperparameter that controls the weighting of the TD error. I show my implementation of this and highlight a mistake I made.\nFirst, lets assume I’ve calculated the rewards-to-go, \\(G_t = \\sum_{t'=t}^T \\gamma^{t'-t}r_t'\\), and the value function, \\(V(s_t)\\), for each time step \\(t\\). First, I compute the deltas:\n# given tensor of values val_tensor and tensor of rewards-to-go dsc_rews\ndeltas = gamma*F.pad(val_tensor[1:], (0, 1), 'constant', 0) + torch.tensor(rew_list) - val_tensor\nFrom this, I can compute the advantages by starting from the last time step, working backwards, and accumulating the weighted deltas:\n# compute advantages from deltas\nadvantages = []\ncur_adv = 0\nfor t in range(len(deltas)):\n    cur_adv = deltas[-(t+1)] + gamma * lam * cur_adv\n    advantages.append(cur_adv)\n\nadvantages = reversed(torch.tensor(advantages, dtype=torch.float32))\nYou can use a similar to collect-and-reverse scheme to compute the rewards to go, or you can be an idiot and forget to do that, like me:\ndsc_rews = []\ncur_rew = 0\n\n# compute discounted rewards\nfor t in range(len(rew_list)):\n    cur_rew = rew_list[-t] + gamma * cur_rew\n    dsc_rews.append(cur_rew)\nBasically my rewards to go were backwards…christ, look, the indexing isn’t even correct. Consider the situation where I have a large negative reward at the end of the episode, the first action will take the full weight for that failure. Unsurprisingly, my LunarLander agent was a bit timid about approaching the surface. However, shockingly, the algorithm still seemed to learn something with this glaring mistake, though it was incredibly inconsistent. My intuition is that successful episodes would still propagate positive reward signal to all state action pairs, even if the actual values were inaccurate, so the agent was still able to attempt to learn good behavior.\nOkay, so now the correct way to compute the rewards to go:\ndsc_rews = []\ncur_rew = 0\n\n# compute discounted rewards\nfor t in range(len(rew_list)):\n    cur_rew = rew_list[-(t+1)] + gamma * cur_rew\n    dsc_rews.append(cur_rew)\n\ndsc_rews = reversed(torch.tensor(dsc_rews, dtype=torch.float32))"
  },
  {
    "objectID": "posts/ppo/index.html#computing-the-loss",
    "href": "posts/ppo/index.html#computing-the-loss",
    "title": "Lunar Lander with PPO",
    "section": "Computing the Loss",
    "text": "Computing the Loss\nOkay so we have the advantages, the rewards-to-go, and some log-probabilities. We need to compute that probability ratio thing, multiply it by the advantage and then take the minimum of that and its clipped version. An important thing to remember is that when forming term \\(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}{A}(s_t, a_t)\\) the only thing that has gradients flow through it is the numerator. The advantage is a static quantity at this point, and the denominator is a frozen version of log-probabilities we saw when collecting experience.\nMy implementation runs the observations back through the actor and critic to get the quantities we need to be differentiated via autograd (everything below is in the colab notebook).\n# cur_batch from a queue of tuples\nobs_batch, act_batch, rew_batch, val_batch, log_prob_batch_prev, dsc_rew_batch, adv_batch = zip(*cur_batch)\n\ndist_batch = actor(torch.tensor(obs_batch, device = device))\n\n# this `requires_grad`, log_prob_batch_prev does not\nlog_prob_batch = dist_batch.log_prob(torch.tensor(act_batch, device=device))\nval_batch = critic(obs_batch.to(device))\nNow I compute the ratio with the differentiable numerator\ndenom = torch.tensor(log_prob_batch_prev, device=device)\nadv_batch = torch.tensor(adv_batch, device=device)\n\nratio = torch.exp(log_prob_batch - denom.to(device))\nAnd compute the minimum of the clipped objective and the unclipped objective, as well as the mean-squared error between the value function estimate and the actual rewards-to-go we observed. The total loss is the negative of the clipped objective plus the value function loss. Note: in the paper they mention an entropy loss to encourage exploration, I show it below, but I don’t use it in my implementation, instead I use a simple exploration scheme where I randomly sample actions at a decreasing probability over training.\nsloss = torch.min(ratio * adv_batch, torch.clamp(ratio, 1.0 - config['eps'], 1.0 + config['eps']) * adv_batch).mean()\nvloss = F.mse_loss(val_batch.squeeze(), torch.tensor(dsc_rew_batch, dtype = torch.float32, device=device))\n\n# possible entropy loss, we would add this to loss\n# entropy = dist_batch.entropy().mean()\n\nloss = -sloss + vloss"
  },
  {
    "objectID": "posts/ppo/index.html#traininghyperparameter-tuning",
    "href": "posts/ppo/index.html#traininghyperparameter-tuning",
    "title": "Lunar Lander with PPO",
    "section": "Training/Hyperparameter Tuning",
    "text": "Training/Hyperparameter Tuning\nI train on the LunarLander environment for 100 rounds of sampling + training, filling a buffer with some number N iterations, and then updating with training batches from that buffer. I also have some fun hyperparameter tuning, varying the following:\nlearning rate (actor and critic), clipping parameter \\(\\epsilon\\), \\(\\lambda\\) parameter in the computation of the advantage, and discount factor \\(\\gamma\\). The results are in this wandb project. Below is the parallel lines plot showing the effect of hyperparameters.\n\n\n\nMany of the training runs stagnate around 0 average reward, others get up to around ~100 average reward. A few seem to be doing okay and then completely collapse…I’m not sure what’s going on here but I’m investigating some methods of alleviating this. Some of the runs maintain a stable reward at over 200, which is what we want to consider it a success. The parameter that training seems most sensitive to is the advantage computation parameter \\(\\lambda\\).\nOkay, below is a gif of an episode from the best training run:\n\nIt has the ‘correct’ behavior of dropping quickly, before engaging the thrusters to land softly. Rather, it does not have some annoyingly not-awful-not-great behaviors such as just hovering in the air, afraid of crashing, or landing and then refusing to turn off thrusters."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Another blog? Why sir?",
    "section": "",
    "text": "I made this blog because I wanted to practice with .qmd documents, as well as have a place to document my learning journey on various data-science/ML topics (I may also post pictures of bread I bake). Also, I needed a way to inflate my ego, and a blog seems like a good way to do that?\n__DISCLAIMER__ This will appear throughout the blog - I am not an expert in anything, I will try to couch everything in uncertainty. Use any code or follow any conclusions I post at your own risk."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Miscellaneous Rambling from Daniel Mabayog Claborne",
    "section": "",
    "text": "Training Diambra Agents on Google Colab\n\n\n\n\n\n\nmachine learning\n\n\n\nRun some fighting game environments on an ec2 instance, retrieve experience for training an RL agent from Google Colab.\n\n\n\n\n\nOct 24, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nUsing MlFlow with Minio, Postgres, and Hydra.\n\n\n\n\n\n\nmachine learning\n\n\n\nUsing mlflow with minio and postgresql as storage backends, and hydra to manage configuration and perform sweeps.\n\n\n\n\n\nJun 21, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Updates, Flipping Coins (Bobgate)\n\n\n\n\n\n\npersonal\n\n\npuzzles\n\n\n\nPersonal updates: PhD applications and OMSCS worries. Bonus: a puzzle about flipping coins.\n\n\n\n\n\nMay 11, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nLunar Lander with PPO\n\n\n\n\n\n\nmachine learning\n\n\nreinforcement learning\n\n\n\nI am a sick man who enjoys watching a little lunar module softly touch down on the surface in a simulated environment. See how to do it with PPO.\n\n\n\n\n\nJul 12, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Bagels With GPT-5\n\n\n\n\n\n\nbaking\n\n\nmachine learning\n\n\n\nSomewhere in this blog I mentioned I would talk about baking? Ok here it is. Look at that bagel, mmmmmm. I got the recipe from GPT-5 which OpenAI has been sitting on since early 2023 and gave me early access to.\n\n\n\n\n\nApr 16, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nAnt-v4 with DDPG\n\n\n\n\n\n\nmachine learning\n\n\nreinforcement learning\n\n\n\nTraining a ‘spider’ to awkwardly walk to the right using DDPG.\n\n\n\n\n\nApr 13, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nPain, Suffering, Vector-quantized VAEs\n\n\n\n\n\n\nmachine learning\n\n\n\nI’ve written before about both the gumbel-max trick and variational autoencoders. The world demanded a post that combined the two, so here it is.\n\n\n\n\n\nMar 14, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nI Follow a GPT Tutorial\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nDumb Mistakes Training Vision Transformer\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\ncode\n\n\n\nRead about me fumbling around recreating vision transformer, one of the components of DALL-E/CLIP.\n\n\n\n\n\nJan 1, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nVectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!\n\n\n\n\n\n\ndata science\n\n\ncode\n\n\n\nBehind this disaster of a title lies the secret to quickly sample from a categorical distribution in python!\n\n\n\n\n\nNov 28, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nYet Another Explainer on Variational Autoencoders\n\n\n\n\n\n\nmachine learning\n\n\n\nMy attempt at explaining VAE’s in an effort to understand new text-to-image systems.\n\n\n\n\n\nNov 12, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nFTX Blowup and Massive Egos on Twitter\n\n\n\n\n\n\nnews\n\n\ncrypto\n\n\n\nI found the 2021-2022 crypto hype cycle and subsequent FTX collapse to be fascinating and infuriating. A couple people wrote books about it, I settled for a blog post.\n\n\n\n\n\nNov 9, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nThe blog lives. Blog blog blog.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/diambra-colab/index.html",
    "href": "posts/diambra-colab/index.html",
    "title": "Training Diambra Agents on Google Colab",
    "section": "",
    "text": "SAD TIMES SPOILER: There is no expert agent at the end of this blog post, really its mostly about interfacing between multiple docker containers running on an EC2 instance and Google Colab. See the example notebook here.\nDiambra is a framework for training reinforcement learning agents on classic video games. In my exploration of the project, I found myself needing a gpu to train CNN based policies and value function approximators. Enter Google Colab, where you can get access to an A100 GPU for 10 bucks a month (granted you probably only have ~20 hours of training on an A100 with your monthly credits before needing to top up). I decided to give this a try since Colab is a nice way to share solutions.\nImmediately there was a problem: Diambra is run with Docker containers, and running these containers on Google Colab is, in my investigations, not well supported/hacky. My solution was to attempt to run the containers on a separate machine, accessible by the Colab instance. The Colab instance would then send actions to the Diambra container over HTTPS, and receive observations and rewards in return. As we’ll see, this solution runs into some issues, hopefully solvable by someone reading this!"
  },
  {
    "objectID": "posts/diambra-colab/index.html#single-environment-setup",
    "href": "posts/diambra-colab/index.html#single-environment-setup",
    "title": "Training Diambra Agents on Google Colab",
    "section": "Single Environment Setup",
    "text": "Single Environment Setup\nFirst lets see how to get the simplest setup working: 1 container on a remote machine, accessed from Google Colab. The remote machine I used was an AWS EC2 instance.\n\nSet up an EC2 instance\nWe need an ec2 instance to host our containers. It does not have to be beefy, just have enough memory to run some docker containers. I started with a t2.micro running AWS Linux, but we’ll have to spin up more . For a tutorial on how to create/connect to an ec2 instance via SSH, see this tutorial.\nOnce you are up and running and connected, we need to install docker. See this AWS documentation. In short, we run:\n# install docker\nsudo yum update -y\nsudo yum install -y docker\n\n# start the docker daemon\nsudo service docker start\n\n# add the ec2-user to the docker group so you dont have to do sudo docker commands\nsudo usermod -a -G docker ec2-user\nCheck that docker works with docker run hello-world or docker ps.\n\nAdd a Security Group\nI am about to give you some advice that is probably bad, which is to allow all traffic on ports 32000-32999 on your ec2 instance. Under the ‘Security’ tab of your ec2 instance, there should be at least one listed security group. Click on that security group, go to the groups inbound rules and ‘Edit inbound rules’. You should see a screen with an option to ‘Add rule’. The rule we want to add is a Custom TCP rule on ports 32000 - 32999 as shown below:\n\nDo this same thing for the outbound rules, as we will be passing data to and receiving data from the running containers on the ec2 instance.\n\n\nInstall Diambra\nThis setup is exactly like in the Diambra installation docs and the vanilla ec2 instance should have all you need to install diambra. You will, as with running locally, need a copy of a compatible ROM (I use DOA++ in this example). You can copy this to the ec2 instance with scp or rsync from your local machine:\nscp -i ~/.ssh/my-key.pem /path/to/doapp.zip &lt;ec2-public-ip&gt;:&lt;ec2-roms-path&gt;\n\n\n\nRun a Diambra Container\nAnd then specify the roms path as usual in the ec2 instance with the DIAMBRAROMSPATH environment variable.\nWe can run a single diambra container with:\n[ec2-user]$ diambra arena --path.roms /path/to/roms --env.host 0.0.0.0 up\n🖥️  logged in\nv2.2: Pulling from diambra/engine\nDigest: sha256:77ba99e5d7d099fe2db292fc7b4e43da310e1bbdb0bcc3a157810e1f933ec81d\nStatus: Image is up to date for diambra/engine:v2.2\nStored credentials found.\nAuthorization granted.\nServer listening on 0.0.0.0:50051\n127.0.0.1:32768\nMake note of the port it is running on, this will be in the range 32000-32999.\nWe can see the running container with:\n[ec2-user]$ docker ps\nCONTAINER ID   IMAGE                 COMMAND                  CREATED         STATUS         PORTS                      NAMES\n7bf96960255f   diambra/engine:v2.2   \"/bin/diambraEngineS…\"   8 seconds ago   Up 6 seconds   0.0.0.0:32768-&gt;50051/tcp   busy_villani"
  },
  {
    "objectID": "posts/diambra-colab/index.html#connect-from-colab",
    "href": "posts/diambra-colab/index.html#connect-from-colab",
    "title": "Training Diambra Agents on Google Colab",
    "section": "Connect From Colab",
    "text": "Connect From Colab\nFrom colab, we can connect using the diambra api. We will need the ec2 instances IPv4 address/IPv4 DNS. These can be obtained from the main page of your ec2 instance in the AWS console.\nimport diambra.arena\nfrom diambra.arena import EnvironmentSettings\n\n# \nos.environ['DIAMBRA_ENVS'] = \"your-ec2-ipv4-address:port\"\n\nsettings = EnvironmentSettings(\n    difficulty=2,\n    splash_screen=False\n)\n\nenv = diambra.arena.make(\"doapp\", render_mode='rgb_array', env_settings=settings)\nenv.reset()\n\nact = env.action_space.sample()\nstate, reward, done, truncated, info = env.step(act)\nIf we take a look at the state we see the start screen of a fight inside DOA++:\n\nNow you have the ability to run a training loop or pass your environment object to e.g. stable-baselines."
  },
  {
    "objectID": "posts/diambra-colab/index.html#connect-to-multiple-environments",
    "href": "posts/diambra-colab/index.html#connect-to-multiple-environments",
    "title": "Training Diambra Agents on Google Colab",
    "section": "Connect to Multiple Environments",
    "text": "Connect to Multiple Environments\nJust one problem…there is a bit of latency between sending the action to the ec2 instance, and receiving the observations, rewards, etc. back. It’s not much for a single ping, but we’re talking about training an RL agent, requiring millions of agent-environment interactions, and if you add 30ms to every single interaction you can imagine the training is gonna be very slow.\nNo problem, lets spin up a bunch of containers and call them in parallel! How do we do this? Well, basically we need to create a class that spins up multiple environments and calls them all at once. Thanks to @lwneal on github for this example which was basically the template for what follows. We essentially have a function that executes multiple commands from an iterable and then use that for our environment’s reset() and step() functions. See the colab notebook for the code and below for an example of 12 observations pulled from 12 environments running in an ec2 instance:\n\nGreat! There are still some hurdles to jump over however. Remember these are 12 different environments being sampled from, we cant just mix the observations from all of them. Instead, we have to keep track of things like states, rewards, etc. on a per-environment basis. Something like below:\n# store running episodes for each parallel environment\nobs_list = [[] for _ in range(len(env.envs))]\nact_list = [[] for _ in range(len(env.envs))]\nrew_list = [[] for _ in range(len(env.envs))]\nval_list = [[] for _ in range(len(env.envs))]\nlog_prob_list = [[] for _ in range(len(env.envs))]\ndone_list = [[] for _ in range(len(env.envs))]\ntruncated_list = [[] for _ in range(len(env.envs))]\ndiscounted_rewards = [[] for _ in range(len(env.envs))]\nFor example, in PPO, we have to complete a few episodes, compute discounted rewards and advantages, and then form training tuples. We don’t want to just wait til a few episodes are complete, update the policy/value function, and then continue, since then the episodes that did not get used since they did not complete will not reflect the current policy.\nFurther, this process is…..still kinda slow. Colab is a great tool, but this might not be the use case for it (we dont really need that A100), and the cost of spinning up an ec2 instance with a GPU might be worth having the environment running on the same machine that is doing the training and seriously reduce latency issues. I’ll probably give a go at actually training a fighter all on an ec2 instance since, well, I want to actually get this working and I’ve been making some mistakes in training previous PPO agents (incoming amendments to previous posts)."
  },
  {
    "objectID": "posts/vision-transformer/vision-transformer.html",
    "href": "posts/vision-transformer/vision-transformer.html",
    "title": "Dumb Mistakes Training Vision Transformer",
    "section": "",
    "text": "Another post about one of the pieces of DALL-E, the vision transformer (Dosovitskiy et al. 2021), which is specifically used as one of the vision encoders in CLIP (Radford et al. 2021), which is itself used to condition the decoding process in DALL-E.\nThe vision transformer is at its base really not all that complicated: An image is divided into patches, and each patch is flattened, sent through an embedding layer, and arranged in a sequence to provide the expected input to a stack of multi-headed-attention + position-wise feed-forward layers identical to the architectures used to process text. Somewhat miraculously, with enough data, the vision transformer learns the spatial nature of the input even without the inductive biases of convolutional networks.\nHere’s a link to the colab notebook for training this thing up.\nI did roughly the following:\n\nEssentially copy the transformer encoder architecture from harvard nlp\nPrepend the patch creation and embedding plus learnable positional embeddings\nAdd the classification token and classification head as described in (Dosovitskiy et al. 2021)\n\n\nComponents added to base transformer architecture\nI implemented an image patcher like so:\nimport torch\nimport einops\n\nclass imagePatches(torch.nn.Module):\n  def __init__(self, patch_size=(8,8), input_channels = 3, stride = 8, embedding_dim=768):\n    super().__init__()\n    self.patch_size = patch_size\n    self.unfold = torch.nn.Unfold(patch_size, stride = stride)\n    self.patch_embedding = torch.nn.Linear(\n        patch_size[0]*patch_size[1]*input_channels, \n        embedding_dim\n    )\n\n  def forward(self, img):\n    patches = self.unfold(img)\n    patches = einops.rearrange(patches, \"b c p -&gt; b p c\")\n\n    embeddings = self.patch_embedding(patches)\n    return embeddings, patches\nThe classification head looked like:\n# ... blah blah module super init\n\ndef forward(self, x:torch.tensor, **kwargs):\n    x = encoder(x, **kwargs)\n    # x = Reduce(\"b p c -&gt; b c\", reduction='mean')(x) #\n    # x = self.layernorm(x)\n    x = self.layernorm(x[:,0])\n    x = self.hidden(x)\n    x = torch.tanh(x)\n    x = self.classification(x)\nI switched back and forth between the mean pooling (commented out) and using only the classification token output before going through the classification layer.\nJust to show it, here’s the class embedding + learnable positional embedding inside the encoder class:\nclass vitEncoder(torch.nn.Module):\n  def __init__(self, n_layers, embedding_dim, image_tokenizer, mha_layer, ff_layer, n_patches):\n    super().__init__()\n    self.image_tokenizer=image_tokenizer\n    self.positional_embedding = torch.nn.Parameter(\n        torch.randn((n_patches + 1, embedding_dim))\n    )\n    self.embedding_dim = embedding_dim\n    self.n_layers = n_layers\n    self.encoder_layers = torch.nn.ModuleList([\n        vitEncoderLayer(copy.deepcopy(mha_layer), copy.deepcopy(ff_layer)) for _ in range(n_layers)\n    ])\n    self.class_embedding = torch.nn.Parameter(torch.randn((1, 1, embedding_dim)))\n    \n  def forward(self, x:torch.tensor, attn_mask:torch.tensor=None):\n    x, _ = self.image_tokenizer(x)\n\n    x = torch.concatenate([\n        einops.repeat(self.class_embedding, \"b t c -&gt; (r b) t c\", r = x.shape[0]), x], \n        axis = 1 # patch sequence axis\n    )\n\n    x = x + self.positional_embedding\n\n    for l in self.encoder_layers:\n      x = l(x, attn_mask)\n\n    return(x)\n\n\nTraining\nFirst attempt:\nEpoch 0, Loss is 2.305\nEpoch 0, Loss is 2.367\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.298\n\n...\n\nEpoch 10, Loss is 2.300\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.312\n… shit. Oh right the positional embeddings.\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.309\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.308\n\n...\n\nEpoch 10, Loss is 2.299\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.311\nEpoch 10, Loss is 2.303\n…damnit. Oh whoops I missed a couple activations.\nEpoch 0, Loss is 2.331\nEpoch 0, Loss is 2.299\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.312\n\n...\n\nEpoch 10, Loss is 2.291\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.312\nEpoch 10, Loss is 2.301\n…shit…\n…\n*Reminds self this is an EXERCISE and pain is expected.* I did eventually get this working, but first a couple bugs I found along the way:\n\nTwo very dumb mistakes implementing the layer normalization (After fixing it I switched to just using torch.nn.LayerNorm)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.w = nn.Parameter(torch.ones(features))\n        self.b = nn.Parameter(torch.zeros(features))\n\n    def forward(self, x, eps=1e-6):\n        return self.w * x.mean(-1, keepdim=True) / (x.std(-1, keepdim=True) + eps) + self.b\nYou see it? Yea I’m not actually mean-centering in the numerator there…\n    ...\n    \n    def forward(self, x, eps=1e-6):\n        self.w * (x - x.mean(-1, keepdim=True)/(x.std(-1, keepdim=True) + eps)) + self.b\nAnd here I’ve just fudged the parentheses…squint a bit and you’ll see.\n\nMultiheadedAttention expects sequence dimension first? Apparently if you dont specify batch_first = True:\n\nattention_layer = torch.nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=n_head, batch_first = True)\nThen the MultiheadAttention treats the first dimension as the sequence dimension of the QKV input…this fails silently since the self-attention matrix multiplication is still valid.\n\nLong story short, I did get the model to train up modestly on MNIST (I’ll try on the other two datasets later). You can see my successful training run against all others here:\nIn the end it was using Adam over SGD in training that got me to see proper training. SGD is known to be very picky about the learning rate. I might try some sweeps over various LR’s and report back.\nAnother (probably related) thing I ran into is that averaging all the outputs of the last layer resulted in some learning with SGD, but taking only the classification token output resulted in zero learning. Probably the learning rate(s) I was using were closer to being appropriate for averaging than with taking just the classification token output.\n\n\n\n\n\nReferences\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], June. http://arxiv.org/abs/2010.11929.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv. https://doi.org/10.48550/arXiv.2103.00020."
  },
  {
    "objectID": "posts/init-bagels/bagels.html",
    "href": "posts/init-bagels/bagels.html",
    "title": "Making Bagels With GPT-5",
    "section": "",
    "text": "I’ve made bagels for a long time, usually bringing them into the office and leaving them out in the lunch-room for co-workers, interns, and others to eat them. If you’d just like to have the recipe and be on your way it is Peter Reinhart’s New York Style Bagels With Wild Sourdough. I use molasses instead of barley malt, and ~30g less flour since I found 4 cups to make them too tough — the gears on my KitchenAid were eventually stripped from mixing the dough. I’ve had it from at least one actual New Yorker that they pass the test.\nI would often anonymously leave them in there and pat myself on the back for so selflessly sharing the gift of baking with people without need for thanks and praise. Of course this was inwardly dishonest, as I was going for the having my-cake-and-eating-it-too situation of everyone knowing who provided the bagels, but me not putting too much effort into having it be known such that I can claim pure intentions.\nAnyway, the pandemic obviously killed the bagel sharing situation in the office. Too bad too since I was on a long streak of having bagels in the break room every Monday morning. For a while I tried to deliver to co-workers houses, distribute to neighbors etc., but did not move nearly the same volume as before. Once I moved out of my apartment and started taking classes again the bagel production went to essentially zero.\nRecently I’ve tried to find time to make bagels again and have fixed a problem that plagued me previously - overproofing. Essentially the recipe is from a professional baker who has a walk in fridge that maintains a super consistent low temperature and can leave the dough in there for 24 hours, so I cut the proofing time to ~10 hours overnight, which seems to consistently produce a nice product. Anyway, here they are:\n\n\nFigure 1 shows histograms of the three color channels for the single bagel image. From the red color channel, we can see that the bagel is very tasty.\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# load an image\nimg = np.array(Image.open('bagel-single.jpg'))\n\n# histogram of the three color channels\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\naxs[0].hist(img[:,:,0].flatten(), bins=256, color='r', alpha=0.5);\naxs[1].hist(img[:,:,1].flatten(), bins=256, color='g', alpha=0.5);\naxs[2].hist(img[:,:,2].flatten(), bins=256, color='b', alpha=0.5);\n\n\n\n\n\n\n\nFigure 1: Histograms of the three color channel intensities of the single bagel image.\n\n\n\n\n\nIf you were invested in the GPT-5 part of the title I am very sorry."
  },
  {
    "objectID": "posts/ftx-blowup/index.html",
    "href": "posts/ftx-blowup/index.html",
    "title": "FTX Blowup and Massive Egos on Twitter",
    "section": "",
    "text": "The last 24 hours on crypto-Twitter have been a cacophony of rage, disbelief and whatever toxic stew of emotions exist in the hearts of crypto traders as the crypto exchange FTX has imploded after some FUD lead to actually-its-not-just-FUD-induced-selling/panic of the Chuck-E-Cheese tokens that made up the majority of assets on the balance sheet of the closely related trading desk Alameda Research. It was shortly thereafter that FTX itself was revealed to be insolvent, with a hole likely billions deep. More competent people than me will surely be releasing detailed postmortems in the coming days with the technicals of how this happened, though I’m almost sure it will boil down to some very, very brazen lies about what was being done with peoples money. Instead I’d like to talk (at bro-science level) about the pathology of the just spectacular lineup of characters who did totally legitimate business are are now totally not trying to evade authorities.\nI’ve written previously on my lightweight adventures in crypto, and though I’ve been out from a financial standpoint since ETH was $2000 and still had one more dance with $4000 left in it, I follow the various personalities in the crypto space, partly from a morbid interest in the strange specimens that inhabit the space, and partly from a lingering belief that there just might be something to crypto.\n\nSince I began to exit my crypto holdings, I’ve been a pretty big cynic about the space, however out of some (possibly misplaced) desire to maintain a nonzero level of optimism, I look for the honest, –or at least non-cringe– players in the space. The ones who will still be around if the meme-fueled casino ever decides to offer something of value.\nI initially found two candidates for groups of crypto people (sorry cryptographers, they are the crypto people now) that would let me proudly say I never completely lost faith in crypto once we are all paying our taxes in digital currency issued by His Excellency. These groups can be roughly categorized as the degens and the (self-proclaimed) builders.\nIn a result that reveals the true heart of crypto, the degens are the ones that still make me smile about crypto. They are lone-wolf traders, brash, pugnacious, but harmless; refreshingly self-aware and under few illusions (few, not none, as the FTX implosion has shown) about the game they are playing. It is just that to them – a game, and most seem to just be having fun and don’t wish anything bad on anyone. The most they might be blamed for is that they serve as an extra light that lures hapless retail traders into the electrified mesh. Finally, to their credit, I believe that until now they were not fooled by the latter group.\n\nThe second group, consisting of high-profile heads of exchanges, trading desks and other crypto ‘services’, never gave me the warm fuzzy feeling that the degens did, but boy did they try. Their Twitter accounts are (were) non-stop thread-presses, expounding their deep philosophical and technical wisdom. However the last year has exposed many of these characters (through the collapse of their projects, not just batshit tweets) as nothing more than reckless –though sufficiently intelligent– gamblers with massive egos.\n\n\n\nUntold wisdom awaits in their threads.\n\n\nEvery time one of them posted these insufferable threads clearly meant to keep their sense of self-importance fully inflated, I would try –as I did with the whole crypto space– to maintain a bit of belief that I might be being too cynical, that these people really were trying their best to build something good, but were possibly deluded about the fact that they were just in the ponzi-as-a-service industry. I said “hey if Sam Bankman-Fried is just transferring money from gamblers to animal-welfare causes (he wasn’t), then I guess I’m okay with that”.\nNever has my cynicism been more validated, and no doubt more validation is to come as the tide continues to recede. The more shocking thing is that these people, after they have been revealed as intellectual and legal frauds, continue to post Twitter threads about what they have learned and how they wish to impart their newfound wisdom as a result of the mistakes they’ve made rough times they’ve endured.\nSomething that keeps me reading their insane rants is a morbid desire to know whats going on in there. When, after incinerating people’s money, they try to explain their remorse and how they had the best intentions, what is their intent?\n\ngenuinely explain to people how things went bad, but that they didn’t want to hurt anyone\nThey have drunk their own Kool-Aid and are trying to prevent the air from rushing out of their inflated sense of self-worth.\nSetting up for the next con .\nMake tweets that sound good, hope that they appear in court as evidence, and that that might help them?\n\nI know its not healthy to spend too much time contemplating this, but you do sometimes just want to see the mask fall of and have them brokenly admit a-la Crime and Punishment that they are the murderer. The people such as His Excellency Justin Sun, though grotesque, at least have the advantage of being so outrageous that there is never any doubt that they are just pure grifters, obtaining their adrenaline rush from getting over on people and spitting in the face of fate.\nThe worst part is that the FTX blowup seems to have been the last straw for the degens. On Twitter, they seem exhausted, dejected, ready to call it quits. Their humorous banter is no longer there when I’m sipping coffee in the morning. Who now will bring a smile to my face about crypto?"
  },
  {
    "objectID": "posts/ddpg/ddpg.html",
    "href": "posts/ddpg/ddpg.html",
    "title": "Ant-v4 with DDPG",
    "section": "",
    "text": "I’ve had a lot of fun (most of the time) training reinforcement learning algorithms to play various gym environments. There is a somewhat sad satisfaction in getting an agent to push a cart to the top of a hill, land a lunar lander, or balance a pole on a cart - cartpole was the first RL task I tackled, and I am not ashamed to say I was very excited to see a pixelated cart successfully balance a pole.\nFor others (the thousands clearly reading this…) looking to learn, heres a list of resources for RL I’ve found useful:\n\nSutton and Barto’s intro RL book http://incompleteideas.net/book/the-book.html\nOpenAI’s ‘spinning up’ website https://spinningup.openai.com/en/latest/index.html\nGeorgia Tech’s RL course https://www.udacity.com/course/reinforcement-learning–ud600\n\nI took a break from self-learning RL, since I felt I’d gotten most of the low-hanging fruit, but decided to give something a try recently. Last time I looked around the state of libraries for RL was pretty bad, but now the suite of environments in gym is being maintained again, and there are a few good libraries that package up standard algorithms.\nIn this post I’ll look at DDPG, or Deep Deterministic Policy Gradient. It’s goal is to learn a policy for environments with continuous actions. The authors of the paper Lillicrap et al. (2019) note several issues arising from working in discrete action spaces, the main one being that the dimensionality of your action grows exponentially with the number of degrees of freedom — e.g. if your action is the movement of 10 joints, and each joint has 8 possible movements, then your action has dimensionality \\(8^{10}\\). Working with continuous action spaces overcomes this limitation, and they borrow heavily from previous work to train successful agents that output continuous actions on several tasks.\nI’ll use this algorithm to train the Ant-v4 environment from gymnasium[mujoco]. This task asks us to train a quadruped —which I will call a spider in defiance of the name of the environment and the number legs on a spider— to walk to the right by applying torques to 8 joints (the action is a continuous, 8 dimensional vector).\nAlright, so how do we do this? Well, I’m gonna level with you, you should just go read the paper, BUT - the short version is that we train a network \\(Q_\\theta\\) with parameters \\(\\theta\\) to estimate the Q-value at each state-action pair, as well as train a network \\(\\mu_\\phi\\) parametrized by \\(\\phi\\) to learn the policy.\nThe Q-value is updated in the usual manner - by minimizing it’s squared deviation from the boostrapped future rewards, i.e. minimizing \\(\\frac{1}{N}\\sum_{i=1}^{N}(r_i + \\gamma Q_\\theta(s_{i+1}, \\mu_\\phi(s_{i+1})) - Q_\\theta(s_i, a_i))^2\\). We then update the policy network \\(\\mu_\\phi\\) by applying the policy gradient, which was proven by Silver et al. (2014) to be approximated by:\n\\[\n\\frac{1}{N}\\sum_i \\nabla_a Q_\\theta(s = s_i, a = \\mu(s_i)) \\nabla_\\phi \\mu_\\phi(s_i, a_i)\n\\]\nWhere the term inside the sum is an expansion of \\(\\nabla_\\phi Q_\\theta(s, \\mu_\\phi(s))\\) via the chain rule.\nThis reminds me a lot of value iteration, where we first try to update our estimate of the value function, and then ‘chase’ our updated estimates of the value function by updating the policy with respect to those value functions. Usually, that means just updating our policy to take the new max, but here we have an actual policy gradient that flows through the value function.\nThey also augment their training with ideas from Mnih et al. (2015), specifically the replay buffer and target networks. The replay buffer is used to break the correlation between samples which is inherent in the sequential nature of the environment. Basically, we store a queue of &lt;state, action, reward, next state&gt; training tuples, and then randomly sample minibatches from this queue at train time. The target networks are used to stabilize training. If not used, then the network you are updating is also the network you are using to compute the target value, which….just seems wrong? Anyway, training without target networks tends to cause divergence in the updates.\nOkay, so how to we do this? First, go read the paper! Their pseudo-algorithm is actually pretty good and this article is just some guy doing the whole learning by explaining thing while trying to maintain some level of online presence. You’re still here? Fine….we’ll do what I just described. I have the full training script in this notebook. There’s also a bonus mini-example of how to use wandb with ray-tune in there.\n\nActor (policy) and Critic (Q-network)\nOkay, first thing, make two architectures to be the Q network and policy network. These are commonly referred to as the critic (Q network criticizes how ‘good’ a state action pair is) and the actor (the policy network chooses actions to be criticized).\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 512)\n        self.l2 = nn.Linear(512, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = F.relu(self.l1(state))\n        a = F.relu(self.l2(a))\n        return self.max_action * torch.tanh(self.l3(a))\n    \nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n\n        self.l1 = nn.Linear(state_dim, 400)\n        self.l2 = nn.Linear(400 + action_dim, 300)\n        self.l3 = nn.Linear(300, 1)\n\n    def forward(self, state, action):\n        q = F.relu(self.l1(state))\n        q = F.relu(self.l2(torch.cat([q, action], 1)))\n        return self.l3(q)\nYup, those are two MLPs … I guess one thing to note is that the critic takes both state and action as input, but the action comes in at the second layer. Another is that the actor outputs a tanh activation times the max of the action space. Now, we make FOUR NETWORKS! Two extra for the ‘target networks’ that will be used to form…the targets. It sounds ridiculous, but like everything else in machine learning, they tried it and it worked, so were trying it.\ncritic = Critic(env.observation_space.shape[0], env.action_space.shape[0])\nactor = Actor(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n\ncritic_tgt = Critic(env.observation_space.shape[0], env.action_space.shape[0])\nactor_tgt = Actor(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n\nopt_critic = AdamW(critic.parameters(), lr=cfg['critic_lr'])\nopt_actor = AdamW(actor.parameters(), lr=cfg['actor_lr'])\nNotice how I’m not making optimizers for the target networks. They are updated by slowly copying the weights from the main networks, like so:\nfor param, param_tgt in zip(critic.parameters(), critic_tgt.parameters()):\n    param_tgt.data.copy_(cfg['tau']*param.data + (1-cfg['tau'])*param_tgt.data)\n\nfor param, param_tgt in zip(actor.parameters(), actor_tgt.parameters()):\n    param_tgt.data.copy_(cfg['tau']*param.data + (1-cfg['tau'])*param_tgt.data)\nThat is, the weights get updated by \\(\\theta_{tgt} = \\tau * \\theta + (1-\\tau) * \\theta_{tgt}\\), where \\(\\theta\\) is the weights of the main network and \\(\\theta_{tgt}\\) is the weights of the target network.\n\n\nReplay Buffer\nThis is simple enough, we make a queue, and while traversing the environment we store state, action, reward, next state tuples in it, and then sample from it to update our actor and critic. I also fill it with a few thousand random samples before starting training, just to get some initial data in there to sample from. The queue is implemented using collections.deque:\nimport collections\n\nreplay_buffer = collections.deque(maxlen=cfg['buffer_size'])\n\n#...then during training ...\n\nreplay_buffer.append((obs, act, rew, obs_next, done))\nI didn’t mention that we also store whether or not we have completed an episode (done) in the replay buffer. This is needed when forming a target for an action where we only want to consider the reward. Storing done allows us to multiply the bootstrapped part of the value function by 1 - done to zero out the value function if we have completed an episode:\ny = np.array(rew_b) + cfg['gamma']*(1-np.array(done_b))*Q_tgt.numpy()\n\n\nTraining\nOkay, so store some interaction with the environment, and on each step sample from the replay buffer and update things. One thing that may not be obvious is how to update the actor network. Yea I wrote some weird chain rule formula up there but whats the code? It is a little weird, but if you stare at it it makes sense:\nacts_actor = actor(torch.from_numpy(np.array(obs_b)).float())\nQ = critic(torch.from_numpy(np.array(obs_b)).float(), acts_actor)\n\nopt_actor.zero_grad()\nloss = -Q.mean()\nloss.backward()\nopt_actor.step()\nWhats happening here is we send a sample through the actor, pass that action to the critic, and then… call backward() on the loss of -Q? Remember the standard backward() + step() pattern updates the weights to minimize whatever you called backward() on. Here we are calling it on the negative of the Q value, that is we are minimizing the negative value, which is maximizing the positive value — we are applying a gradient to our policy network that results in greater value. When we call step() we only call it on the actor network (at this point in the training loop we’ve already updated the critic). The whole chain rule stuff is taken care of by the fact that the gradients flowed backward through the critic.\n\nHyperparameters\nI do some hyperparameter tuning on things like batch size, replay buffer size, and learning rates. Here’s a link to a group of runs produced by ray-tune and wandb where you can see the variability in training outcomes due to hyperparameters:\n\n\n\n\n\n\nWeird walking spiders…\nHere is a funny walking spider trained via this procedure, the goal is to move to the right:\n\nIt funnily pulls itself along with one leg, which I will call a success, though I’ve seen other solutions where it majestically launches itself to the right like a gazelle.\n\n\n\n\n\nReferences\n\nLillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. “Continuous Control with Deep Reinforcement Learning.” arXiv. https://doi.org/10.48550/arXiv.1509.02971.\n\n\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. https://doi.org/10.1038/nature14236.\n\n\nSilver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, I-387-I-395. ICML’14. Beijing, China: JMLR.org."
  },
  {
    "objectID": "posts/use-shift/index.html",
    "href": "posts/use-shift/index.html",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "",
    "text": "Honestly, what a disaster of a title. I don’t know if either part in isolation would be more likely to get someone to read this, but I just wanted to make a post. Maybe I should have click-baited with something completely unrelated, oh well.\nI’m currently taking Machine Learning for Trading from the Georgia Tech online CS masters program as part of my plan to motivate myself to self-study by paying them money. While much of the ML parts of the class are review for me, it has been fun to learn things about trading, as well as do some numpy/pandas exercises.\\(^{[1]}\\)\nThe class is heavy on vectorizing your code so its fast (speed is money in trading), as well working with time series. I’ll go over two things I’ve found neat/useful. One is vectorizing sampling from a categorical distribution where the rows are logits. The other is using the .shift() method of pandas DataFrames and Series."
  },
  {
    "objectID": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "href": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "Vectorized sampling from a categorical distribution",
    "text": "Vectorized sampling from a categorical distribution\nOkay, so the setup is you have an array that looks like this:\n\nimport numpy as np\nnp.random.seed(23496)\n\n# The ML people got to me and I now call everything that gets squashed to a probability distribution 'logits'\nlogits = np.random.uniform(size = (1000, 10))\nlogits = logits/logits.sum(axis = 1)[:,None]\n\nlogits[:5,:]\n\narray([[0.13460263, 0.12458665, 0.05453746, 0.11991544, 0.07351353,\n        0.11034637, 0.07374194, 0.11460002, 0.11551094, 0.07864502],\n       [0.18602867, 0.09960763, 0.02422872, 0.10095124, 0.02961313,\n        0.04475981, 0.08855924, 0.11246979, 0.16960986, 0.14417191],\n       [0.0142491 , 0.14630917, 0.11735343, 0.12211442, 0.11230253,\n        0.12474719, 0.13253043, 0.01106296, 0.08627144, 0.13305933],\n       [0.09227899, 0.15207502, 0.07677232, 0.16330634, 0.11855988,\n        0.08710454, 0.05458428, 0.18425363, 0.0224089 , 0.04865609],\n       [0.01826615, 0.1956786 , 0.03484824, 0.12495028, 0.11824123,\n        0.01893324, 0.17954348, 0.15826364, 0.1351583 , 0.01611684]])\n\n\nEach row can be seen as the bin probabilities of a categorical distribution. Now suppose we want to sample from each of those distributions. One way you might do it is by leveraging apply_along_axis:\n\nsamples = np.apply_along_axis(\n    lambda x: np.random.choice(range(len(x)), p=x), \n    1, \n    logits\n)\n\nsamples[:10], samples.shape\n\n(array([3, 1, 6, 7, 8, 9, 2, 4, 5, 3]), (1000,))\n\n\nHm, okay this works, but it is basically running a for loop over the rows of the array. Generally, apply_along_axis is not what you want to be doing if speed is a concern.\nSo how do we vectorize this? The answer I provide here takes advantage of the Gumbel-max trick for sampling from a categorical distribution. Essentially, given probabilities \\(\\pi_i, i \\in {0,1,...,K}, \\sum_i \\pi_i = 1\\) if you add Gumbel distribution noise to the log of the probabilites and then take the max, it is equivalent to sampling from a categorical distribution.\nAgain, take the log of the probabilities, add Gumbel noise, then take the arg-max of the result.\n\nsamples = (\n    np.log(logits) + \\\n    np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1)  \n\nsamples[:10], samples.shape\n\n(array([0, 0, 5, 2, 2, 5, 6, 9, 7, 9]), (1000,))\n\n\nLets test if this is actually faster:\n\n%%timeit\n(np.log(logits) + np.random.gumbel(size = logits.shape)).argmax(axis = 1)  \n\n332 μs ± 775 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\nnp.apply_along_axis(lambda x: np.random.choice(range(len(x)), p=x), 1, logits)  \n\n17.5 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nYea, so a couple orders of magnitude faster with vectorization. We should probably also check that it produces a similar distribution across many samples (and also put a plot in here to break up the wall of text). I’ll verify by doing barplots for the distribution of 1000 values drawn from 4 of the probability distributions. Brb, going down the stackoverflow wormhole because no one knows how to make plots, no one.\n…\nOk I’m back, here is a way to make grouped barplots with seaborn:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# do 1000 draws from all distributions using gumbel-max\ngumbel_draws = []\n\nfor i in range(1000):\n    samples = (\n        np.log(logits) + np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1) \n\n    gumbel_draws.append(samples) \n\ngumbel_arr = np.array(gumbel_draws)\n\n# ...and 1000 using apply_along_axis + np.random.choice\napply_func_draws = []\n\nfor i in range(1000):\n    samples = np.apply_along_axis(\n        lambda x: np.random.choice(range(len(x)), p=x), \n        1, \n        logits\n    )\n    apply_func_draws.append(samples)\n\napply_func_arr = np.array(apply_func_draws)\n\nIn the above, if you ran the two for loops separately, you would get a better sense of how much faster the vectorized code is. Now well munge these arrays into dataframes, pivot, and feed them to seaborn.\n\ngumbel_df = pd.DataFrame(gumbel_arr[:,:4])\napply_func_df = pd.DataFrame(apply_func_arr[:,:4])\n\ngumbel_df = pd.melt(gumbel_df, var_name = \"distribution\")\napply_func_df = pd.melt(apply_func_df, var_name = \"distribution\")\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 8))\n\np = sns.countplot(data = gumbel_df, x=\"distribution\", hue=\"value\", ax = axs[0])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[0].set_title(\"Using Gumbel-max\")\n\np = sns.countplot(data = apply_func_df, x=\"distribution\", hue=\"value\", ax = axs[1])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[1].set_title(\"Using apply_along_axis + np.random.choice\")\n\nfig.tight_layout()\n\n\n\n\nThe distribution of drawn values should be roughly the same.\n\n\n\n\nEyeballing these, they look similar enough that I feel confident I’ve not messed up somewhere.\nFinally, of note is that a modification of this trick is used a lot in training deep learning models that want to sample from a categorical distribution (Wav2vec(Baevski et al. 2020) and Dall-E(Ramesh et al. 2021) use this). I’ll go over it in another post, but tl;dr, the network learns the probabilities and max is changed to softmax to allow backpropagation."
  },
  {
    "objectID": "posts/use-shift/index.html#pandas.dataframe.shift",
    "href": "posts/use-shift/index.html#pandas.dataframe.shift",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "pandas.DataFrame.shift()",
    "text": "pandas.DataFrame.shift()\nYou could probably just go read the docs on this function, but I’ll try to explain why its useful. We often had to compute lagged differences or ratios for trading data indexed by date. To start I’ll give some solutions that don’t work or are bad for some reason, but might seem like reasonable starts. Lets make our dataframe with a date index to play around with:\n\nimport pandas as pd\n\nmydf = pd.DataFrame(\n    {\"col1\":np.random.uniform(size=100), \n    \"col2\":np.random.uniform(size=100)}, \n    index = pd.date_range(start = \"11/29/2022\", periods=100)\n)\n\nmydf.head()\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n2022-11-29\n0.478292\n0.121631\n\n\n2022-11-30\n0.664101\n0.781843\n\n\n2022-12-01\n0.245395\n0.005426\n\n\n2022-12-02\n0.726935\n0.532795\n\n\n2022-12-03\n0.658744\n0.970972\n\n\n\n\n\n\n\nNow, suppose we want to compute the lag 1 difference. Specifically, make a new series \\(s\\) where \\(s[t] = col1[t] - col2[t-1]: t &gt; 0\\), \\(s[0] =\\) NaN. Naive first attempt:\n\nmydf[\"col1\"][1:] - mydf[\"col2\"][:-1]\n\n2022-11-29         NaN\n2022-11-30   -0.117742\n2022-12-01    0.239969\n2022-12-02    0.194140\n2022-12-03   -0.312228\n                ...   \n2023-03-04   -0.214499\n2023-03-05    0.361584\n2023-03-06   -0.199390\n2023-03-07    0.272564\n2023-03-08         NaN\nFreq: D, Length: 100, dtype: float64\n\n\nUh, so what happened here? Well, pandas does subtraction by index, like a join, so we just subtracted the values at the same dates, but the first and last dates were missing from col1 and col2 respectively, so we get NaN at those dates. Clearly this is not what we want.\nAnother option converts to numpy, this is essentially just a way to move to element-wise addition:\n\nlag1_arr = np.array(mydf[\"col1\"][1:]) - np.array(mydf[\"col2\"][:-1])\nlag1_arr[:5], lag1_arr.shape\n\n(array([ 0.54247038, -0.53644839,  0.72150904,  0.12594842, -0.225511  ]),\n (99,))\n\n\nOf course, this is not the same length as the series, so we have to do some finagling to get it to look right.\n\n# prepend a NaN\nlag1_arr = np.insert(lag1_arr, 0, np.nan)\nlag1_arr[:5], lag1_arr.shape\n\n(array([        nan,  0.54247038, -0.53644839,  0.72150904,  0.12594842]),\n (100,))\n\n\nOk, its the same length and has the right values so we can put it back in the dataframe as a column or create a new series (and add the index again)\n\n# make a new series\nlag1_series = pd.Series(lag1_arr, index=mydf.index)\n\n# or make a new column\n# mydf[\"col3\"] = lag1_arr\n\nAlright, but this looks kinda ugly, we can do the same thing much more cleanly with the .shift() method of pandas DataFrames and Series. .shift(N) does what it sounds like, it shifts the values N places forward (or backward for negative values), but keeps the indices of the series/dataframe fixed.\n\nmydf[\"col1\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    0.478292\n2022-12-03    0.664101\n                ...   \n2023-03-04    0.807256\n2023-03-05    0.992331\n2023-03-06    0.974969\n2023-03-07    0.339215\n2023-03-08    0.625530\nFreq: D, Name: col1, Length: 100, dtype: float64\n\n\nWith this we can easily compute the lag 1 difference, keeping the indices and such.\n\n# difference\nmydf[\"col1\"] - mydf[\"col2\"].shift(1)\n\n# lag 3 ratio\nmydf[\"col1\"]/mydf[\"col2\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    5.976563\n2022-12-03    0.842553\n                ...   \n2023-03-04    0.478158\n2023-03-05    0.635103\n2023-03-06    0.694335\n2023-03-07    0.804248\n2023-03-08    3.747774\nFreq: D, Length: 100, dtype: float64\n\n\nThis lag-N difference or ratio is extremely common and honestly I can’t believe I hadn’t been using .shift() more.\n\n\\(^{[1]}\\)I am not affiliated with GA-Tech beyond the new washing machine and jacuzzi they gave me to advertise their OMSCS program"
  }
]
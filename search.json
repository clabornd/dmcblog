[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Miscellaneous Rambling from Daniel Mabayog Claborne",
    "section": "",
    "text": "I am the father of a delicious sandwich.\n\n\n\npersonal\n\n\n\nAm on leave and lazy to do an actual technical post, so here’s a low-effort life update.\n\n\n\n\n\nOct 15, 2025\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Various Cross-Entropy Implementations and Attention Masks Double Feature\n\n\n\nmachine learning\n\ncode\n\n\n\nWhat the HECK is the difference between various implementations of cross-entropy and transformer padding?\n\n\n\n\n\nJul 17, 2025\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nUnconditional Diffusion With a UNET\n\n\n\nmachine learning\n\ncode\n\n\n\nTrying to rewrite the whole guided latent diffusion thing is hard, so instead here’s some smushy images of butterflies.\n\n\n\n\n\nMay 19, 2025\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nDoubling your pay by doubling your masters degrees\n\n\n\npersonal\n\n\n\nI graduated from Georgia Tech’s Online Master’s in Computer Science and they promoted me straight to the top.\n\n\n\n\n\nJan 6, 2025\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nDiambra Agents on EC2 and Google Colab\n\n\n\nmachine learning\n\ntools\n\n\n\nRun some fighting game environments on an ec2 instance, retrieve experience for training an RL agent from Google Colab.\n\n\n\n\n\nOct 24, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nUsing MlFlow with Minio, Postgres, and Hydra.\n\n\n\nmachine learning\n\ntools\n\n\n\nUsing mlflow with minio and postgresql as storage backends, and hydra to manage configuration and perform sweeps.\n\n\n\n\n\nJun 21, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Updates, Flipping Coins (Bobgate)\n\n\n\npersonal\n\npuzzles\n\n\n\nPersonal updates: PhD applications and OMSCS worries. Bonus: a puzzle about flipping coins.\n\n\n\n\n\nMay 11, 2024\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nLunar Lander with PPO\n\n\n\nmachine learning\n\nreinforcement learning\n\n\n\nI am a sick man who enjoys watching a little lunar module softly touch down on the surface in a simulated environment. See how to do it with PPO.\n\n\n\n\n\nJul 12, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Bagels With GPT-5\n\n\n\nbaking\n\nmachine learning\n\n\n\nSomewhere in this blog I mentioned I would talk about baking? Ok here it is. Look at that bagel, mmmmmm. I got the recipe from GPT-5 which OpenAI has been sitting on since early 2023 and gave me early access to.\n\n\n\n\n\nApr 16, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nAnt-v4 with DDPG\n\n\n\nmachine learning\n\nreinforcement learning\n\n\n\nTraining a ‘spider’ to awkwardly walk to the right using DDPG.\n\n\n\n\n\nApr 13, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nPain, Suffering, Vector-quantized VAEs\n\n\n\nmachine learning\n\n\n\nI’ve written before about both the gumbel-max trick and variational autoencoders. The world demanded a post that combined the two, so here it is.\n\n\n\n\n\nMar 14, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nI Follow a GPT Tutorial\n\n\n\ndata science\n\nmachine learning\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nDumb Mistakes Training Vision Transformer\n\n\n\ndata science\n\nmachine learning\n\ncode\n\n\n\nRead about me fumbling around recreating vision transformer, one of the components of DALL-E/CLIP.\n\n\n\n\n\nJan 1, 2023\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nVectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!\n\n\n\ndata science\n\ncode\n\n\n\nBehind this disaster of a title lies the secret to quickly sample from a categorical distribution in python!\n\n\n\n\n\nNov 28, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nYet Another Explainer on Variational Autoencoders\n\n\n\nmachine learning\n\n\n\nMy attempt at explaining VAE’s in an effort to understand new text-to-image systems.\n\n\n\n\n\nNov 12, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nFTX Blowup and Massive Egos on Twitter\n\n\n\nnews\n\ncrypto\n\n\n\nI found the 2021-2022 crypto hype cycle and subsequent FTX collapse to be fascinating and infuriating. A couple people wrote books about it, I settled for a blog post.\n\n\n\n\n\nNov 9, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\nThe blog lives. Blog blog blog.\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nDaniel Claborne\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/vision-transformer/vision-transformer.html",
    "href": "posts/vision-transformer/vision-transformer.html",
    "title": "Dumb Mistakes Training Vision Transformer",
    "section": "",
    "text": "Another post about one of the pieces of DALL-E, the vision transformer (Dosovitskiy et al. 2021), which is specifically used as one of the vision encoders in CLIP (Radford et al. 2021), which is itself used to condition the decoding process in DALL-E.\nThe vision transformer is at its base really not all that complicated: An image is divided into patches, and each patch is flattened, sent through an embedding layer, and arranged in a sequence to provide the expected input to a stack of multi-headed-attention + position-wise feed-forward layers identical to the architectures used to process text. Somewhat miraculously, with enough data, the vision transformer learns the spatial nature of the input even without the inductive biases of convolutional networks.\nHere’s a link to the colab notebook for training this thing up.\nI did roughly the following:\n\nEssentially copy the transformer encoder architecture from harvard nlp\nPrepend the patch creation and embedding plus learnable positional embeddings\nAdd the classification token and classification head as described in (Dosovitskiy et al. 2021)\n\n\nComponents added to base transformer architecture\nI implemented an image patcher like so:\nimport torch\nimport einops\n\nclass imagePatches(torch.nn.Module):\n  def __init__(self, patch_size=(8,8), input_channels = 3, stride = 8, embedding_dim=768):\n    super().__init__()\n    self.patch_size = patch_size\n    self.unfold = torch.nn.Unfold(patch_size, stride = stride)\n    self.patch_embedding = torch.nn.Linear(\n        patch_size[0]*patch_size[1]*input_channels, \n        embedding_dim\n    )\n\n  def forward(self, img):\n    patches = self.unfold(img)\n    patches = einops.rearrange(patches, \"b c p -&gt; b p c\")\n\n    embeddings = self.patch_embedding(patches)\n    return embeddings, patches\nThe classification head looked like:\n# ... blah blah module super init\n\ndef forward(self, x:torch.tensor, **kwargs):\n    x = encoder(x, **kwargs)\n    # x = Reduce(\"b p c -&gt; b c\", reduction='mean')(x) #\n    # x = self.layernorm(x)\n    x = self.layernorm(x[:,0])\n    x = self.hidden(x)\n    x = torch.tanh(x)\n    x = self.classification(x)\nI switched back and forth between the mean pooling (commented out) and using only the classification token output before going through the classification layer.\nJust to show it, here’s the class embedding + learnable positional embedding inside the encoder class:\nclass vitEncoder(torch.nn.Module):\n  def __init__(self, n_layers, embedding_dim, image_tokenizer, mha_layer, ff_layer, n_patches):\n    super().__init__()\n    self.image_tokenizer=image_tokenizer\n    self.positional_embedding = torch.nn.Parameter(\n        torch.randn((n_patches + 1, embedding_dim))\n    )\n    self.embedding_dim = embedding_dim\n    self.n_layers = n_layers\n    self.encoder_layers = torch.nn.ModuleList([\n        vitEncoderLayer(copy.deepcopy(mha_layer), copy.deepcopy(ff_layer)) for _ in range(n_layers)\n    ])\n    self.class_embedding = torch.nn.Parameter(torch.randn((1, 1, embedding_dim)))\n    \n  def forward(self, x:torch.tensor, attn_mask:torch.tensor=None):\n    x, _ = self.image_tokenizer(x)\n\n    x = torch.concatenate([\n        einops.repeat(self.class_embedding, \"b t c -&gt; (r b) t c\", r = x.shape[0]), x], \n        axis = 1 # patch sequence axis\n    )\n\n    x = x + self.positional_embedding\n\n    for l in self.encoder_layers:\n      x = l(x, attn_mask)\n\n    return(x)\n\n\nTraining\nFirst attempt:\nEpoch 0, Loss is 2.305\nEpoch 0, Loss is 2.367\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.298\n\n...\n\nEpoch 10, Loss is 2.300\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.312\n… shit. Oh right the positional embeddings.\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.309\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.308\n\n...\n\nEpoch 10, Loss is 2.299\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.311\nEpoch 10, Loss is 2.303\n…damnit. Oh whoops I missed a couple activations.\nEpoch 0, Loss is 2.331\nEpoch 0, Loss is 2.299\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.312\n\n...\n\nEpoch 10, Loss is 2.291\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.312\nEpoch 10, Loss is 2.301\n…shit…\n…\n*Reminds self this is an EXERCISE and pain is expected.* I did eventually get this working, but first a couple bugs I found along the way:\n\nTwo very dumb mistakes implementing the layer normalization (After fixing it I switched to just using torch.nn.LayerNorm)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.w = nn.Parameter(torch.ones(features))\n        self.b = nn.Parameter(torch.zeros(features))\n\n    def forward(self, x, eps=1e-6):\n        return self.w * x.mean(-1, keepdim=True) / (x.std(-1, keepdim=True) + eps) + self.b\nYou see it? Yea I’m not actually mean-centering in the numerator there…\n    ...\n    \n    def forward(self, x, eps=1e-6):\n        self.w * (x - x.mean(-1, keepdim=True)/(x.std(-1, keepdim=True) + eps)) + self.b\nAnd here I’ve just fudged the parentheses…squint a bit and you’ll see.\n\nMultiheadedAttention expects sequence dimension first? Apparently if you dont specify batch_first = True:\n\nattention_layer = torch.nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=n_head, batch_first = True)\nThen the MultiheadAttention treats the first dimension as the sequence dimension of the QKV input…this fails silently since the self-attention matrix multiplication is still valid.\n\nLong story short, I did get the model to train up modestly on MNIST (I’ll try on the other two datasets later). You can see my successful training run against all others here:\nIn the end it was using Adam over SGD in training that got me to see proper training. SGD is known to be very picky about the learning rate. I might try some sweeps over various LR’s and report back.\nAnother (probably related) thing I ran into is that averaging all the outputs of the last layer resulted in some learning with SGD, but taking only the classification token output resulted in zero learning. Probably the learning rate(s) I was using were closer to being appropriate for averaging than with taking just the classification token output.\n\n\n\n\n\nReferences\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], June. http://arxiv.org/abs/2010.11929.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv. https://doi.org/10.48550/arXiv.2103.00020."
  },
  {
    "objectID": "posts/ddpg/ddpg.html",
    "href": "posts/ddpg/ddpg.html",
    "title": "Ant-v4 with DDPG",
    "section": "",
    "text": "I’ve had a lot of fun (most of the time) training reinforcement learning algorithms to play various gym environments. There is a somewhat sad satisfaction in getting an agent to push a cart to the top of a hill, land a lunar lander, or balance a pole on a cart - cartpole was the first RL task I tackled, and I am not ashamed to say I was very excited to see a pixelated cart successfully balance a pole.\nFor others (the thousands clearly reading this…) looking to learn, heres a list of resources for RL I’ve found useful:\n\nSutton and Barto’s intro RL book http://incompleteideas.net/book/the-book.html\nOpenAI’s ‘spinning up’ website https://spinningup.openai.com/en/latest/index.html\nGeorgia Tech’s RL course https://www.udacity.com/course/reinforcement-learning–ud600\n\nI took a break from self-learning RL, since I felt I’d gotten most of the low-hanging fruit, but decided to give something a try recently. Last time I looked around the state of libraries for RL was pretty bad, but now the suite of environments in gym is being maintained again, and there are a few good libraries that package up standard algorithms.\nIn this post I’ll look at DDPG, or Deep Deterministic Policy Gradient. It’s goal is to learn a policy for environments with continuous actions. The authors of the paper Lillicrap et al. (2019) note several issues arising from working in discrete action spaces, the main one being that the dimensionality of your action grows exponentially with the number of degrees of freedom — e.g. if your action is the movement of 10 joints, and each joint has 8 possible movements, then your action has dimensionality \\(8^{10}\\). Working with continuous action spaces overcomes this limitation, and they borrow heavily from previous work to train successful agents that output continuous actions on several tasks.\nI’ll use this algorithm to train the Ant-v4 environment from gymnasium[mujoco]. This task asks us to train a quadruped —which I will call a spider in defiance of the name of the environment and the number legs on a spider— to walk to the right by applying torques to 8 joints (the action is a continuous, 8 dimensional vector).\nAlright, so how do we do this? Well, I’m gonna level with you, you should just go read the paper, BUT - the short version is that we train a network \\(Q_\\theta\\) with parameters \\(\\theta\\) to estimate the Q-value at each state-action pair, as well as train a network \\(\\mu_\\phi\\) parametrized by \\(\\phi\\) to learn the policy.\nThe Q-value is updated in the usual manner - by minimizing it’s squared deviation from the boostrapped future rewards, i.e. minimizing \\(\\frac{1}{N}\\sum_{i=1}^{N}(r_i + \\gamma Q_\\theta(s_{i+1}, \\mu_\\phi(s_{i+1})) - Q_\\theta(s_i, a_i))^2\\). We then update the policy network \\(\\mu_\\phi\\) by applying the policy gradient, which was proven by Silver et al. (2014) to be approximated by:\n\\[\n\\frac{1}{N}\\sum_i \\nabla_a Q_\\theta(s = s_i, a = \\mu(s_i)) \\nabla_\\phi \\mu_\\phi(s_i, a_i)\n\\]\nWhere the term inside the sum is an expansion of \\(\\nabla_\\phi Q_\\theta(s, \\mu_\\phi(s))\\) via the chain rule.\nThis reminds me a lot of value iteration, where we first try to update our estimate of the value function, and then ‘chase’ our updated estimates of the value function by updating the policy with respect to those value functions. Usually, that means just updating our policy to take the new max, but here we have an actual policy gradient that flows through the value function.\nThey also augment their training with ideas from Mnih et al. (2015), specifically the replay buffer and target networks. The replay buffer is used to break the correlation between samples which is inherent in the sequential nature of the environment. Basically, we store a queue of &lt;state, action, reward, next state&gt; training tuples, and then randomly sample minibatches from this queue at train time. The target networks are used to stabilize training. If not used, then the network you are updating is also the network you are using to compute the target value, which….just seems wrong? Anyway, training without target networks tends to cause divergence in the updates.\nOkay, so how to we do this? First, go read the paper! Their pseudo-algorithm is actually pretty good and this article is just some guy doing the whole learning by explaining thing while trying to maintain some level of online presence. You’re still here? Fine….we’ll do what I just described. I have the full training script in this notebook. There’s also a bonus mini-example of how to use wandb with ray-tune in there.\n\nActor (policy) and Critic (Q-network)\nOkay, first thing, make two architectures to be the Q network and policy network. These are commonly referred to as the critic (Q network criticizes how ‘good’ a state action pair is) and the actor (the policy network chooses actions to be criticized).\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 512)\n        self.l2 = nn.Linear(512, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = F.relu(self.l1(state))\n        a = F.relu(self.l2(a))\n        return self.max_action * torch.tanh(self.l3(a))\n    \nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n\n        self.l1 = nn.Linear(state_dim, 400)\n        self.l2 = nn.Linear(400 + action_dim, 300)\n        self.l3 = nn.Linear(300, 1)\n\n    def forward(self, state, action):\n        q = F.relu(self.l1(state))\n        q = F.relu(self.l2(torch.cat([q, action], 1)))\n        return self.l3(q)\nYup, those are two MLPs … I guess one thing to note is that the critic takes both state and action as input, but the action comes in at the second layer. Another is that the actor outputs a tanh activation times the max of the action space. Now, we make FOUR NETWORKS! Two extra for the ‘target networks’ that will be used to form…the targets. It sounds ridiculous, but like everything else in machine learning, they tried it and it worked, so were trying it.\ncritic = Critic(env.observation_space.shape[0], env.action_space.shape[0])\nactor = Actor(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n\ncritic_tgt = Critic(env.observation_space.shape[0], env.action_space.shape[0])\nactor_tgt = Actor(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0])\n\nopt_critic = AdamW(critic.parameters(), lr=cfg['critic_lr'])\nopt_actor = AdamW(actor.parameters(), lr=cfg['actor_lr'])\nNotice how I’m not making optimizers for the target networks. They are updated by slowly copying the weights from the main networks, like so:\nfor param, param_tgt in zip(critic.parameters(), critic_tgt.parameters()):\n    param_tgt.data.copy_(cfg['tau']*param.data + (1-cfg['tau'])*param_tgt.data)\n\nfor param, param_tgt in zip(actor.parameters(), actor_tgt.parameters()):\n    param_tgt.data.copy_(cfg['tau']*param.data + (1-cfg['tau'])*param_tgt.data)\nThat is, the weights get updated by \\(\\theta_{tgt} = \\tau * \\theta + (1-\\tau) * \\theta_{tgt}\\), where \\(\\theta\\) is the weights of the main network and \\(\\theta_{tgt}\\) is the weights of the target network.\n\n\nReplay Buffer\nThis is simple enough, we make a queue, and while traversing the environment we store state, action, reward, next state tuples in it, and then sample from it to update our actor and critic. I also fill it with a few thousand random samples before starting training, just to get some initial data in there to sample from. The queue is implemented using collections.deque:\nimport collections\n\nreplay_buffer = collections.deque(maxlen=cfg['buffer_size'])\n\n#...then during training ...\n\nreplay_buffer.append((obs, act, rew, obs_next, done))\nI didn’t mention that we also store whether or not we have completed an episode (done) in the replay buffer. This is needed when forming a target for an action where we only want to consider the reward. Storing done allows us to multiply the bootstrapped part of the value function by 1 - done to zero out the value function if we have completed an episode:\ny = np.array(rew_b) + cfg['gamma']*(1-np.array(done_b))*Q_tgt.numpy()\n\n\nTraining\nOkay, so store some interaction with the environment, and on each step sample from the replay buffer and update things. One thing that may not be obvious is how to update the actor network. Yea I wrote some weird chain rule formula up there but whats the code? It is a little weird, but if you stare at it it makes sense:\nacts_actor = actor(torch.from_numpy(np.array(obs_b)).float())\nQ = critic(torch.from_numpy(np.array(obs_b)).float(), acts_actor)\n\nopt_actor.zero_grad()\nloss = -Q.mean()\nloss.backward()\nopt_actor.step()\nWhats happening here is we send a sample through the actor, pass that action to the critic, and then… call backward() on the loss of -Q? Remember the standard backward() + step() pattern updates the weights to minimize whatever you called backward() on. Here we are calling it on the negative of the Q value, that is we are minimizing the negative value, which is maximizing the positive value — we are applying a gradient to our policy network that results in greater value. When we call step() we only call it on the actor network (at this point in the training loop we’ve already updated the critic). The whole chain rule stuff is taken care of by the fact that the gradients flowed backward through the critic.\n\nHyperparameters\nI do some hyperparameter tuning on things like batch size, replay buffer size, and learning rates. Here’s a link to a group of runs produced by ray-tune and wandb where you can see the variability in training outcomes due to hyperparameters:\n\n\n\n\n\n\nWeird walking spiders…\nHere is a funny walking spider trained via this procedure, the goal is to move to the right:\n\nIt funnily pulls itself along with one leg, which I will call a success, though I’ve seen other solutions where it majestically launches itself to the right like a gazelle.\n\n\n\n\n\nReferences\n\nLillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. “Continuous Control with Deep Reinforcement Learning.” arXiv. https://doi.org/10.48550/arXiv.1509.02971.\n\n\nMnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. https://doi.org/10.1038/nature14236.\n\n\nSilver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, I-387-I-395. ICML’14. Beijing, China: JMLR.org."
  },
  {
    "objectID": "posts/hello-word/index.html",
    "href": "posts/hello-word/index.html",
    "title": "The blog lives. Blog blog blog.",
    "section": "",
    "text": "A test of the new website, deployed on github pages. Errr, I guess this post can be about how I did it? Look, just follow this tutorial okay.\nHere’s some python code I guess?\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter([1,2], [1,2])\nfig.show()"
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html",
    "href": "posts/mlflow-hydra/mlflow-hydra.html",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "",
    "text": "PROGRAMMING NOTE: My post on running diambra reinforcement learning training from Colab is still coming, but might be posted elsewhere.\nUPDATE: This post discusses running the storage (postgres and minio) in docker containers and the mlflow UI from the command line. The post references a github repo at this commit. I’ve since edited the associated github repo to run the mlflow UI in a docker container as well from docker-compose, inspired by this tds post. The below should still work, but take a look at that post and the repo for the full docker setup."
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html#extra-options-with-hydra",
    "href": "posts/mlflow-hydra/mlflow-hydra.html#extra-options-with-hydra",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "Extra options with hydra",
    "text": "Extra options with hydra\nHydra allows us to manage configuration options in a more organized way. In this simple example, I have under cfg a config.yaml file that specifies the parameters for the training script. One is the experiment name to be displayed under the mlflow UI, and the other is the config group ‘model’, which allows me to specify what kind of model I want to train. The parameters of these models are stored in further yaml files under cfg/model. They contain the actual sklearn class or ‘target’ that I want to use to construct the model object as well as the parameters to pass to the constructor. For example, cfg/model/randomforest.yaml looks like this:\n_target_: sklearn.ensemble.RandomForestClassifier\nn_estimators: 100\nmax_depth: 10\nrandom_state: 42\nUnder the ‘top level’ config.yaml file, this is the default:\ndefaults:\n  - model: randomforest\n\nexperiment_name: mlflow-practice\nNote that I do not include the file type in the model parameter. See the hydra docs for more about the defaults list.\nSuppose I wanted to train an SVM instead of a random forest, I have another config for SVM in cfg/model/svm.yaml:\n_target_: sklearn.svm.SVC\nC: 1.0\ngamma: scale\nkernel: rbf\nI can specify the model to use with the model parameter in the command line:\npython src/basic-example.py model=svm\nOr change the value of one of the model parameters:\npython src/basic-example.py model=svm model.C=0.1\n\n# random forest\n\npython src/basic-example.py model=randomforest model.n_estimators=50\nOr even add a parameter that is not in the default config:\n# add the degree parameter to the svm model\npython src/basic-example.py model=svm model.C=0.1 model.kernel=poly +model.degree=4"
  },
  {
    "objectID": "posts/mlflow-hydra/mlflow-hydra.html#hyperparameter-sweeps",
    "href": "posts/mlflow-hydra/mlflow-hydra.html#hyperparameter-sweeps",
    "title": "Using MlFlow with Minio, Postgres, and Hydra.",
    "section": "Hyperparameter sweeps",
    "text": "Hyperparameter sweeps\nYes yes, this is an sklearn example and you should just be using GridSearchCV or RandomizedSearchCV (in fact, with mlflow.autolog() mlflow will automatically log child runs of the main run) but I wanted to show how you can use hydra to perform multiple runs.\nFirst the easy way with the -m flag. Here we sweep over 3 values of the C parameter for the SVM model:\npython src/basic-example.py -m model=svm model.C=0.1,1,10 model.kernel=rbf\nYou can also specify a sweep config under the main config.yaml file. For example, I can specify a sweep over the C parameter for the SVM model like this in the top level config:\ndefaults:\n  - model: randomforest\n\nexperiment_name: mlflow-practice\n\nhydra:\n  sweeper:\n    params:\n      model.C: 0.1,1,10\nAnd then run the script without the -m flag:\npython src/basic-example.py -m model=svm\nThis of course hard-cores the sweep configuration, we might like to have modular sweep configs. We can put further yaml files under cfg/sweep that specify the parameters to sweep over. For example, cfg/sweep/svm.yaml:\n# @package _global_\nhydra:\n  sweeper:\n    params:\n      model: svm\n      model.C: 0.1,1,11\nThe hydra/sweeper/params group is a special hydra group for performing sweeps. Here we specify the @package directive to notify hydra that we’re editing the config at the top level. We can then run the script with the sweep config like this:\npython src/basic-example.py -m +sweep=svm\nIf you go to the mlflow or minio UI after all this you should see all these runs populated, and sortable by parameters and performance metrics:"
  },
  {
    "objectID": "posts/vqvae/vqvae.html",
    "href": "posts/vqvae/vqvae.html",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "",
    "text": "I’ve written before about both the gumbel-max trick and variational autoencoders. The world demanded a post that combined the two, so here it is. I mostly follow the repo for the DALL-E paper Ramesh et al. (2021). They also use the attrs package, which was nice to learn about, kinda neat, but a bit opaque.\nAs usual, I hope to entertain you with some of my mistakes and general troubles. If you are a sick, sick individual and came here to hear about taking the log of 0, I will happily indulge you. Here’s a Colab notebook if you’re on a similar learning journey and just need to see some code. Mega-disclaimer that I am just some dude, use at your own risk."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#categorical-vaes",
    "href": "posts/vqvae/vqvae.html#categorical-vaes",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "Categorical VAE’s",
    "text": "Categorical VAE’s\nThe general idea of categorical VAE’s is that our encoder learns the probabilities of a categorical distribution with \\(K\\) categories. These probabilities are used to sample from one of \\(K\\) vectors in a codebook (a collection of vectors). These sampled codebook vectors are then fed into a decoder to try to reconstruct the input.\nOptimization is done in the usual way, by maximizing the evidence lower bound (ELBO, see my VAE post or other resources for details): \\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\n\\end{align}\n\\]\nThe first term on the RHS (the reconstruction objective) is usually defined as a Gaussian or Laplace distribution, and can be maximized with the \\(l2\\) or \\(l1\\) loss. In the paper, they use a different distribution called the logit-laplace distribution, with the reasoning that it models the distribution of pixel values better, specifically that they lie in a bounded range. The logit-laplace distribution is defined as:\n\\[\nf(x, \\mu, b) = \\frac{1}{2bx(1-x)}\\exp\\left(-\\frac{|logit(x) - \\mu|}{b}\\right)\n\\]\nThey use the log of the RHS as the reconstruction objective, with the decoder outputting 6 channels per pixel location (3 \\(\\mu\\)’s and 3 \\(b\\)’s for each pixel). I couldn’t tell you why this is better than assuming the inputs (between 0 and 1) are the probabilites of Bernoulli distributions, perhaps it is more flexible? But their concern of a bounded range is just as well taken care of by outputting values in the range \\([0, 1]\\) and using cross entopy with logits against the input image. In fact, I do this against MNIST (yes, its MNIST again, gimme a break ok).\nThe second (KL) term of the ELBO in the case of categorical VAE’s is comparing our encoder’s outputted distribution with a uniform categorical distribution over the \\(K\\) classes, which is easily calculated as:\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = \\sum_{k=1}^K q(k\\vert x;\\phi) \\log\\left(\\frac{q(k\\vert x;\\phi)}{1/K}\\right)\n\\]\nsince we assume \\(p(z) = 1/K\\) for all \\(z\\)."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#the-gumbel-softmax-trick",
    "href": "posts/vqvae/vqvae.html#the-gumbel-softmax-trick",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "The Gumbel-(soft)Max Trick",
    "text": "The Gumbel-(soft)Max Trick\nOk, if you remember in VAE’s we have to deal with this whole non-differentiable thing, since our process goes:\n\nEncode our input to the probabilities of a categorical distribution\nSample from that distribution and use the sample to select a vector from a codebook.\nDecode the sample to try and match the input\n\nand we cant backpropagate through 2. This is handled by using a relaxation of the Gumbel-max trick which if you’ll recall, is a way to sample from a categorical distribution by taking the arg-max of the log of the probabilities plus noise from a Gumbel(0,1) distribution. Arg-max isn’t differentiable, so we use softmax (which is more accurately described as soft-arg-max) as a differentiable approximation. We can adjust the temperature of the softmax operation to more closely approximate the arg-max operation.1\nprobs = self.encoder(x) # B x K x H x W\ngumbel_noise = gumbel_sample(probs.shape).to(probs.device)\n\n# apply softmax to log(probs) + gumbel noise divided by temperature tau.\n# very small tau essentialy makes this one-hot\nz = torch.nn.functional.softmax((probs.log() + gumbel_noise)/tau, dim=1) \n\n# 'soft-samples' from the vector quantized embeddings\nz = torch.einsum(\"bchw,cv -&gt; bvhw\", z, self.embedding)\n\n# reconstruct to B x C x H x W\nx_reconstr = self.decoder(z)\nNotice that the probabilities are a B x vocab_size x H x W feature map. So we are sampling a vector from the codebook at each location in this feature map, or rather we are approximating sampling from it by taking a weighted (where the weights sum to one) combination of the codebook vectors, where one weight is very large is the rest are very small (due to the softmax operation). Once we have the reconstructed image and the probabilities of the categorical distribution, we can calculate the reconstruction loss and the KL divergence. The reconstruction loss is just:\nreconstr_loss = nn.functional.binary_cross_entropy_with_logits(xrecon, x, reduction=\"mean\")\nOr it would be the logit-laplace loss if I had implemented that:\ndef laplace_loss(x, mu, b):    \n    loss = -torch.log(2*b*x*(1-x)) - torch.abs(torch.logit(x) - mu)/b\n\n    return loss\nThe KL divergence is as defined above, and can be calculated like so:\ndef KL_loss(logits, vocab_size=4096):\n  loss = logits * (logits.log() - torch.log(torch.ones_like(logits)/vocab_size))\n\n  # B x C x H x W\n  return loss\n…well, really I should be summing over the channel axis (KL definition has a sum), I do it in the notebook outside the function. As we’ll see later though, whether you sum or average is kinda arbitrary, because it seems empirical results suggests multiplying the KL loss by some constant is a good idea.\n\nMistakes were made\n1) My first attempt was on the STL-10 dataset, which I leave code for in the notebook, but I couldn’t get the model to output good reconstructions. An example of one of the reconstructions:\n\n\n\n\n\n\nFigure 1: Left: Hi I’m a bird. Right: alks3lkj3olkdfffff\n\n\n\n2) In the paper, they describe dividing the pixel values by 255, when performing preprocessing steps necessary to prevent divide-by-zeros in the logit-Laplace loss, which I gladly replicated without checking if my images were already 0-1 normalized. Funilly enough, it seemed to learn better than the non-normalized version, perhaps this is a clue to why I can’t get good quality samples from STL-10.\n3) I had switched to cross-entropy loss, and was doing okay, until I got divide-by-zero errors. You see, I am calling softmax twice, once to do the Gumbel-softmax sampling, but also once before, in the encoder, to form the probabilities that go into the gumbel-softmax sampling. Remember we take the log of those values and then add Gumbel noise, so they all better be positive. Well, you might think that the softmax equation:\n\\[\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n\\]\nwould always produce positive values, but there’s this thing called numerical underflow which is a real pain in the ass, the ass of Daniel Claborne trying to take the logarithm of things, that is. Well, I just add a small constant and divide by the appropriate constant to make them still sum to 1 along the channel axis:\n# forward method of the encoder\ndef forward(self, x):\n    x = self.blocks(x)\n    x = torch.nn.functional.softmax(x, dim = 1)\n\n    x = (x + 1e-6)/(1 + self.vocab_size*1e-6)\n\n    return x\n4) I underestimated how much I would need to increase the importance of the KL term. In the paper, they multiply the KL loss term by \\(\\beta\\) which is increased from 0 to 6.6 over some number of iteration. My initial attempts that stopped at \\(\\beta = 2\\) produced poor sample quality, so I just tried their suggested \\(\\beta\\)\n\n\nSome success with MNIST\nOk, I was defeated by STL-10 (I’ll try again once Colab gives me some more compute credits), but good ol easy-mode MNIST gave me some nice results. I load my best test loss model from https://wandb.ai/clabornd/VQVAE/runs/9bb6w3ru?workspace=user-clabornd and use it to generate some samples.s\nFirst, we see if it can reconstruct an image that is passed to the encoder. Intuitively the encoder should map the image \\(X\\) to a latent representation \\(z\\) that is likely to produce something similar to \\(X\\) in the output, and so it does as seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Left: Input. Right: decoded output\n\n\n\nBut remember we not only wanted to be able to produce an image when providing an image, but be able to produce images when sampling from random noise. This is the point of the KL loss term, making the latent representation close to a uniform categorical distribution. We should then be able to sample from a uniform categorical (for each pixel location in a latent feature map), pass this sample to our decoder, and get things that look like our training data. And so we do (sorta):\n\n\n\n\n\n\nFigure 3: Output of the decoder when fed feature maps of codebook vectors sampled from a uniform categorical distribution\n\n\n\nWow, what a bunch o beauts’. Hopefully I can get you some pictures of slightly less blurry birds/airplanes soon."
  },
  {
    "objectID": "posts/vqvae/vqvae.html#footnotes",
    "href": "posts/vqvae/vqvae.html#footnotes",
    "title": "Pain, Suffering, Vector-quantized VAEs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the paper they start with temperature \\(tau = 1\\) and reduce it to \\(\\frac{1}{16}\\) over some number of iterations - I do as well in the notebook.↩︎"
  },
  {
    "objectID": "posts/variational-autoencoder/index.html",
    "href": "posts/variational-autoencoder/index.html",
    "title": "Yet Another Explainer on Variational Autoencoders",
    "section": "",
    "text": "For a while, I’ve been struggling to understand variational autoencoders (VAE’s) at a satisfactory level. An initial pass produced a bit of understanding, but I got sucked back in when I tried to understand Dalle-E, which led me to try to understand diffusion models, which directed me back to going over the techniques used in variational autoencoders again. Some day I will write a Johnny-come-lately post about the different components of Dall-E, but today is about VAE’s.\nBefore I jump into things, my default disclaimer that I am probably not someone who should be writing authoritative-sounding articles about VAE’s - this is an exercise in understanding through explanation. Also, I think its just generally useful to have as many angles at explaining something available as possible – the sources I used to understand VAE’s had great diversity in the ways they explained recurring topics and how much to delve into certain pieces of the puzzle. Here are some resources that I have found useful and are probably more trustworthy (maybe mine is more humorous?):\n\nPaper by Kingma and Welling\\(^{[1]}\\)\nThis tutorial by some people at UC-Berkeley and Carnegie Mellon\\(^{[2]}\\)\nThis blog post\\(^{[3]}\\)\nThis other blog post\\(^{[4]}\\)\n\nMuch of the material here is going to be a re-hash of what has been presented in these. If you notice something wrong, please submit a pull request or open an issue in the git repo for this blog to correct my mistake.\n\n\nSetup\nAlright, so variational autoencoders are cool, they can produce synthetic samples (images) that are realistic and serve as great material for articles and Reddit posts about how ‘none of these faces are real!’ and how we will soon all live in an identity-less, AI-controlled dystopian nightmare.\n\n\nThis person doesn’t exist\n\nHow do VAE’s help us get there? One thing that [1] and [2] do is to initially put any sort of deep learning architectures aside and just focus on the general variational inference problem. At first I found this very annoying (get to the point!), but now think it is probably useful (yea yea, the people who are way smarter than me were right and I was wrong, who would have thought).\nThe setup is that we have some real observations, \\(X_{obs} = \\{x^{(1)}, x^{(2)}, ... x^{(N)}\\}\\) (When I talk about some arbitrary sample from the observed data, i’ll drop the superscript and just say \\(x\\)) that we assume are generated from some process that goes like:\n\nA random variable \\(z \\in \\mathcal{Z}\\) is drawn from some distribution \\(p(z\\vert\\psi)\\) with parameters \\(\\psi\\).\n\\(x^{(i)} \\in \\mathcal{X}\\) are generated through a conditional distribution \\(p(x\\vert z;\\theta)\\) with parameters \\(\\theta\\)\n\nThe \\(x^{(i)}\\) could be anything from single values to something very high dimensional like an image. We seek to maximize the likelihood of our data under the entire generative process:\n\\[p(x) = \\int p(x\\vert z; \\theta)p(z\\vert\\psi)dz\\]\nHowever there are some issues, the true values of \\(\\theta\\) and \\(\\psi\\) are not necessarily known to us. We also cannot assume that the posterior \\(p(z\\vert x;\\phi)\\) is tractable, which we will see is important later. Here we can note that we have the two pieces that correspond to the ‘encoder’ and ‘decoder’ pieces of the problem. \\(p(x\\vert z;\\theta)\\) is like a decoder, taking the hidden \\(z\\) and turning it into the observed \\(x\\). \\(p(z\\vert x;\\phi)\\) is like an encoder, taking some input \\(x\\) and produces a hidden representation \\(z\\) that was likely given that we observed that \\(x\\). A graph of the whole process including the ‘encoder’ and ‘decoder’ pieces in shown below.\n\n\n\n“Plate diagram” with solid arrows representing \\(p(x\\vert z; \\theta)\\) and dashed arrows representing \\(p(z\\vert x;\\phi)\\). From Kingma and Welling (2014)\n\n\nOk now I’ll try to make the jump to neural networks (you can stop scrolling). This was one of the hardest parts of understanding vae’s for me – it was hard to get used to the idea of networks that produced distributions. The notation of using \\(p(...)\\) to refer to both a distribution under the variational Bayes framework and a neural network that parametrizes that distribution can be a bit confusing, but try to get comfortable switching between the two views. The important part is that \\(p(x\\vert z;\\theta)\\) can be approximated by a neural network. We’ll see soon that \\(p(z\\vert x;\\phi)\\) can be as well, and I’ll explain why we need it to be.\n\nLets start with \\(p(x\\vert z;\\theta)\\). First some assumptions are made:\n\n\\(p(x\\vert z;\\theta)\\) comes from a distribution such that it can be approximated/parametrized by a differentiable function \\(f(z; \\theta)\\)\n\\(z\\) is drawn from some probability distribution, often an isotropic Gaussian \\(p(z) = N(0, I)\\)\n\nThe first assumption is simply so we can perform gradient descent given a sampled \\(z\\) and optimize the likelihood of \\(x\\) given that \\(z\\). Here is where we have our neural network that produces a distribution. One common way is for \\(f(z; \\theta)\\) to take the form of encoding the mean of an isotropic normal: \\[p(x\\vert z; \\theta) = \\mathcal{N}(X\\vert f(z;\\theta), \\sigma^2_x*I)\\] Ok, I usually have to stop here…how does this help us? Well, we now have a network that will output a distribution from which we can calculate a likelihood for any given \\(x\\), and it is differentiable, such that we can edit the parameters \\(\\theta\\) through gradient descent to maximize the likelihood of all \\(x \\in X_{obs}\\). Having this ‘decoder’ network represent a distribution is also necessary when fitting it into the objective function later. From now on when I refer to \\(p(x\\vert z; \\theta)\\), I’ll be simultaneously talking about the distribution, and the network that produces that distribution.\nI think it is worthwhile to consider what maximizing this likelihood looks like in a real example. Suppose the \\(x^{(i)}\\) are greyscale images, what should \\(f(z; \\theta)\\) output to maximize the likelihood of a given \\(x\\)? Intuitively (and mathematically) it should output a mean vector with each element corresponding to a pixel in \\(x\\) and having the same value as that pixel – that is, the multivariate Gaussian it outputs should be directly centered over the multivariate representation of the image. One can also consider other forms of \\(x\\) and output distributions that make sense, such as if \\(x\\) is from a multivariate bernoulli, and \\(f(z; \\theta)\\) would then output the probability vector \\(p\\), maximizing the likelihood by having elements of \\(p\\) as close to 1 as possible for corresponding 1’s in \\(x\\) (and close to zero for zeros).\nThe second assumption is a bit weirder, and took me a while to get comfortable with. Essentially it is very dubious to try to handcraft a distribution for \\(z\\) that represents some informative latent representation of the data. Better to let \\(f(z; \\theta)\\) sort out the random noise and construct informative features through gradient descent. Later, when we get to the encoder/objective, we’ll also see that having \\(z\\) be \\(N(0, I)\\) is convenient when computing a component of the objective.\n\nAt this point, we could go ahead and try to train our decoder to maximize the likelihood of our observed data. We can estimate \\(p(x)\\) by drawing a whole bunch of \\(z\\) and then computing \\(\\frac{1}{N}\\sum_{i=1}^{N} p(x\\vert z_i;\\theta)\\) for each \\(x^{(i)}\\) and maximizing \\(log(p(X_{obs})) = \\sum_i log(p(x^{(i)}))\\) through gradient descent. However as mentioned in [2] we may need an unreasonable number of samples from \\(z\\) to get a good estimate of any \\(p(x)\\). This seems to be a problem with the complexity of the observed data, and how reasonable it is that \\(x\\) arises from \\(N(0, I)\\) random noise being passed through a function approximator:\n\nUs: Hey! we need you to turn this random noise in \\(\\mathbb{R}^2\\) into more of like….a diagonal line.\nModel: Yea sure I can kinda learn to move stuff in the second and fourth quadrants to the first and third quadrants.\n…\nUs: Hi, good job on the diagonal thing, now we need you to make some pictures of numbers from the noise.\nModel: What, like….images? Of digits? Christ man thats really high dimensional, how do I even…I mean i’ll give it a try but this is sort of a stretch.\nUs: Go get em!\n…\nUs: Hi champ, back again. We need you to recreate the diversity of pictures of human faces from the same random noi…\nModel: *explodes*\n\nAnother intuition I have for why this does not work, is that how does \\(f(z; \\theta)\\) decide to what distributions to map certain regions of the latent space \\(\\mathcal{Z}\\)? In the example of generating images of digits, it has to balance maximizing the probability of all digits. However if there is a group of digits (say, all the twos) that are very different from the rest of the digits than the rest of the digits are from each other, then maximizing \\(p(X_{obs})\\) might involve simply not mapping any region of \\(\\mathcal{Z}\\) to a distribution that odd digit is likely under – not a model we want to end up with. It helps then to define, for a given \\(x\\), regions of \\(\\mathcal{Z}\\) that were likely given we observed that \\(x\\).\n\n\nMaking things easier with an encoder\nI mentioned we would need to consider the posterior \\(p(z\\vert x;\\phi)\\) – it directly addresses the problem I just brought up: that we need to know what \\(z\\) values are likely given we observed a particular \\(x\\). We now develop our ‘encoder’, which again will map \\(x\\) to distributions over \\(\\mathcal{Z}\\).\nOne reasonable question is why do we want to map to a distribution? Why don’t we just map to specific points in \\(\\mathcal{Z}\\) that are most likely to produce \\(x\\). Well, remember that we want to be able to generate examples by sampling from \\(\\mathcal{Z}\\), and it is highly unlikely that a regular old encoder will map \\(x\\)’s to \\(z\\)’s in such a way that is ‘smooth’ or ‘regular’ enough such that we can do so.\nConsider some \\(z^{(i)}\\) that is likely to have produced a \\(4\\). The encoder is under no constraint to make some other \\(z^{(j)}\\) that is very close to \\(z^{(i)}\\) also likely to produce a \\(4\\). Similarly, given two regions of \\(\\mathcal{Z}\\) that produce, say, \\(1\\)’s and \\(7\\)’s, as we move from one region to the other, the model is under no obligation to smoothly change from decoding to \\(1\\)’s to decoding to \\(7\\)’s. [4] explains this concept very well. Another way to think about it is that this does not really match our initial definition of the generative process of \\(z\\)’s randomly occurring and producing \\(x\\)’s from some conditional distribution \\(p(x\\vert z)\\). We could make it fit, but \\(p(z)\\) would be some very wacky distribution with completely unknown form. So, it makes sense to encode the \\(x\\) to well behaved distributions over \\(z\\) as well as consider penalizing the encoder for producing sets of these conditional distributions \\(p(z\\vert x)\\) which are far apart, so that we can sample the latent space and reasonably get a high-quality example of an \\(x\\). We’ll see how this is done in the next section.\nTo wrap up this section, \\(p(z\\vert x)\\) will be approximated in a very similar way as \\(p(x\\vert z;\\theta)\\) – it’s parameters are determined by a neural network that takes in \\(x\\) and outputs the parameters of a distribution, in this case, the mean and variance vectors of an isotropic normal distribution:\n\\[q(z\\vert x; \\phi) = \\mathcal{N}(Z \\vert \\mu(x;\\phi), \\mathbf{\\Sigma}(x;\\phi))\\]\nWhere \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) are the mean vector and covariance matrix for the isotropic normal distribution output by our encoder network with parameters \\(\\phi\\) when fed \\(x\\) as input (in practice the network just outputs the diagonal elements for \\(\\mathbf{\\Sigma}(x;\\phi))\\), since the off-diagonals are forced to be zero). When referring to \\(q(z\\vert x; \\phi)\\) I’ll be talking both about the distribution induced by the network that outputs \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) and the network itself.\n\n\nThe objective\nSo, we have an encoder \\(q(z\\vert x; \\phi)\\) and decoder \\(p(x\\vert z;\\theta)\\), now all that is left is to train them. So we have to find an objective that incorporates them, as well as satisfies our goal of maximizing \\(p(X_{obs})\\) under a generative process with a \\(z\\) we can reasonably sample from and produce realistic examples.\nThere is a lot of variation (man…I think I missed the pun on this one earlier) in tutorials about how they arrive at the loss function. I’m working backwards from the definition in Kingma and Welling (2014) for the reason that the starting point is well motivated: We want to maximize \\(p(x)\\) under the generative process; can we find a differentiable form related to \\(p(x)\\) that includes our encoder and decoder structures?\nAnother common approach starts with the motivation of trying to optimize \\(q(z\\vert x; \\phi)\\) to match the intractable posterior \\(p(z\\vert x)\\), but I found it hard to make the logical leaps as to how someone could reasonably start here. That way is maybe algebraically easier to get to what we want, but I like how starting with \\(p(x)\\) and rewriting it feels more intuitively motivated.\nThe way I’ll get our encoder and decoder into the definition of \\(p(x)\\) is by doing a bit of the ol add zero trick:\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(z \\vert x))] + log(p(x))\\]\nYes, I am just taking the expectation of zero in there. I’m taking the log of \\(p(x)\\) because we’ll need log-everything on the right side and maximizing \\(log(p(x))\\) will also maximize \\(p(x)\\). As a bonus, we have the form of our encoder in the equation, great! (GREAT!) Notice that the expectation is over \\(z\\)’s drawn from \\(q(z\\vert x;\\phi)\\), that is, to approximate this expectation we would sample from \\(q(z\\vert x;\\phi)\\) by providing an \\(x\\) and sampling from the conditional distribution. Ok, now we need to get the decoder in there. I’ll move the \\(log(p(x))\\) inside the expectation (legal since it doesn’t depend on \\(z\\)) and then rewrite using the equality \\(log(p(z\\vert x)p(x)) = log(p(x,z)) = log(p(x\\vert z)p(z))\\):\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(x \\vert z;\\theta)) + log(p(z))]\\]\nYay! Theres our decoder! (Dont doubt me, I really am that excited as I write this). Now what we can do is collapse things into Kullback-Leibler divergences - specifically any +/- pair of log probabilities dependent on \\(z\\) can be rewritten as a KL divergence:\n\\[\nlog(p(x)) = KL(q(z\\vert x;\\phi) \\vert\\vert p(z \\vert x)) - KL(q(z\\vert x;\\phi)\\vert\\vert p(z)) + E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\label{eq1}\\tag{eqn-1}\n\\]\nHrm, we have that pesky intractable \\(p(z \\vert x)\\) in there. Thankfully, we can focus on the other terms and simply rewrite/rearrange using the fact that KL-divergence is non-negative:\n\\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\label{eq2}\\tag{eqn-2}\n\\end{align}\n\\]\nThe right hand side is known as the evidence lower bound (ELBO), and various derivations of it and the above inequality can be found all over the internet if you found the above unsettling, disturbing, offensive.\nIt is useful to stare a bit at (\\(\\ref{eq1}\\)) and (\\(\\ref{eq2}\\)). First, if our \\(q(z\\vert x;\\phi)\\) eventually ends up matching \\(p(z \\vert x)\\), then the first KL divergence in (\\(\\ref{eq1}\\)) will be zero, and maximizing the RHS of (\\(\\ref{eq2}\\)) will be like directly maximizing \\(p(x)\\) [2].\nThe other notable thing is that (\\(\\ref{eq2}\\)) has a nice interpretation. \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\) can be though of as the reconstruction loss - how close are our reconstructions to the data. \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\) is like a regularization term telling our encoder: “You must try to be like the prior distribution \\(p(z)\\)”. This regularization term achieves the goal of making sure the conditional distributions across the \\(x\\) are nice and compact around the prior distribution - so if we sample from a \\(\\mathcal{N}(0,I)\\), we are likely to get a \\(z\\) that is likely to produce one of our \\(x\\)’s.\n\n\nMaximizing the objective through gradient descent\nSo, uhm…is (\\(\\ref{eq2}\\)) differentiable? Nope. Tutorial over, you’ve been had, AHAHAHAHA!\nJoking aside it actually isn’t currently amenable to backpropagation but we will get around that in a second. So remember we have two things we need to compute:\n\nThe regularization term \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\)\nThe reconstruction term \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\)\n\nFor 1, when the two distributions in the KL divergence are Gaussian, then there is a nice closed form solution that reduces nicely when, as in our case, the second distribution is \\(\\mathcal{N}(0, I)\\):\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = KL(\\mathcal{N}(Z \\vert \\mu(x;\\phi), \\Sigma(x;\\phi))\\vert\\vert \\mathcal{N}(0,I))\n\\] \\[\n= \\frac{1}{2}(tr(\\Sigma(x;\\phi)) + \\mu(x;\\phi)^T\\mu(x;\\phi) - k - log(det(\\Sigma(x;\\phi))))\n\\]\nWhere remember \\(\\mu(x;\\phi)\\) and \\(\\Sigma(x;\\phi)\\) are our mean vector and covariance matrix computed by our encoder. Nice, this is a value, it is differentiable with respect to the parameters \\(\\phi\\), great.\nFor 2, note that when we compute gradients, we move the gradient inside the expectation, and are just computing a gradient from a single example of \\(z\\), drawn from \\(q(z\\vert x;\\phi)\\) given our input \\(x\\) at train time (and over many gradient computations, we have well approximated maximizing the expectation over \\(z\\sim q(z\\vert x;\\phi)\\)). However we have a bit of a problem. Again notice the expectation is over \\(z\\) sampled from \\(q(z\\vert x;\\phi)\\), therefore maximizing the expression inside the expectation depends not only on updating \\(p(x \\vert z;\\theta)\\) to perform reconstruction well, but on updating \\(q(z\\vert x;\\phi)\\) to produce distributions that output \\(z\\)’s that \\(p(x \\vert z;\\theta)\\) finds easy to decode.\nOk, so no big deal? Just update both networks based on the reconstruction loss? This won’t immediately work because our training looks like:\n\nSend \\(x\\) through \\(q(z\\vert x;\\phi)\\) to get a distribution over \\(z\\)’s\nSample a \\(z\\) from that distribution\nSend \\(z\\) through \\(p(x \\vert z;\\theta)\\) to produce \\(x\\) and compute the reconstruction loss\n\nThe problem is that we cannot backpropagate through step 2 (‘sample from a distribution’ is not a math operation we can take the gradient of). The solution is what is known as the reparametrization trick. Instead of sampling directly from the distribution inferred by the output of our encoder, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and then multiply/add… :\n\\[\\epsilon*\\sqrt{\\Sigma(x;\\phi)} + \\mu(x;\\phi)\\]\n…to mimic sampling from the implied distribution. The difference being that in this case we are just adding and multiplying by constants, things which backpropogation is fine with (even though those constants were obtained from sampling).\nNow we can compute gradients for batches of \\(x\\) and average the gradients in the normal fashion to train the model."
  },
  {
    "objectID": "posts/init-bagels/bagels.html",
    "href": "posts/init-bagels/bagels.html",
    "title": "Making Bagels With GPT-5",
    "section": "",
    "text": "I’ve made bagels for a long time, usually bringing them into the office and leaving them out in the lunch-room for co-workers, interns, and others to eat them. If you’d just like to have the recipe and be on your way it is Peter Reinhart’s New York Style Bagels With Wild Sourdough. I use molasses instead of barley malt, and ~30g less flour since I found 4 cups to make them too tough — the gears on my KitchenAid were eventually stripped from mixing the dough. I’ve had it from at least one actual New Yorker that they pass the test.\nI would often anonymously leave them in there and pat myself on the back for so selflessly sharing the gift of baking with people without need for thanks and praise. Of course this was inwardly dishonest, as I was going for the having my-cake-and-eating-it-too situation of everyone knowing who provided the bagels, but me not putting too much effort into having it be known such that I can claim pure intentions.\nAnyway, the pandemic obviously killed the bagel sharing situation in the office. Too bad too since I was on a long streak of having bagels in the break room every Monday morning. For a while I tried to deliver to co-workers houses, distribute to neighbors etc., but did not move nearly the same volume as before. Once I moved out of my apartment and started taking classes again the bagel production went to essentially zero.\nRecently I’ve tried to find time to make bagels again and have fixed a problem that plagued me previously - overproofing. Essentially the recipe is from a professional baker who has a walk in fridge that maintains a super consistent low temperature and can leave the dough in there for 24 hours, so I cut the proofing time to ~10 hours overnight, which seems to consistently produce a nice product. Anyway, here they are:\n\n\nFigure 1 shows histograms of the three color channels for the single bagel image. From the red color channel, we can see that the bagel is very tasty.\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# load an image\nimg = np.array(Image.open('bagel-single.jpg'))\n\n# histogram of the three color channels\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\naxs[0].hist(img[:,:,0].flatten(), bins=256, color='r', alpha=0.5);\naxs[1].hist(img[:,:,1].flatten(), bins=256, color='g', alpha=0.5);\naxs[2].hist(img[:,:,2].flatten(), bins=256, color='b', alpha=0.5);\n\n\n\n\n\n\n\nFigure 1: Histograms of the three color channel intensities of the single bagel image.\n\n\n\n\n\nIf you were invested in the GPT-5 part of the title I am very sorry."
  },
  {
    "objectID": "posts/update-2024/index.html",
    "href": "posts/update-2024/index.html",
    "title": "Personal Updates, Flipping Coins (Bobgate)",
    "section": "",
    "text": "Havent posted in a while, this is gonna be mostly a personal post to describe what I’ve been up to and practice regularly updating this blog so it doesn’t go stale. If you shockingly don’t care about the details of some random internet person’s life and just wanna read about an interesting puzzle, skip to Bobgate.\n\nPhD Applications\nI applied to about a dozen computer science PhD programs for Fall 2024, focusing on programs with good artificial intelligence labs preferably doing RL research. I aimed high and was rejected across the board. I was somewhat expecting it, given the competitiveness of the programs I applied to and the current hype around AI. There is a lot of doomerism about CS PhD admissions on Reddit, with people saying there’s high schoolers with AI/ML publications in top conferences, so the competition is incredibly strong. No doubt some of this is just the bias of the most worried people posting on Reddit, but probably there is some truth to it. Of course I’m still a bit disappointed, but am trying to shift focus back to work and finishing my masters in OMSCS, which I’m now two classes away from completing. For anyone interested, my list of schools and general application profile were:\n\nSchools: UW, UMD, UMass Amherst, Northeaster, Northwester, Oregon State, Penn, Columbia, GaTech, Duke, Chicago, USC.\nEducation: Oregon State: Undergrad in Economics, 2.88 GPA, Masters in Statistics, 3.67 GPA, OMSCS currently 4.0 GPA.\nResearch: Some publications, a couple in ML/AI but not at top venues, one first author.\nExperience: 6 years as a data scientist at Pacific Northwest National Laboratory.\nRecommendations: 3 from colleagues with PhD’s who I’ve co-authored with.\n\n\n\nOMSCS Worries\nI took machine learning (CS7641) this Spring and tried to basically wing it with my current knowledge of ML. I wouldn’t recommend this as the class is quite challenging even if you’re familiar with the material. They expect you to really engage with the content and the requirements of the assignments are somewhat ambiguous. In typical fashion, I was lamenting that I might not get a B, that I would have to take it again (B required for ‘core’ classes, ML being one of them), and of course in the end did well on the final and got an A. Along with an A in the second class I took this term, my 4.0 at OMSCS lives on. This probably won’t be the last time I’m worried for nothing.\n\n\nBobgate\nThere was recently a question posed by @littmath on Twitter that stumped some people as far as explaining the answer. The best answer I found can be seen here: Reddit.\nThe problem, briefly: 100 coins are flipped. For every HH that appears, Alice gets a point. For every HT that appears, Bob gets a point. Who is more likely to win? That is, who is more likely to have more points at the end of the 100 flips? Doesn’t matter by how much they win, just who is more likely to win.\nThere are reasonable arguments to say Alice will win (HHH is 2 points!), or that it is a tie (something about equal probability), however the answer is Bob. The easiest way to see this is by simply simulating a bunch of flips and seeing who wins more often, or by enumerating all possible outcomes for a smaller number of flips and counting who wins more:\nlibrary(dplyr)\nlibrary(purrr)\n\n# all combos of 10 flips\nbinary_list = lapply(1:10, function(i) c(0,1))\nall_combos = do.call(expand.grid, binary_list)\n\nscore_flip &lt;- function(...) {\n  x = list(...)\n  score_bob = 0\n  score_alice = 0\n  for (i in 1:(length(x)-1)) {\n    if ((x[i] == 1) && (x[i+1] == 0)) {\n      score_bob = score_bob + 1\n    }\n    \n    if ((x[i] == 1) && (x[i+1] == 1)) {\n      score_alice = score_alice + 1\n    }\n  }\n  \n  return(list('alice' = score_alice, 'bob' = score_bob))\n}\n\nresult = all_combos |&gt;  mutate(scores = purrr::pmap(all_combos, .f = score_flip))\n\nscore_alice = purrr::map(result$scores, ~.x$alice) |&gt; unlist()\nscore_bob = purrr::map(result$scores, ~.x$bob) |&gt; unlist()\n\nprint(c(mean(score_alice &gt; score_bob), mean(score_alice &lt; score_bob)))\n[1] 0.3623047 0.4531250\nWe see that Bob is winning more often in this example of 10 flips. The question asked for 100 flips though, which does matter, and indeed simulating it shows that Alice catches up a bit but loses more often. The intuition in the reddit post is that scoring happens only when there is one or more heads. This could be a single H, or a sequence of H’s, both terminated by a T or reaching the 100th flip. We can ask, for any such sequence, what is the expected points for Bob or Alice? Lets consider Bob first. They are almost always going to get 1 point, only in the case where there are H’s until the 100th flip will there not be 1 point which is usually very unlikely, so lets approximate the expected points for Bob as 1 for now.\nNow for Alice. We consider that HT makes up half of all sequences of H’s: Half the time, a T is flipped after the first head, ending the sequence with zero points. Then we consider that HHT happens 1/4 of the time with one point, HHHT 1/8 of the time with 2 points, etc. Assuming there are 99 possible flips to go after the initial H, we can write out this sequence of expected points for Alice as:\n\\[\n\\sum_{i=1}^{100} \\frac{(i-1)}{2^i} \\lt \\sum_{i=1}^{\\infty} \\frac{(i-1)}{2^i}\n\\]\nYes the first term is zero but I’m starting there to simplify things later. Eh…okay I’m going to rewrite this and then use the power of stackexchange! to finally get the answer.\n\\[\\begin{align*}\n\\sum_{i=1}^{\\infty} \\frac{(i-1)}{2^i} &= \\sum_{i=1}^{\\infty} \\frac{i}{2^i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^i} \\\\\n&= \\sum_{i=1}^{\\infty} \\frac{i}{2^i} - 1 \\\\\n\\end{align*}\\]\nWhere the last step uses the definition of a geometric series. Now we have to solve for the final infinite summation. The solution is here, which I’ll repeat here in case my blog outlives stackexchange (likely). We start by stating the partial sum of the series (with a general \\(r\\) instead of our \\(r=\\frac{1}{2}\\)) as:\n\\[\nS_m = \\sum_{i=1}^{m} i r^i\n\\]\nNow notice the following:\n\\[\\begin{align*}\nS_m - r S_m &= \\sum_{i=1}^{m} r^i - mr^{m+1} \\\\\n&= \\frac{r - r^{m+1}}{1 - r} - mr^{m+1} \\\\\n&= \\frac{r - (m+1)r^{m+1} + mr^{m+2}}{1 - r}\n\\end{align*}\\]\nAnd since \\(S_m - r S_m = (1 - r) S_m\\), we have:\n\\[\nS_m = \\frac{r - (m+1)r^{m+1} + mr^{m+2}}{(1 - r)^2}\n\\]\nWith our \\(r=\\frac{1}{2}\\), this sucker is going to 2 as \\(m \\rightarrow \\infty\\) and is basically already there at \\(m=100\\). So the expected points for Alice is 1, but this is only true in the limit. If we begin our sequence with, say, only 5 flips left, then we actually have:\n\\[\nS_5 - \\sum_{i=1}^{5} \\frac{1}{2^i} = 1.78125 - 0.96875 = 0.8125\n\\]\nversus Bob who is \\(1-\\frac{1}{2^5} = 0.96875\\), a serious edge. This appears to be true for all values of \\(m\\), so that Bob’s expected score for a series of H’s is always greater. I’m not sure if there is a straightforward way to compute the expected score over the entire sequence of flips without enumerating each outcome as I did previously.\n\n\nComing Soon!\nI’ve been trying to train an agent to play fighting games with diambra. It has been a bit tricky since I’m trying to run it in Colab so I can share it, but that requires spinning up docker on another system (EC2) and pinging them from the Colab notebook. I’ve got this mostly sorted, so hopefully I can show something soon."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Another blog? Why sir?",
    "section": "",
    "text": "I made this blog because I wanted to practice with .qmd documents, as well as have a place to document my learning journey on various data-science/ML topics (I may also post pictures of bread I bake). Also, I needed a way to inflate my ego, and a blog seems like a good way to do that?\n__DISCLAIMER__ This will appear throughout the blog - I am not an expert in anything, I will try to couch everything in uncertainty. Use any code or follow any conclusions I post at your own risk."
  },
  {
    "objectID": "posts/baby-1/sandwich.html",
    "href": "posts/baby-1/sandwich.html",
    "title": "I am the father of a delicious sandwich.",
    "section": "",
    "text": "A blog post for the sake of being able to claim my blog isn’t stale! Wooo! I became a father on August 18, 2025. Our little boy is doing well and I’ve been enjoying being on leave and bonding with him. I’m working on a little chatbot/RAG app using github models and Qdrant to retrieve indexed documents from my Zotero collection, configured with hydra. I might get around to actually writing something up for it once my kid is off to college."
  },
  {
    "objectID": "posts/baby-1/sandwich.html#new-job",
    "href": "posts/baby-1/sandwich.html#new-job",
    "title": "I am the father of a delicious sandwich.",
    "section": "New Job",
    "text": "New Job\nI accepted a new job doing data science for a healthcare middleman, and will be leaving Pacific Northwest National Laboratory after 7.5 years. Its been an incredible time at the lab, and I’m both sad to leave and excited about the new opportunity. I’ve found there’s a lot of benefit to simply doing something new after a while: I remember when I moved out a house I had lived in for several years in Corvallis, mostly doing nothing while working at a pizza restaurant1 to support myself. I’m pretty sure just the act of moving to a new environment caused me to start exercising more, study harder, eat better, and crew the first manned mission to Mars."
  },
  {
    "objectID": "posts/baby-1/sandwich.html#a-sandwich-i-made",
    "href": "posts/baby-1/sandwich.html#a-sandwich-i-made",
    "title": "I am the father of a delicious sandwich.",
    "section": "A Sandwich I Made",
    "text": "A Sandwich I Made\nI have this random childhood memory where one of my older brother’s friends just told us about a BLT he made for like 10 minutes…Well, this is a shorter, blog-post version of that. See Figure 1 for the final product. The sandwich is an impossible patty on ciabatta bread, with an egg, avocado, gouda cheese, mayo and sriracha sauce. Mozzarella sticks and marinara sauce occupy the rest of the plate. It tasted delicious, however the ciabatta bread made the sandwich difficult to keep together as I bit into it. Luckily I, like every other American, have impeccable ability to maintain the integrity of an unruly sandwich while eating it.\n\n\n\n\n\n\nFigure 1: A delicious sandwich."
  },
  {
    "objectID": "posts/baby-1/sandwich.html#footnotes",
    "href": "posts/baby-1/sandwich.html#footnotes",
    "title": "I am the father of a delicious sandwich.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShoutout Papas Pizza (not to be confused with Papa John’s/Murphy’s) in Corvallis.↩︎"
  },
  {
    "objectID": "posts/use-shift/index.html",
    "href": "posts/use-shift/index.html",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "",
    "text": "Honestly, what a disaster of a title. I don’t know if either part in isolation would be more likely to get someone to read this, but I just wanted to make a post. Maybe I should have click-baited with something completely unrelated, oh well.\nI’m currently taking Machine Learning for Trading from the Georgia Tech online CS masters program as part of my plan to motivate myself to self-study by paying them money. While much of the ML parts of the class are review for me, it has been fun to learn things about trading, as well as do some numpy/pandas exercises.\\(^{[1]}\\)\nThe class is heavy on vectorizing your code so its fast (speed is money in trading), as well working with time series. I’ll go over two things I’ve found neat/useful. One is vectorizing sampling from a categorical distribution where the rows are logits. The other is using the .shift() method of pandas DataFrames and Series."
  },
  {
    "objectID": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "href": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "Vectorized sampling from a categorical distribution",
    "text": "Vectorized sampling from a categorical distribution\nOkay, so the setup is you have an array that looks like this:\n\nimport numpy as np\nnp.random.seed(23496)\n\n# The ML people got to me and I now call everything that gets squashed to a probability distribution 'logits'\nlogits = np.random.uniform(size = (1000, 10))\nlogits = logits/logits.sum(axis = 1)[:,None]\n\nlogits[:5,:]\n\narray([[0.13460263, 0.12458665, 0.05453746, 0.11991544, 0.07351353,\n        0.11034637, 0.07374194, 0.11460002, 0.11551094, 0.07864502],\n       [0.18602867, 0.09960763, 0.02422872, 0.10095124, 0.02961313,\n        0.04475981, 0.08855924, 0.11246979, 0.16960986, 0.14417191],\n       [0.0142491 , 0.14630917, 0.11735343, 0.12211442, 0.11230253,\n        0.12474719, 0.13253043, 0.01106296, 0.08627144, 0.13305933],\n       [0.09227899, 0.15207502, 0.07677232, 0.16330634, 0.11855988,\n        0.08710454, 0.05458428, 0.18425363, 0.0224089 , 0.04865609],\n       [0.01826615, 0.1956786 , 0.03484824, 0.12495028, 0.11824123,\n        0.01893324, 0.17954348, 0.15826364, 0.1351583 , 0.01611684]])\n\n\nEach row can be seen as the bin probabilities of a categorical distribution. Now suppose we want to sample from each of those distributions. One way you might do it is by leveraging apply_along_axis:\n\nsamples = np.apply_along_axis(\n    lambda x: np.random.choice(range(len(x)), p=x), \n    1, \n    logits\n)\n\nsamples[:10], samples.shape\n\n(array([3, 1, 6, 7, 8, 9, 2, 4, 5, 3]), (1000,))\n\n\nHm, okay this works, but it is basically running a for loop over the rows of the array. Generally, apply_along_axis is not what you want to be doing if speed is a concern.\nSo how do we vectorize this? The answer I provide here takes advantage of the Gumbel-max trick for sampling from a categorical distribution. Essentially, given probabilities \\(\\pi_i, i \\in {0,1,...,K}, \\sum_i \\pi_i = 1\\) if you add Gumbel distribution noise to the log of the probabilites and then take the max, it is equivalent to sampling from a categorical distribution.\nAgain, take the log of the probabilities, add Gumbel noise, then take the arg-max of the result.\n\nsamples = (\n    np.log(logits) + \\\n    np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1)  \n\nsamples[:10], samples.shape\n\n(array([0, 0, 5, 2, 2, 5, 6, 9, 7, 9]), (1000,))\n\n\nLets test if this is actually faster:\n\n%%timeit\n(np.log(logits) + np.random.gumbel(size = logits.shape)).argmax(axis = 1)  \n\n348 μs ± 107 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\nnp.apply_along_axis(lambda x: np.random.choice(range(len(x)), p=x), 1, logits)  \n\n17.3 ms ± 1.94 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nYea, so a couple orders of magnitude faster with vectorization. We should probably also check that it produces a similar distribution across many samples (and also put a plot in here to break up the wall of text). I’ll verify by doing barplots for the distribution of 1000 values drawn from 4 of the probability distributions. Brb, going down the stackoverflow wormhole because no one knows how to make plots, no one.\n…\nOk I’m back, here is a way to make grouped barplots with seaborn:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# do 1000 draws from all distributions using gumbel-max\ngumbel_draws = []\n\nfor i in range(1000):\n    samples = (\n        np.log(logits) + np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1) \n\n    gumbel_draws.append(samples) \n\ngumbel_arr = np.array(gumbel_draws)\n\n# ...and 1000 using apply_along_axis + np.random.choice\napply_func_draws = []\n\nfor i in range(1000):\n    samples = np.apply_along_axis(\n        lambda x: np.random.choice(range(len(x)), p=x), \n        1, \n        logits\n    )\n    apply_func_draws.append(samples)\n\napply_func_arr = np.array(apply_func_draws)\n\nIn the above, if you ran the two for loops separately, you would get a better sense of how much faster the vectorized code is. Now well munge these arrays into dataframes, pivot, and feed them to seaborn.\n\ngumbel_df = pd.DataFrame(gumbel_arr[:,:4])\napply_func_df = pd.DataFrame(apply_func_arr[:,:4])\n\ngumbel_df = pd.melt(gumbel_df, var_name = \"distribution\")\napply_func_df = pd.melt(apply_func_df, var_name = \"distribution\")\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 8))\n\np = sns.countplot(data = gumbel_df, x=\"distribution\", hue=\"value\", ax = axs[0])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[0].set_title(\"Using Gumbel-max\")\n\np = sns.countplot(data = apply_func_df, x=\"distribution\", hue=\"value\", ax = axs[1])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[1].set_title(\"Using apply_along_axis + np.random.choice\")\n\nfig.tight_layout()\n\n\n\n\nThe distribution of drawn values should be roughly the same.\n\n\n\n\nEyeballing these, they look similar enough that I feel confident I’ve not messed up somewhere.\nFinally, of note is that a modification of this trick is used a lot in training deep learning models that want to sample from a categorical distribution (Wav2vec(Baevski et al. 2020) and Dall-E(Ramesh et al. 2021) use this). I’ll go over it in another post, but tl;dr, the network learns the probabilities and max is changed to softmax to allow backpropagation."
  },
  {
    "objectID": "posts/use-shift/index.html#pandas.dataframe.shift",
    "href": "posts/use-shift/index.html#pandas.dataframe.shift",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "pandas.DataFrame.shift()",
    "text": "pandas.DataFrame.shift()\nYou could probably just go read the docs on this function, but I’ll try to explain why its useful. We often had to compute lagged differences or ratios for trading data indexed by date. To start I’ll give some solutions that don’t work or are bad for some reason, but might seem like reasonable starts. Lets make our dataframe with a date index to play around with:\n\nimport pandas as pd\n\nmydf = pd.DataFrame(\n    {\"col1\":np.random.uniform(size=100), \n    \"col2\":np.random.uniform(size=100)}, \n    index = pd.date_range(start = \"11/29/2022\", periods=100)\n)\n\nmydf.head()\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n2022-11-29\n0.478292\n0.121631\n\n\n2022-11-30\n0.664101\n0.781843\n\n\n2022-12-01\n0.245395\n0.005426\n\n\n2022-12-02\n0.726935\n0.532795\n\n\n2022-12-03\n0.658744\n0.970972\n\n\n\n\n\n\n\nNow, suppose we want to compute the lag 1 difference. Specifically, make a new series \\(s\\) where \\(s[t] = col1[t] - col2[t-1]: t &gt; 0\\), \\(s[0] =\\) NaN. Naive first attempt:\n\nmydf[\"col1\"][1:] - mydf[\"col2\"][:-1]\n\n2022-11-29         NaN\n2022-11-30   -0.117742\n2022-12-01    0.239969\n2022-12-02    0.194140\n2022-12-03   -0.312228\n                ...   \n2023-03-04   -0.214499\n2023-03-05    0.361584\n2023-03-06   -0.199390\n2023-03-07    0.272564\n2023-03-08         NaN\nFreq: D, Length: 100, dtype: float64\n\n\nUh, so what happened here? Well, pandas does subtraction by index, like a join, so we just subtracted the values at the same dates, but the first and last dates were missing from col1 and col2 respectively, so we get NaN at those dates. Clearly this is not what we want.\nAnother option converts to numpy, this is essentially just a way to move to element-wise addition:\n\nlag1_arr = np.array(mydf[\"col1\"][1:]) - np.array(mydf[\"col2\"][:-1])\nlag1_arr[:5], lag1_arr.shape\n\n(array([ 0.54247038, -0.53644839,  0.72150904,  0.12594842, -0.225511  ]),\n (99,))\n\n\nOf course, this is not the same length as the series, so we have to do some finagling to get it to look right.\n\n# prepend a NaN\nlag1_arr = np.insert(lag1_arr, 0, np.nan)\nlag1_arr[:5], lag1_arr.shape\n\n(array([        nan,  0.54247038, -0.53644839,  0.72150904,  0.12594842]),\n (100,))\n\n\nOk, its the same length and has the right values so we can put it back in the dataframe as a column or create a new series (and add the index again)\n\n# make a new series\nlag1_series = pd.Series(lag1_arr, index=mydf.index)\n\n# or make a new column\n# mydf[\"col3\"] = lag1_arr\n\nAlright, but this looks kinda ugly, we can do the same thing much more cleanly with the .shift() method of pandas DataFrames and Series. .shift(N) does what it sounds like, it shifts the values N places forward (or backward for negative values), but keeps the indices of the series/dataframe fixed.\n\nmydf[\"col1\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    0.478292\n2022-12-03    0.664101\n                ...   \n2023-03-04    0.807256\n2023-03-05    0.992331\n2023-03-06    0.974969\n2023-03-07    0.339215\n2023-03-08    0.625530\nFreq: D, Name: col1, Length: 100, dtype: float64\n\n\nWith this we can easily compute the lag 1 difference, keeping the indices and such.\n\n# difference\nmydf[\"col1\"] - mydf[\"col2\"].shift(1)\n\n# lag 3 ratio\nmydf[\"col1\"]/mydf[\"col2\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    5.976563\n2022-12-03    0.842553\n                ...   \n2023-03-04    0.478158\n2023-03-05    0.635103\n2023-03-06    0.694335\n2023-03-07    0.804248\n2023-03-08    3.747774\nFreq: D, Length: 100, dtype: float64\n\n\nThis lag-N difference or ratio is extremely common and honestly I can’t believe I hadn’t been using .shift() more.\n\n\\(^{[1]}\\)I am not affiliated with GA-Tech beyond the new washing machine and jacuzzi they gave me to advertise their OMSCS program"
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html",
    "href": "posts/unet-diffusion/unet-diffusion.html",
    "title": "Unconditional Diffusion With a UNET",
    "section": "",
    "text": "A Bad Primer on Diffusion Models\nModel Implementation\nA Simple Training Loop\nSampling Images From Noise\nA while ago this whole guided diffusion thing made waves in the ML world and provided us with amazing images of anthropomorphized cucumbers walking chiwawas. Of course, I tried my best to understand this amazing technology as well as play around with various open-source implementations.\nMy post on vector-quantized VAE’s was the first part of trying to implement parts of these Rube-Goldberg machines. Unfortunately, life and laziness happened, in addition to the remaining components being a bit more difficult to implement imo. I decided to try and recreate a portion of a popular form of guided diffusion model called a UNET Ronneberger, Fischer, and Brox (2015) based on the implementation in the codebase for stable diffusion.\nIn the end, it turns out sometimes following some code pretty closely but trying to also ‘code it on your own’ is still kinda frustrating. The reasons they code things up a certain way is not always clear, and I ended up breaking things and had a rough time debugging various discrepancies and unintended memory issues with my broken implementations. Additionally, the UNET in stable diffusion processes the images after they’ve been transformed into a latent space by a VAE…. My initial attempts at trying to put these two pieces together produced some….uninspiring results as seen in Figure 1.\nAnyway, this post is gonna be about my UNET implementation, and there will be some images of fake butterflies at the end! As always, I’m going to scold you for reading some random dude’s blog instead of looking at the following:\nIf you wanna skip to the code portion of this post, here it is, or you can see the associated colab notebook."
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html#a-brief-primer-on-diffusion-models",
    "href": "posts/unet-diffusion/unet-diffusion.html#a-brief-primer-on-diffusion-models",
    "title": "Unconditional Diffusion With a UNET",
    "section": "A Brief Primer on Diffusion Models",
    "text": "A Brief Primer on Diffusion Models\nDiffusion models are based around the random process of….diffusion…in which an input has noise slowly added to it until it is pure noise. There’s some physics background on this but sorry physicists, the AI people are in charge of all science now and have even taken your nobel prizes. I present an incomplete description of the process here, but really just read Ho, Jain, and Abbeel (2020).\nWe consider a series of inputs \\(x_t\\): \\(t \\in {0,1,...T}\\). They describe the diffusion process as \\(p_{\\theta}(x_0) := \\int p_{\\theta}(x_{0:T})dx_{1:T}\\). They begin by defining the ‘reverse process’, that is, the process that takes noise and maps it back to the input. It is defined as a Markov chain with Gaussian transitions: \\[\n\\begin{align}\np_{\\theta}(x_{0:T}):= p(x_T) \\prod_{t=1}^T p_{\\theta}(x_{t-1}\\vert x_t) \\quad \\quad p_{\\theta}(x_{t-1}\\vert x_t) := \\mathcal{N}(x_t;\\mu_{\\theta}(x_t,t), \\Sigma_{\\theta}(x_t, t))\n\\end{align} \\label{eq1}\\tag{eqn-1}\n\\]\ni.e. we have some image \\(x_t\\), and we ‘transition’ to a different (hopefully denoised) image \\(x_{t-1}\\) by adding a draw from a normal distribution with mean and variance dependent on \\(x_t\\) and the timestep \\(t\\).\nThe other part of the model is the forward process, which adds noise based on a series of coefficients \\(\\beta_1, \\beta_2, ..., \\beta_T \\in (0,1)\\):\n\\[\nq(x_{1:T} \\vert x_0):= \\prod_{t=1}^T q(x_t \\vert x_{t-1})\\quad \\quad q(x_t \\vert x_{t-1}):= \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t \\mathbf{I}) \\label{eq2}\\tag{eqn-2}\n\\]\nThat is, the forward process takes an input (say, an image) \\(x_0\\) and gradually transitions to a noise distribution centered at 0. If you look at the equation, it should make sense why this is: Each step has a mean that is closer to zero than the mean before it, and after sufficient steps, this is gonna be around 0 with high probability.\nFrom here you can optimize the evidence lower bound as described in the VAE literature, except with \\(x_0\\) as the evidence and \\(x_{1:T}\\) is the latent representation \\(z\\). See my other post about VAE’s.\nOne particular property of the forward process is that we can sample an \\(x_t\\) at an arbitrary timestep in closed form:\n\\[q(x_t \\vert x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)\\mathbf{I}) \\quad \\quad \\alpha_t = 1 - \\beta_t \\quad \\quad \\bar{\\alpha}_t= \\prod_{s=1}^t \\alpha_s \\label{eq3}\\tag{eqn-3}\\]\nIt is a nice exercise to try and show that this is the case … no I’m not going to show it, I did it once and forgot okay, look at (\\(\\ref{eq2}\\)) and cascade that second equality all the way back to \\(x_0\\) to show that it is the case.\nThe objective reduces to something where we randomly sample elements of the forward process and optimize the evidence lower bound by optimizing the paramers of the model, which are currently \\(\\beta_t\\) and \\(\\theta\\). In Ho, Jain, and Abbeel (2020) They hold fixed \\(\\beta\\), as well as reparametrize the elements of the reverse process \\(p_{\\theta}(x_{0:T})\\), which we notice were defined by two parameterized functions \\(\\mu_{\\theta}(x_t,t)\\) and \\(\\Sigma_{\\theta}(x_t, t)\\). Well, first they actually just set the variance to be fixed to: \\(\\Sigma_{\\theta}(x_t, t) = \\sigma_t^2\\mathbf{I}\\), where they experiment with a couple schemes for the values of \\(\\sigma_t^2\\) based on \\(\\beta_t\\) which seemed to both work well.\nThey eventually get to the parametrization:\n\\[\\mu_{\\theta}(x_t, t) = \\tilde{\\mu}\\left(x_t, \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t))\\right) = \\frac{1}{\\alpha_t}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t, t))\\]\nForget what \\(\\tilde{\\mu}\\) is, or read the paper for the details … the tldr of this whole thing is that what we need to optimize in the end is this \\(\\epsilon_{\\theta}(x_t, t)\\) function which predicts the noise component of \\(x_t\\). That is, the training simply consists of training a model to predict the noise component given an input \\(x_t\\) and timestep \\(t\\).\nThe math works out that to get the ‘previous sample’ \\(x_{t-1} \\sim p_{\\theta}(x_{t-1} \\vert x_t)\\) we can simply compute: \\[x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t, t)) + \\sigma_t z \\quad \\quad z \\sim \\mathcal{N}(0, \\mathbf{I})\\label{eq4}\\tag{eqn-4}\\]\nMmmkay so this is the framework for training and sampling from an unconditional diffusion model. We will simply do the following:\n\nRepresent \\(\\epsilon_{\\theta}(x_t, t)\\) as a UNET that takes in a noisy image (\\(x_t\\)) and \\(t\\) as a timestep embedding.\nDraw some \\(\\mathcal{N}(0, \\mathbf{I})\\) noise, some random timesteps, and produce noisy images according to (\\(\\ref{eq3}\\))\nMake ‘time embeddings’ from the integer timesteps and shove the time embeddings and noisy image through the UNET.\nCompute MSE loss between the output and noise, compute loss, update."
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html#implementing-the-unet",
    "href": "posts/unet-diffusion/unet-diffusion.html#implementing-the-unet",
    "title": "Unconditional Diffusion With a UNET",
    "section": "Implementing the UNET",
    "text": "Implementing the UNET\nAs for what I actually coded up for this, it was the UNET, the model which predicts the noise component from \\(x_t\\). I wanted to just…practice model building, so I built the UNET based on the OpenAI implementation in the codebase for stable diffusion. “So you copied it?”. No! I used it as a guide for developing my code. “So you copied it”. Sigh, fine yea pretty much, but even so it was a pain in the ass to actually get it to line up perfectly with their implementation:\n\nMy first attempt blew up the A100’s in Colab, I still have no idea why.\nI had to meticulously go through each layer and figure out where discrepancies in layer/block sizes were. The stable diffusion code is a bit opaque…\n\nAnyway, this post uses the code listed here, and I’ll go through some of it in this post, the notebook that uses the code in that repo is here.\nThe things to implement for the UNET:\n\nThe time embeddings\nResNet blocks that can incorporate a time embedding.\nAttention blocks (attention + feed-forward)\nSpatial transformers (basically expands the spatial dimensions into a ‘time’ dimension and attends over it)\n\nTechnically for the unconditional diffusion I don’t need 3. and 4. but I’ll include them anyway. Okay first the time embedding, this is almost literally copied from the stable diffusion repo. Lets take a look:\n\n\nCode\ndef time_embeddings(t: torch.tensor, out_dim: int, max_period: int = 10000):\n    \"\"\"Create timestep embeddings from a vector of ints representing timesteps\n    Args:\n        t (torch.Tensor): Tensor of shape (B,) containing timesteps\n        out_dim (int): Dimension of the output embeddings\n        max_period (int): Maximum period for the sine and cosine functions\n\n    Returns:\n        torch.Tensor: Tensor of shape (B, out_dim) containing the timestep embeddings\n    \"\"\"\n    half = out_dim // 2\n    denom = torch.exp(\n        -torch.tensor(max_period).log() * torch.arange(0, half, dtype=torch.float32) / half\n    )\n\n    denom = denom.to(t.device)\n\n    phases = t[:, None] * denom[None]\n\n    # concatentate to form a tensor of shape B x out_dim\n    out_emb = torch.cat([phases.sin(), phases.cos()], dim=-1)\n\n    # if out_dim is odd, add a zero column\n    if out_dim % 2:\n        out_emb = torch.cat([out_emb, torch.zeros_like(out_emb[:, :1])], dim=-1)\n\n    return out_emb\n\n\nThis is essentially creating these \\(d\\)-dimensional sine-cosine embeddings according to\n\\[\n\\begin{align}\n&P(t,j) = sin(\\frac{t}{10,000^{2j/d}}), j \\in {1,2,... d//2} \\\\\n&P(t,j) = cos(\\frac{t}{10,000^{2j/d}}), j \\in {d//2+1, d//2+2... d}\n\\end{align}\n\\]\nYes, it really is, look at the form of whats inside the function and do some algebra, it is the same. Am I going to show you? No.\nOkay we have these \\(d\\) dimensional embeddings that will get injected into each of the ResNet blocks. We’ll see how that happens below. The ResNet blocks consist of the following steps:\n\nGroup normalization\nSiLU activation\nResampling (either up-sample or down-sample), cutting dimension of the image in half.\nConvolution layer\nAddition of time embeddings\nOutput block consisting of groupnorm -&gt; SiLU -&gt; dropout -&gt; convolution\n\nThese layers are used to reduce the spatial dimension of the image in 3. and increase (or decrease) the channel dimension usually in 6. The time embeddings work by first projecting the \\(d\\) dimensional embedding to the channel dimension via a linear layer and them element-wise adding this new embedding to every spatial dimension.\n\n\nCode\nclass ResBlock(nn.Module):\n    def __init__(self, channels_in, d_emb, channels_out=None, resample=None, dropout=0.0):\n        super().__init__()\n\n        self.channels_in = channels_in\n        self.channels_out = channels_out or channels_in\n\n        self.group_norm_in = nn.GroupNorm(32, channels_in)\n\n        self.conv_in = conv_nd(2, channels_in, self.channels_out, 3, padding=1)\n        self.conv_out = conv_nd(2, self.channels_out, self.channels_out, 3, padding=1)\n\n        if resample == \"down\":\n            self.resample_h = DownSample(channels_in, dims=2, use_conv=False)\n            self.resample_x = DownSample(channels_in, dims=2, use_conv=False)\n        elif resample == \"up\":\n            self.resample_h = Upsample(channels_in, dims=2, use_conv=False)\n            self.resample_x = Upsample(channels_in, dims=2, use_conv=False)\n        else:\n            self.resample_x = self.resample_h = nn.Identity()\n\n        if self.channels_out == channels_in:\n            self.skip_connection = nn.Identity()\n        else:\n            self.skip_connection = conv_nd(2, channels_in, self.channels_out, 1)\n\n        self.time_emb = nn.Sequential(nn.SiLU(), nn.Linear(d_emb, self.channels_out))\n\n        self.out_block = nn.Sequential(\n            nn.GroupNorm(32, self.channels_out), nn.SiLU(), nn.Dropout(dropout), self.conv_out\n        )\n\n    def forward(self, x: torch.tensor, emb: torch.tensor):\n        \"\"\"\n        Args:\n            x (torch.tensor): Input tensor of shape (B, C, H, W)\n            emb (torch.tensor): Time embedding of dimension (B, D)\n\n        Returns:\n            torch.tensor: Output tensor of shape (B, C*, H*, W*)\n        \"\"\"\n\n        h = self.group_norm_in(x)\n        h = F.silu(h)\n        h = self.resample_h(h)\n        h = self.conv_in(h)\n\n        emb = self.time_emb(emb)\n        h = h + emb[:, :, None, None]  # expand spatial dims, add along channel dim\n\n        h = self.out_block(h)\n\n        x = self.resample_x(x)\n\n        return h + self.skip_connection(x)\n\n\nIn the above, the Upsample and Downsample classes are implemented either as convolutions with stride 2, or a simple average pooling operation that cuts the spatial dimension in half. In the forward implementation, you can see I’m adding the time embedding to the channel dimension via broadcasting over all spatial dimensions.\nThe other main layer type in the UNET is the spatial transformer, which, as I said before, is not really necessary in this unguided (no text embeddings) version of the diffusion model. There’s uh, a few pieces to this. There’s a standard cross-attention layer, which is meant to take in context. In this case, we are not using context embeddings, so we fix the query dimension to be the same as the key and value dimensions and simply have the inputs to the QKV projection layers be the same input….see below:\n\n\nCross-attention layers\nclass FeedForward(nn.Sequential):\n    \"\"\"Feed forward module for the attention block.  Has two  linear layers with a GeLU and dropout layer in between.\n\n    Args:\n        d_in (int): Input dimension to the first linear layer.\n        d_out (int): Output dimension of the second linear layer.\n        mult (int): Multiplier (of the input dimension) for the hidden dimension. Default: 4\n        dropout (float): Dropout rate. Default: 0.1\n    \"\"\"\n\n    def __init__(self, d_in, d_out, mult=4, dropout=0.1):\n        super().__init__()\n\n        self.proj_in = nn.Linear(d_in, int(d_in * mult))\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n        self.proj_out = nn.Linear(int(d_in * mult), d_out)\n\n\n# Cross attention module, from scratch\nclass CrossAttention(nn.Module):\n    \"\"\"Basic cross attention module.\n\n    Args:\n        d_q (int): Input dimension of the query.\n        d_model (int): Inner dimension of the QKV projection layers. Default: 512\n        d_cross (int): Input dimension of the key and value inputs, for cross attention. Default: None\n        n_heads (int): Number of attention heads. Default: 8\n        dropout (float): Dropout rate. Default: 0.0\n    \"\"\"\n\n    def __init__(self, d_q, d_model=512, d_cross=None, n_heads=8, dropout=0.0):\n        super().__init__()\n\n        assert d_model % n_heads == 0, f\"n_heads {n_heads} must divide d_model {d_model}\"\n\n        if d_cross is None:\n            d_cross = d_q\n\n        self.proj_q = nn.Linear(d_q, d_model, bias=False)\n        self.proj_k = nn.Linear(d_cross, d_model, bias=False)\n        self.proj_v = nn.Linear(d_cross, d_model, bias=False)\n\n        self.proj_out = nn.Linear(d_model, d_q)\n        self.dropout = nn.Dropout(dropout)\n        self.n_heads = n_heads\n\n    def forward(self, x, context=None, mask=None):\n        # prevent einops broadcasting\n        if context is not None:\n            assert (\n                x.shape[0] == context.shape[0]\n            ), f\"Batch size of x and context must match, found {x.shape[0]} and {context.shape[0]}\"\n\n        if context is None:\n            context = x\n\n        q = self.proj_q(x)\n        k = self.proj_k(context)\n        v = self.proj_v(context)\n\n        # at this point we've already flattened the h/w of the input\n        q = einops.rearrange(q, \"b n (h d) -&gt; b h n d\", h=self.n_heads)\n        k = einops.rearrange(k, \"b m (h d) -&gt; b h m d\", h=self.n_heads)\n        v = einops.rearrange(v, \"b m (h d) -&gt; b h m d\", h=self.n_heads)\n\n        qk = einops.einsum(q, k, \"b h n d, b h m d -&gt; b h n m\") / (q.shape[-1] ** 0.5)\n\n        if mask is not None:\n            # mask initially of shape b x m, need to expand to b x h x 1 x m\n            mask = einops.repeat(mask, \"b m -&gt; b h () m\", h=self.n_heads)\n            min_value = -torch.finfo(qk.dtype).max\n            qk.masked_fill_(~mask, min_value)\n\n        qk = F.softmax(qk, dim=-1)\n        out = einops.einsum(qk, v, \"b h n m, b h m d -&gt; b h n d\")\n        out = einops.rearrange(out, \"b h n d -&gt; b n (h d)\")\n\n        out = self.dropout(self.proj_out(out))\n\n        return out\n\n\nThe spatial transformer is essentially a wrapper around blocks that contain some number of these cross-attention blocks. In the forward pass, we flatten the spatial dimensions into a single dimension and attend over this dimension:\n\n\nCode\nclass SpatialTransformer(nn.Module):\n    \"\"\"Spatial transformer module for the UNet architecture.  Contains cross-attention layers that attend over the spatial dimensions of an image, while ingesting cross-attention embeddings from e.g. a text embedding model.\n\n    Args:\n        in_channels (int): Number of input channels.\n        d_q (int): Input dimension of the query.\n        d_cross (int): Input dimension of the key and value inputs, for cross attention. Default: None\n        d_model (int): Inner dimension of the QKV projection layers. Default: 512\n        n_heads (int): Number of attention heads. Default: 8\n        dropout (float): Dropout rate. Default: 0.0\n        depth (int): Number of attention blocks. Default: 1\n    \"\"\"\n\n    def __init__(\n        self, in_channels, d_q, d_cross=None, d_model=512, n_heads=8, dropout=0.0, depth=1\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.d_q = d_q\n        self.d_cross = d_cross\n        self.d_model = d_model\n        self.n_heads = n_heads\n\n        self.norm = torch.nn.GroupNorm(\n            num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n        )\n\n        self.conv_in = nn.Conv2d(self.in_channels, d_q, kernel_size=1, stride=1, padding=0)\n\n        self.blocks = nn.ModuleList(\n            [AttentionBlock(d_q, d_cross, d_model, n_heads, dropout) for _ in range(depth)]\n        )\n\n        self.conv_out = nn.Conv2d(d_q, in_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x, context=None):\n        x_in = x\n\n        x = self.norm(x)\n        x = self.conv_in(x)  # B, d_q, H, W\n\n        b, d_q, h, w = x.shape\n\n        x = einops.rearrange(x, \"b c h w -&gt; b (h w) c\")  # attention mechanism expects B T C\n\n        for b in self.blocks:\n            x = b(x, context)\n\n        x = einops.rearrange(x, \"b (h w) c -&gt; b c h w\", h=h, w=w)\n        x = self.conv_out(x)\n        return x + x_in\n\n\nMMkay, the definition of the full UNET model essentially builds these blocks into the UNET in a loop:\n\n\nCode\nclass EmbeddingWrapper(nn.Sequential):\n    \"\"\"\n    Wrapper for a sequence of layers that take an input tensor and optionally either a tensor of time embeddings or context embeddings.\n    \"\"\"\n\n    def forward(self, x: torch.tensor, t_emb: torch.tensor = None, context: torch.tensor = None):\n        \"\"\"\n        Args:\n            x (torch.tensor): main input image tensor B x C x H x W\n            t_emb (torch.tensor): time embedding B x t_emb_dim\n            context (torch.tensor): context tensor B x d_cross to be passed as context to cross-attention mechanism.\n\n        Returns:\n            torch.tensor: output tensor\n        \"\"\"\n        for layer in self:\n            if isinstance(layer, ResBlock):\n                x = layer(x, t_emb)\n            elif isinstance(layer, SpatialTransformer):\n                x = layer(x, context)\n            else:\n                x = layer(x)\n\n        return x\n\n\nclass UNET(nn.Module):\n    \"\"\"A simpler implementation of the UNET at https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/diffusionmodules/openaimodel.py\n    Here I force the use of spatial attention when adding the guidance layers.\n\n    Args:\n        channels_in (int): number of input channels\n        channels_model (int): number of initial channels which is then multiplied by the values in `channel_mults`\n        channels_out (int): number of output channels\n        context_dim (int): context dimension when performing guided diffusion\n        d_model (int): embedding dimension of the attention layers\n        t_emb_dim (int): time embedding dimension\n        channel_mults (list): list of channel multipliers which will determine the number of channels at each block depending on `channels_model`\n        attention_resolutions (list): list of attention resolutions where attention is applied\n        dropout (float): dropout rate\n    \"\"\"\n\n    def __init__(\n        self,\n        channels_in: int,\n        channels_model: int,\n        channels_out: int,\n        context_dim: int,\n        d_model: int,\n        t_emb_dim: int = None,\n        channel_mults: list[int] = [1, 2, 4, 8],\n        attention_resolutions: list[int] = [2, 4],\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n\n        self.channel_mults = channel_mults\n        self.channels_in = channels_in\n        self.channels_model = channels_model\n        self.context_dim = context_dim\n        self.t_emb_dim = t_emb_dim or channels_model\n\n        # will fill up the downsampling and upsampling trunks in for loops\n        self.down_blocks = nn.ModuleList(\n            [EmbeddingWrapper(nn.Conv2d(channels_in, channels_model, kernel_size=3, padding=1))]\n        )\n        self.up_blocks = nn.ModuleList()\n\n        track_chans = [channels_model]\n        ch_in = channels_model\n\n        # create each downsampling trunk\n        for i, mult in enumerate(channel_mults):\n            # lets assume 1 depth for each downsampling layer\n            # append the reblock first, then spatial transformer\n            # what is timestep dimension???? t_emb -&gt; d_t -&gt; d_model\n            ch_out = channels_model * mult\n\n            resblock = ResBlock(channels_in=ch_in, d_emb=self.t_emb_dim, channels_out=ch_out)\n            layers = [resblock]\n\n            if i in attention_resolutions:\n                sp_transformer = SpatialTransformer(\n                    in_channels=ch_out,\n                    d_q=ch_out,\n                    d_cross=context_dim if context_dim else ch_out,\n                    d_model=d_model,\n                    dropout=dropout,\n                    n_heads=2,\n                )\n                layers.append(sp_transformer)\n\n            self.down_blocks.append(EmbeddingWrapper(*layers))\n\n            track_chans.append(ch_out)\n\n            # downsample after every mult except the last\n            if i != len(channel_mults) - 1:\n                res_ds = ResBlock(\n                    ch_out, d_emb=self.t_emb_dim, channels_out=ch_out, resample=\"down\"\n                )\n                self.down_blocks.append(EmbeddingWrapper(res_ds))\n                track_chans.append(ch_out)\n                ch_in = ch_out\n\n        # middle block, this is Res, Attention, Res\n        # ch_out is the last channel dimension for constructing the downsampling layers\n        self.middle_block = EmbeddingWrapper(\n            ResBlock(channels_in=ch_out, d_emb=self.t_emb_dim),\n            SpatialTransformer(\n                in_channels=ch_out,\n                d_q=ch_out,\n                d_cross=context_dim if context_dim else ch_out,\n                d_model=d_model,\n                dropout=dropout,\n            ),\n            ResBlock(channels_in=ch_out, d_emb=self.t_emb_dim),\n        )\n\n        # upsampling block\n        # this block has 2x the channels, why?  Because of the UNET architecture, we concatenate the channels from the corresponding layer of the downsample section.\n        # There is also an additional res + attention block, this additional block 'matches' the channel dimension of the downsampling module in the downsampling trunk.\n        for i, mult in reversed(list(enumerate(channel_mults))):\n            # We assume there's two resblocks here for simplicity\n\n            # first res block\n            down_ch = track_chans.pop()\n\n            # We have two of these, one that matches the Res + Transformer block, and another that matches the downsampling block\n\n            # first res block\n            res1 = ResBlock(\n                ch_out + down_ch, d_emb=self.t_emb_dim, channels_out=channels_model * mult\n            )\n\n            # this block will output this many channels, we set it here since we want the next iteration to start the channels of the previous block.\n            ch_out = channels_model * mult\n\n            layers = [res1]\n\n            if i in attention_resolutions:\n                sp_trf = SpatialTransformer(\n                    in_channels=channels_model * mult,\n                    d_q=channels_model * mult,\n                    d_cross=context_dim if context_dim else channels_model * mult,\n                    d_model=d_model,\n                    dropout=dropout,\n                )\n                layers.append(sp_trf)\n\n            self.up_blocks.append(EmbeddingWrapper(*layers))\n\n            down_ch = track_chans.pop()\n\n            # and again, same dimension ...\n            layers = []\n\n            layers.append(ResBlock(ch_out + down_ch, d_emb=self.t_emb_dim, channels_out=ch_out))\n\n            if i in attention_resolutions:\n                layers.append(\n                    SpatialTransformer(\n                        in_channels=ch_out,\n                        d_q=ch_out,\n                        d_cross=context_dim if context_dim else ch_out,\n                        d_model=d_model,\n                        dropout=dropout,\n                    )\n                )\n\n            # ... with an upsampling layer at all but the last, since at the last we are matching the initial convolutional layer and a res + spatial transformer block, not an upsampling layer\n            if i &gt; 0:\n                layers.append(\n                    ResBlock(ch_out, d_emb=self.t_emb_dim, channels_out=ch_out, resample=\"up\")\n                )\n\n            self.up_blocks.append(EmbeddingWrapper(*layers))\n\n        # output block that normalizes and maps back to\n        self.out_block = nn.Sequential(\n            nn.GroupNorm(32, channels_model),\n            nn.SiLU(),\n            nn.Conv2d(channels_model, channels_out, kernel_size=3, padding=1),\n        )\n\n    def forward(self, x, timesteps=None, context=None):\n        \"\"\"\n        Args:\n            x (torch.tensor): input tensor\n            timesteps (torch.tensor): Size (B,) tensor containing time indices to be turned into embeddings.\n            context (torch.tensor): context tensor\n        \"\"\"\n        if context is None:\n            assert self.context_dim is None, \"Must pass context if context_dimension is set\"\n        else:\n            assert (\n                self.context_dim is not None\n            ), \"You must set context_dim when creating the model if planning on passing context embeddings.\"\n\n        if timesteps is not None:\n            timesteps = time_embeddings(timesteps, self.t_emb_dim)\n\n        # downsample\n        downsampled = []\n        for block in self.down_blocks:\n            x = block(x, timesteps, context)\n            downsampled.append(x)\n\n        # middle block\n        x = self.middle_block(x, timesteps, context)\n\n        # upsample\n        for block in self.up_blocks:\n            x = torch.cat([x, downsampled.pop()], dim=1)\n            x = block(x, timesteps, context)\n\n        x = self.out_block(x)\n        return x\n\n\nBleh, what a mess. You can look through the loop, but essentially we identify the number of input and output channels, as well as a ‘model channels’ dimension. This is the channel dimension which is multiplied by the i-th element of channel_mults at the i-th level of the UNET when it is downsampling, and in reverse (x8, x4 ,x2 ,x1) when upsampling. We additionally choose the levels at which we perform context-embedding with attention_resolutions, but again, we just ignore that in the case by setting context_dim to None."
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html#training",
    "href": "posts/unet-diffusion/unet-diffusion.html#training",
    "title": "Unconditional Diffusion With a UNET",
    "section": "Training",
    "text": "Training\nJust see the notebook if you wanna run something, but I’ll go over the inner part of training as well.\n\n\nCode\nfor step, batch in enumerate(train_dataloader):\n    with torch.autocast(\"cuda\"):\n        input = batch['images'].to(device)\n\n        timepoints = torch.LongTensor(input.shape[0]).random_(0, dconfig.n_timesteps).to(device) # 1\n\n        noise = torch.randn_like(input).to(device) # 2\n\n        noisy_input = noise_scheduler.add_noise(input, noise, timepoints) # 3\n\n        with accelerator.accumulate(model):\n            out = model(noisy_input, timesteps = timepoints) # 4\n\n            # compute MSE between this and the NOISE\n            loss = torch.nn.functional.mse_loss(out, noise) # 5\n\n            optimizer.zero_grad()\n            accelerator.backward(loss)\n\n            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            lr_scheduler.step()\n\n            epoch_loss.append(loss.item())\n\n\nIgnore the stuff that isn’t defined, we are doing the following:\n\nWe create a random set of integer timepoints between 0 and a user-selected maximum number of timepoints, here 1000.\nWe sample some \\(\\mathcal{N}(0, \\mathbf{I})\\) noise.\nWe use \\(\\ref{eq3}\\) to produce noisy samples from the forward process at random timepoints. The diffusers package has implemented this computation.\nUse our UNET to preduct the noise from the noisy inputs, also passing the timepoints, which internally get embedded as we discussed.\nWe compute the MSE loss between the predicted noise and the \\(\\mathcal{N}(0, \\mathbf{I})\\) noise and perform model updates."
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html#sampling",
    "href": "posts/unet-diffusion/unet-diffusion.html#sampling",
    "title": "Unconditional Diffusion With a UNET",
    "section": "Sampling",
    "text": "Sampling\nTo to this, we simply implement \\(\\ref{eq4}\\), using implementations in the diffusers package:\n\n\nCode\n# Do the diffusion process, start with noise...\ncur_img = torch.randn(16, 3, 128, 128, dtype=torch.float32).to(device)\n\npbar = tqdm.tqdm(noise_scheduler.timesteps)\n\nfor t in pbar:\n  with torch.no_grad():\n      timesteps = torch.LongTensor([t]*cur_img.shape[0]).to(device)\n      # predict the noise component\n      noisy_residual = model(cur_img, timesteps)\n\n  # reverse the diffusion process using the predicted noise\n  previous_noisy_sample = noise_scheduler.step(noisy_residual, t, cur_img).prev_sample\n  cur_img = previous_noisy_sample\n\n\nSpecifically, this noise_sheduler object is implementing (\\(\\ref{eq4}\\)), and you can see it ingests everything needed in the equation. I’m assuming \\sigma_t is set to something internally, perhaps \\(\\beta_t\\) as suggested in the paper, though there are different schedulers that probably implement different schemes. Would be a nice exercise to dig in and verify whats going on in there…\nAnyway, after training and producing samples on the Smithsonian Butterflies dataset, we can produce some weird butterflies from pure noise:\n\n\n\n\n\n\nFigure 2: Honestly, kind of an interesting watercolor look to em."
  },
  {
    "objectID": "posts/unet-diffusion/unet-diffusion.html#next-up",
    "href": "posts/unet-diffusion/unet-diffusion.html#next-up",
    "title": "Unconditional Diffusion With a UNET",
    "section": "Next Up…",
    "text": "Next Up…\nOkay so now I want to actually try this with some text guidance, and with the VAE at the start. The idea is essentially, instead of an image, we’ll ingest the lower-dimensional representation at the bottleneck of a pretrained VAE, and actually add some context embeddings via some pretrained text embedding model like CLIP (Radford et al. (2021)). Also I need a dataset that actually has image-text pairs…sigh."
  },
  {
    "objectID": "posts/binary-classification/binary-classification.html",
    "href": "posts/binary-classification/binary-classification.html",
    "title": "Comparing Various Cross-Entropy Implementations and Attention Masks Double Feature",
    "section": "",
    "text": "Been a while1, so I’m posting about some random things I had to puzzle through as I did some model training at work. The first is essentially going through the various flavors of cross-entropy in pytorch for training binary/multi-class classification tasks. The second is about the common types of attention masks you might encounter when building a transformer based model in pytorch. Essentially, this whole blog post is a long stackoverflow answer to a question no one asked…listen, I didn’t ask you to click on this article, I’m just trying to pass the time on a Saturday."
  },
  {
    "objectID": "posts/binary-classification/binary-classification.html#cross-entropy-variants",
    "href": "posts/binary-classification/binary-classification.html#cross-entropy-variants",
    "title": "Comparing Various Cross-Entropy Implementations and Attention Masks Double Feature",
    "section": "Cross-Entropy Variants",
    "text": "Cross-Entropy Variants\nThere are several variants of cross-entropy in pytorch. While the documentation is very good, I found myself wanting a code example of how exactly they all related (if at all). Specifically, I was doing a binary classification task and wanted to implement possibly class balanced focal loss, a variant of cross-entropy loss, with soft labels. I’ll show how I ended up doing this later, but for now we’ll focus on the core cross-entropy loss.\nThe loss for binary classification can be accomplished with both the binary and multi-class version of the loss function. I hope to show how to link between the three.\nHere are the various cross-entropy-based losses in pytorch:\n\ntorch.nn.CrossEntropyLoss\ntorch.nn.BCELoss\ntorch.nn.BCEWithLogitsLoss\n\nAnd their functional forms, which I wont use, but implement the same calculations:\n\ntorch.nn.functional.cross_entropy\ntorch.nn.functional.binary_cross_entropy\ntorch.nn.functional.binary_cross_entropy_with_logits\n\nLets first make some inputs for these, assuming they are \\((N,2)\\) logits coming out of some model for a binary classification problem. We also have targets both as a dimension \\((N)\\) tensor of class labels and a \\((N,2)\\) tensor of class probabilities, one where all the mass is on the correct class, and one with ‘soft’ labels where the mass is distributed between the two classes.\n\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\n\ntorch.manual_seed(42142)\n\n# random logits as (5, 2) vector\nlogits = torch.randn(5, 2, requires_grad=True)\n\n# targets as dimension (N,) tensor with class labels\ntgt_cls = torch.randint(0, 2, (5,)).long()\n\n# targets as (N, 2) tensor where dimension 1 is a probability distribution with all mass on the correct label\ntgt_probs_hard = torch.cat([1 - tgt_cls.unsqueeze(1), tgt_cls.unsqueeze(1)], dim=1).float()\n\n# targets as (N,2) tensor where dimension 1 is a probability distribution (soft targets)\ntgt_probs_soft = torch.randn(5,2).softmax(dim=-1)\n\nNow the three losses we want to compare, with no reduction.\n\ncross_entropy_loss = torch.nn.CrossEntropyLoss(reduction = 'none')\nbceloss = torch.nn.BCELoss(reduction = 'none')\nbce_logits_loss = torch.nn.BCEWithLogitsLoss(reduction = 'none')\n\nLets try to use them all with various inputs and transformations.\n\nCrossEntropyLoss\n\n# helper to grab the gradients induced by a particular loss\ndef get_grad_and_zero(logits, loss):\n    loss.mean().backward()\n    grad = logits.grad.clone()\n    _ = logits.grad.zero_()\n    return grad\n\nloss_ce_cls = cross_entropy_loss(logits, tgt_cls)\ngrad_ce_cls = get_grad_and_zero(logits, loss_ce_cls)\n\nloss_ce_prob_hard = cross_entropy_loss(logits, tgt_probs_hard)\ngrad_ce_prob_hard = get_grad_and_zero(logits, loss_ce_prob_hard)\n\nloss_ce_prob_soft = cross_entropy_loss(logits, tgt_probs_soft)\ngrad_ce_prob = get_grad_and_zero(logits, loss_ce_prob_soft)\n \nprint(f\"Cross entropy loss with just batch dimension: {loss_ce_cls.detach().numpy()}\")\nprint(f\"Cross entropy loss with soft targets: {loss_ce_prob_hard.detach().numpy()}\")\nprint(f\"Cross entropy loss with probabilities: {loss_ce_prob_soft.detach().numpy()}\")\n\nCross entropy loss with just batch dimension: [2.4427776  1.4271057  0.99968183 0.06437338 2.0198019 ]\nCross entropy loss with soft targets: [2.4427776  1.4271057  0.99968183 0.06437338 2.0198019 ]\nCross entropy loss with probabilities: [0.4737186  1.0628904  0.5669178  0.39352775 1.4264644 ]\n\n\nOk, with reduction = None these all output a \\((N,)\\) dimensional loss tensor with a per-batch loss. We can replicate the default reduction='mean' by averaging these. CrossEntropyLoss returns the same thing for both class labels and \\((N,2)\\) hard probabilities, and of course something different for the soft class probabilities. This is consistent with the documentation which handles different formats of the target tensor differently:\n\ntgt_cls: This is the case where we have a dimension \\((N,)\\) of type torch.long.\ntgt_probs_hard: This is the case where we have a dimension \\((N,2)\\) of type torch.float. When we pass floats, CrossEntropyLoss assumes these are probabilities (between 0 and 1), and that the target tensor is the same shape as the input.\ntgt_probs_soft: This is the same as tgt_probs_hard but with not all the probability mass on one class.\n\nOk, all the above seems reasonable. We can perform some sort of binary classification task with any of these. As a sanity check we can also see that the gradients produced by the first two targets are the same:\n\nprint(f\"Gradients the same for (N,) and (N,2): {(grad_ce_cls == grad_ce_prob_hard).all()}!\")\n\nGradients the same for (N,) and (N,2): True!\n\n\n\n\nBCELoss\nAlright, lets look at the two version of BCELoss, the ones with and without logits. This is where I stumbled a bit, because the format of the input/targets in BCELoss is fundamentally different that that of CrossEntropyLoss. Specifically, the docs say that the input and targets should always be the same shape. WARNING: I’ll have several code examples that either don’t work, or ‘work’ but are not really correct usage for a binary classification task. For instance, the code snippet below throws an error because tgt_cls is not the same shape as logits.\n\nbce_logits_loss(logits, tgt_cls)\n# &gt; ValueError: Target size (torch.Size([5])) must be the same as input size (torch.Size([5, 2]))\n\nOkay, what about the other targets. BCELoss expects inputs in the interval [0,1], so we have to take care of that. I’ll try a sigmoid and softmax.\n\n# bce with logits, expects just raw logits\nloss_bcewl_probs_hard = bce_logits_loss(logits, tgt_probs_hard)\ngrad_bcewl_probs_hard = get_grad_and_zero(logits, loss_bcewl_probs_hard)\n\n# regular bce, wants inputs in [0,1], use sigmoid to take care of that\nloss_bcesigmoid_probs_hard = bceloss(F.sigmoid(logits), tgt_probs_hard)\ngrad_bcesigmoid_probs_hard = get_grad_and_zero(logits, loss_bcesigmoid_probs_hard)\n\n# regular bce again, use softmax to get values in [0,1]\nloss_bcesm_probs_hard = bceloss(logits.softmax(dim=-1), tgt_probs_hard)\ngrad_bcesm_probs_hard = get_grad_and_zero(logits, loss_bcesm_probs_hard)\n\nI’ll do the same but using the probabilities as targets:\n\n## Same as above but with soft targets ##\n\nloss_bcewl_probs_soft = bce_logits_loss(logits, tgt_probs_soft)\ngrad_bcewl_probs_soft = get_grad_and_zero(logits, loss_bcewl_probs_soft)\n\nloss_bcesigmoid_probs_soft = bceloss(F.sigmoid(logits), tgt_probs_soft)\ngrad_bcesigmoid_probs_soft = get_grad_and_zero(logits, loss_bcesigmoid_probs_soft)\n\nloss_bcesm_probs_soft = bceloss(logits.softmax(dim=-1), tgt_probs_soft)\ngrad_bcesm_probs_soft = get_grad_and_zero(logits, loss_bcesm_probs_soft)\n\nThe first thing to quickly get out of the way is that the only difference2 between BCELoss and BCEWithLogitsLoss is that in BCEWithLogitsLoss the input goes through a sigmoid activation internally. Applying sigmoid to the logits before passing through regular BCELoss produces the exact same result:\n\n# we all good\nassert torch.allclose(loss_bcewl_probs_hard, loss_bcesigmoid_probs_hard), \"Release the hounds.\"\nassert torch.allclose(loss_bcewl_probs_soft, loss_bcesigmoid_probs_soft), \"Call in a drone strike on the reader.\"\n\n# whhooaaa, close one, dont uncomment this!\n# assert torch.allclose(loss_bcewl_probs_soft, loss_bcesm_probs_soft,), \"Launch the warheads.\"\n\nprint(\"Whew! made it!\")\n\nWhew! made it!\n\n\nAlright, but what does the output look like…is it correct? First, the output is not even the same shape: torch.Size([5, 2]). The losses using sigmoid (either explicitly or internally with BCEWithLogitsLoss) also do not seem to have any relationship to the values as CrossEntropyLoss:\n\nprint(f\"BCELoss: {loss_bcewl_probs_hard.detach().numpy()}\")\nprint(f\"CrossEntropyLoss: {loss_ce_cls.detach().numpy()}\")\n\nBCELoss: [[3.5669463  0.26643512]\n [0.9737141  1.0721908 ]\n [1.1070106  0.6140938 ]\n [0.28206122 0.18568796]\n [1.7221372  0.8846513 ]]\nCrossEntropyLoss: [2.4427776  1.4271057  0.99968183 0.06437338 2.0198019 ]\n\n\nThe losses for BCELoss when passing in softmax values seems to contain the same values as CrossEntropyLoss, but in two columns:\n\nprint(f\"BCELoss with softmax and hard targets: \\n{loss_bcesm_probs_hard.detach().numpy()}\")\nprint(f\"CrossEntropyLoss with hard targets: \\n{loss_ce_prob_hard.detach().numpy()}\\n\")\n\nprint(f\"BCELoss with softmax with soft targets: \\n{loss_bcesm_probs_soft.detach().numpy()}\")\nprint(f\"CrossEntropyLoss with soft targets: \\n{loss_ce_prob_soft.detach().numpy()}\")\n\nBCELoss with softmax and hard targets: \n[[2.4427779  2.4427776 ]\n [1.4271057  1.4271059 ]\n [0.99968195 0.9996819 ]\n [0.06437341 0.06437336]\n [2.0198019  2.0198016 ]]\nCrossEntropyLoss with hard targets: \n[2.4427776  1.4271057  0.99968183 0.06437338 2.0198019 ]\n\nBCELoss with softmax with soft targets: \n[[0.47371858 0.47371864]\n [1.0628904  1.0628906 ]\n [0.5669178  0.56691784]\n [0.39352778 0.3935278 ]\n [1.4264644  1.4264642 ]]\nCrossEntropyLoss with soft targets: \n[0.4737186  1.0628904  0.5669178  0.39352775 1.4264644 ]\n\n\nWhats going on here is that the BCE versions of cross entropy assume dimensions other than the batch dimension are part of a multi-dimensional output, where each element is a separate prediction/target for a binary classification task, not a prediction/probability for a class based on index. Think of an image model that outputs a tensor of size \\((N, H, W)\\). Pytorch’s implementation of BCELoss would expect you to pass in a \\((N, H, W)\\) target to perform per-pixel loss calculations before averaging (with reduction='mean') across all pixels/batch indices.\nSo, when we pass in a size \\((N,2)\\) input, BCE loss says oh, you have some weird 2-dimensional output per observation, with each of the two values representing a prediction/target for that output index. It is not expecting some probability distribution over 2 discrete values.\nSo, the softmax into BCELoss doesn’t really make sense, but it just happens to transform the input into something that BCELoss accepts (inputs are in [0,1]). Also, why does it contain the same values as CrossEntropyLoss in two columns?\nWell, CrossEntropyLoss is initially passing the input through torch.nn.LogSoftMax and then through torch.nn.NLLLoss. That is, it takes the softmax of the logits, applies the log, then sums along the columns weighted by the (hard or soft) targets. When we softmax the input to BCELoss in the binary case, we essentially replicate this element-wise. Lets take a look at the element-wise loss from BCELoss, ignoring the optional weighting:\n\\[l_n = y_n \\cdot \\log x_n + (1-y_n) \\cdot \\log(1-x_n)\\]\nFirst, the \\(x_n\\) in there has gone through a softmax, and is now having a log applied to it, replicating the torch.nn.LogSoftMax. The rest replicates torch.nn.NLLLoss in the binary case/ What about why the two columns are identical? Well, for each row each element is 1 minus the other element, and it is easy to verify that in this case the loss equation for BCELoss ends up being the same for both elements.\nOh wow, I’ve created a useless wall of text….so how DO we replicate the CrossEntropyLoss binary classification with BCELoss? Well, first off, the best correct way of using BCELoss when you just have a single classification per observation is to have a single output per observation for your model, i.e. your logits should be of size (N,1) and subsequently your targets should be of size (N,1), something like:\n\nlogits_single = torch.randn(5, 1, requires_grad=True)\ntargets_single = torch.randint(0, 2, (5,)).float().unsqueeze(1)\n\nbce_single_output = bceloss(F.sigmoid(logits_single), targets_single)\n\nBut suppose we just want to see how they would be the same, given we’re committed to softmax. Well, given we’ve seen the output matches when we softmax -&gt; BCELoss but with two duplicate columns….we could just only take one of the columns and call .backwards() based on that:\n\nloss_bcesm_probs_hard = bceloss(logits.softmax(dim=-1), tgt_probs_hard)\nloss_bcesm_probs_soft = bceloss(logits.softmax(dim=-1), tgt_probs_soft)\n\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_hard[:,1]), grad_ce_prob_hard), \"Not even close!\"\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_soft[:,1]), grad_ce_prob), \"sudo rm -rf /* --no-preserve-root\"\n\nprint(\"Whew, made it!\")\n\nWhew, made it!\n\n\nWe could also just take one column of the softmaxed input and targets and pass those to BCELoss. Note that it doesn’t matter which index we take before we pass, in terms of the gradients; your intuition about how the two values are probabilistic inverses of each other should tell you this. This will however change what a prediction close to 1 means.\n\nloss_bcesm_probs_hard_0 = bceloss(logits.softmax(dim=-1)[:,0], tgt_probs_hard[:,0])\nloss_bcesm_probs_soft_0 = bceloss(logits.softmax(dim=-1)[:,0], tgt_probs_soft[:,0])\n\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_hard_0), grad_ce_prob_hard), \"Not even close!\"\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_soft_0), grad_ce_prob), \"sudo rm -rf /* --no-preserve-root\"\n\nloss_bcesm_probs_hard_1 = bceloss(logits.softmax(dim=-1)[:,1], tgt_probs_hard[:,1])\nloss_bcesm_probs_soft_1 = bceloss(logits.softmax(dim=-1)[:,1], tgt_probs_soft[:,1])\n\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_hard_1), grad_ce_prob_hard), \"Not even close!\"\nassert torch.allclose(get_grad_and_zero(logits, loss_bcesm_probs_soft_1), grad_ce_prob), \"sudo rm -rf /* --no-preserve-root\""
  },
  {
    "objectID": "posts/binary-classification/binary-classification.html#attention-and-padding-masks",
    "href": "posts/binary-classification/binary-classification.html#attention-and-padding-masks",
    "title": "Comparing Various Cross-Entropy Implementations and Attention Masks Double Feature",
    "section": "Attention and Padding Masks",
    "text": "Attention and Padding Masks\nPart two of this post is making sense of all the different types of attention masks inside both the transformers library as well as base pytorch.\nLets start with imo the easiest to understand, the transformers padding mask. I’ll first load a tokenizer and some random text and pass the text through the tokenizer.\n\nfrom transformers import AutoTokenizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# example list of different texts:\ntexts = [\n    \"I am a sick man...I am a wicked man.\",\n    \"And if you be a philosopher, though seated in the whale-boat, you would not at heart feel one whit more of terror, than though seated before your evening fire with a poker, and not a harpoon, by your side.\",\n    \"It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.\",\n    \"We thought we were the only thinking beings in the universe, until we met you, but never did we dream that thought could arise from the lonely animals who cannot dream each other's dreams.\"\n]\n\nKV_texts = [\n    \"Still less did that genius, Napoleon, know it, for no one issued any orders to him.\",\n    \"Someone must have been telling lies about Josef K., he knew he had done nothing wrong but, one morning, he was arrested.\",\n    \"...this thought, which had until then been but a doubt, became a conviction, and his last words were, 'Maximilian, it was Edmond Dantès!'\",\n    \"Like, you take over a small public company and you announce that you are raising $200 million to buy MicroStrategy Inc. stock. Maybe that would be worth $400 million?\"\n]\n\ninputs = tokenizer(\n    texts,\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\",\n    add_special_tokens=False\n)\n\nkv_inputs = tokenizer(\n    KV_texts,\n    padding=True,\n    truncation=True,\n    return_tensors=\"pt\",\n    add_special_tokens=False\n)\n\n\n\n\n\n\n\n\n\n\ninputs has a ‘attention_mask’ key that contains a boolean mask where 1 means ‘this token can be attended to’. This mask is necessary to pass batch inputs to models since different texts can be of different length, and we need to pad the shorter sequences to the max sequence length but also track which tokens are padding and should not be attended to.\n\nprint(inputs['attention_mask'])\nprint(inputs['attention_mask'].shape)\n\n# then we'd pass the attention masks to some huggingface transformers model like:\n# outputs = model(**inputs)\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0]])\ntorch.Size([4, 49])\n\n\nYou may have also heard of a ‘causal mask’ used autoregressive models. Huggingface transformers usually handles this type of attention masking internally in decoder style models.\n\nPyTorch Attention\nPyTorch handles attention differently than huggingface, I mean christ lets look at the docs of torch.nn.MultiheadAttention and torch.nn.TransformerDecoderLayer. They have the following arguments for attention masks:\ntorch.nn.MultiheadAttention\n\nkey_padding_mask (Optional[Tensor]) – If specified, a mask of shape (N,S) indicating which elements within key to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query, shape should be (S). Binary and float masks are supported. For a binary mask, a True value indicates that the corresponding key value will be ignored for the purpose of attention. For a float mask, it will be directly added to the corresponding key value.\nattn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape (L,S) or (N⋅num_heads,L,S), where N is the batch size, L is the target sequence length, and S is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary and float masks are supported. For a binary mask, a True value indicates that the corresponding position is not allowed to attend. For a float mask, the mask values will be added to the attention weight. If both attn_mask and key_padding_mask are supplied, their types should match.\n\ntorch.nn.TransformerDecoderLayer\n\ntgt_mask (Optional[Tensor]) – the mask for the tgt sequence (optional).\nmemory_mask (Optional[Tensor]) – the mask for the memory sequence (optional).\ntgt_key_padding_mask (Optional[Tensor]) – the mask for the tgt keys per batch (optional).\nmemory_key_padding_mask (Optional[Tensor]) – the mask for the memory keys per batch (optional).\n\nFor fucks sake….okay lets go through these. I’ll begin by making these modules and some embeddings of the text I created above that we’ll use as input. We need a Q, K, V to shove through these attention-based layers. Additionally, I’ll keep with the pytorch documentation and use \\(L\\) to denote the ‘target’ (query) sequence length and \\(S\\) to denote the ‘source’ (key/value) sequence length.\n\nd_model = 16\n\n# sequence length for the target (Q)\nL = inputs['input_ids'].shape[1]\n\n# sequence length for memory/source (K-V)\nS = kv_inputs['input_ids'].shape[1]\n\nmha = nn.MultiheadAttention(embed_dim=d_model, num_heads=4, batch_first=True)\ndecoder = nn.TransformerDecoderLayer(d_model=d_model, nhead=4, batch_first=True)\n\nemb_layer = torch.nn.Embedding(tokenizer.vocab_size,d_model)\n\ntarget = emb_layer(inputs['input_ids']) # Q\nmemory = emb_layer(kv_inputs['input_ids']) # K, V\n\nFirst up, the attention arguments for MultiHeadedAttention, we have key_padding_mask and attn_mask.\nkey_padding_mask is pretty self-explanatory: which elements of the K matrix in attention should we treat as padding, so as not to attend to them? Unlike transformers, pytorch expects a boolean mask where True indicates that we treat that element as padding, or a float mask that is added element-wise before the softmax in the attention operation. We can create either of these from the transformers padding mask like so:\n\nkey_padding_mask = kv_inputs['attention_mask'].bool() == False\n\n# as float\nkey_padding_mask_ft = kv_inputs['attention_mask'].clone().float() \nkey_padding_mask_ft[key_padding_mask_ft == 0] = -torch.inf\nkey_padding_mask_ft -= 1\n\noutput = mha(target, memory, memory, key_padding_mask = key_padding_mask)\n\nThis essentially zeros out elements after the softmax operation, which we can see in the returned attention weights from the MultiHeadedAttention forward method in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Cross attention weights with key padding mask evident as dark purple strip on the right, e.g. bottom right has no key padding mask. Strips of equal color among the non-zero attention weights are padding tokens for the query.\n\n\n\n\n\nOkay, what about the attn_mask? This is a more flexible mask that can determine which positions in the ‘source’ sequence are attended to. For instance, perhaps we want something like a causal mask in self attention. Below I use MultiheadAttention to perform self attention with a causal plus padding mask Figure 2.\n\nattn_mask = torch.nn.Transformer.generate_square_subsequent_mask(L)\nkey_padding_mask_ft = inputs['attention_mask'].clone().float() \nkey_padding_mask_ft[key_padding_mask_ft == 0] = -torch.inf\nkey_padding_mask_ft -= 1\n\noutput = mha(target, target, target, key_padding_mask = key_padding_mask_ft, attn_mask = attn_mask)\n\n\ncmap = mpl.colormaps.get_cmap('viridis')  # viridis is the default colormap for imshow\ncmap.set_bad(color='black')\n\nfig, axs = plt.subplots(2,2, figsize=(10,10))\n\nfor i, ax in enumerate(axs.ravel()):\n    tmp_out = output[1][i].clone()\n    tmp_out[tmp_out == 0] = float('nan')\n    ax.imshow(tmp_out.detach().numpy(), cmap=cmap)\n    ax.axis('off')\n\nfig.subplots_adjust(wspace=0.05, hspace=0.05)\nfig.tight_layout()\n\n\n\n\n\n\n\nFigure 2: Attention weights of 4 sequences in self attention with causal mask and padding mask. Longest sequence is in the top right with familiar causal mask shape. Other sequences are truncated due to padding.\n\n\n\n\n\n\nNow for the arguments to torch.nn.DecoderLayer - a lot of this is analagous to the arguments in MultiheadAttention. We have tgt_mask and memory_mask, which are both analagous to attn_mask in MultiheadAttention, in that they apply an element-wise multiplicative boolean or additive float causal mask before the softmax operation. tgt_key_padding_mask and memory_key_padding_mask are analagous to key_padding_mask in MultiheadAttention.\nOkay but why are there two? The simple answer is that the original decoder block from Vaswani et al. (2017), which is implemented in DecoderLayer, has both a self-attention and cross-attention block3. The arguments with tgt prefix handles attention masking in the self-attention layers, and the arguments with memory prefix handle attention masking in the cross attention. If you look at the source for DecoderLayer, there are simply two MultiheadAttention layers that are passed (target, target, target) and (target, memory memory) for Q, K, V and the tgt and memory masks respectively. The ‘normal’ use case in a decoder is to pass both padding masks, as well as a causal mask for the self-attention layers. Note that in the cross-attention block, we need to provide a dimension \\((L, S)\\) mask, which is a bit weird…you probably shouldn’t be using this mask.\n\nmemory_mask = torch.nn.Transformer.generate_square_subsequent_mask(S)\n# pad L-S rows of zeros to memory_mask to make it the 'right' shape.\nmemory_mask = torch.cat([memory_mask, torch.zeros(L-S, S)], dim=0)\n\ntgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(L)\n\ntgt_key_padding_mask = inputs['attention_mask'].clone().float()\ntgt_key_padding_mask[tgt_key_padding_mask == 0] = -torch.inf\ntgt_key_padding_mask -= 1\n\nmemory_key_padding_mask = kv_inputs['attention_mask'].clone().float()\nmemory_key_padding_mask[memory_key_padding_mask == 0] = -torch.inf\nmemory_key_padding_mask -= 1\n\n# example with just a padding mask\nout_pad = decoder(\n    target,\n    memory,\n    tgt_key_padding_mask=tgt_key_padding_mask,\n    memory_key_padding_mask=memory_key_padding_mask\n)\n\n# causal mask plus both paddings (this is the 'usual' use case)\nout_tgt = decoder(\n    target,\n    memory,\n    tgt_mask=tgt_mask,\n    tgt_key_padding_mask=tgt_key_padding_mask,\n    memory_key_padding_mask=memory_key_padding_mask\n)\n\n# mask for just the memory, not sure when this would be used.\nout_mem = decoder(\n    target,\n    memory,\n    memory_mask=memory_mask,\n    tgt_key_padding_mask=tgt_key_padding_mask,\n    memory_key_padding_mask=memory_key_padding_mask\n)\n\nOne question you might have (“ahhh no I don’t have that question actually bec…” - “SILENCE!”) is why there is not a separate mask for the query in the MultiheadAttention module. That is, we prevent attention to certain indices of V by masking them out with a padding mask and another more flexible mask. This is because we dont attend to anything in Q. Q determines, through dot-products with the values of K, which elements of V we attend to. There is no notion of whether we attend to the \\(i\\)-th element in the sequence of Q, it is simply the thing that determines how much we attend to the other thing (the values V, which happen to be the same thing in self attention).\nStill, what about the padding mask? Certainly we must ignore certain elements of the ‘target’ sequence. Sure, when you’ve got your output of target sequence length \\(L\\), just throw away the padded inputs. Returning to the previous example:\n\nout = mha(target, target, target, key_padding_mask = key_padding_mask_ft, attn_mask = attn_mask)\n\nout_pad_mask = (out[0] * inputs['attention_mask'].unsqueeze(-1)) # (B, L, D) x (B, L, 1) broadcasting last dimension\n\n# suppose we want to average:\ndenoms = inputs['attention_mask'].sum(axis=-1)\nout_mean = out_pad_mask.sum(axis=1) / denoms.unsqueeze(-1)"
  },
  {
    "objectID": "posts/binary-classification/binary-classification.html#footnotes",
    "href": "posts/binary-classification/binary-classification.html#footnotes",
    "title": "Comparing Various Cross-Entropy Implementations and Attention Masks Double Feature",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI got married, and am having a kid in less than 5 weeks, so will probably be a while again.↩︎\nBCEWithLogitsLoss has better numerical stability, and so is preferred over nn.Sigmoid + BCELoss.↩︎\nLike in that picture from the paper! No I’m not going to show it, you’re going to remember or Google it.↩︎"
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html",
    "href": "posts/omscs-grad/omscs-grad.html",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "",
    "text": "First of all, happy new year, wow, 2025. This opening is quickly dated as no one besides me is gonna be reading this during January 2025, but anyway. What happened to me in 2024? Well one big one was that I graduated from Georgia Tech’s online M.S. in Computer Science (OMSCS) with a specialization in Machine Learning. Other things included getting engaged1 to my wonderful girlfriend, it’ll happen sometime in 2025, probably around early June, perhaps more on that later. This posts thumbnail is a drawing by her daughter. Today I’m giving my ‘review’ of the OMSCS along with some of the highlights of my experiences during the program."
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html#stuff-i-liked",
    "href": "posts/omscs-grad/omscs-grad.html#stuff-i-liked",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "Stuff I liked",
    "text": "Stuff I liked\nThis program is comically cheap, like I think the total cost is between 6-7 thousand USD at the moment. No competitors for online masters programs in CS come close to this. The asynchronous nature of the program is very convenient, though you need to make sure to force yourself to watch the lectures or go to office hours if necessary.\nI learned some cool new stuff and felt like I solidified my knowledge in other areas. Some of my favorite classes:\nReinforcement Learning. A topic I find really awesome and have done some self-directed learning with previous to OMSCS. I need no encouragement to re-read Sutton & Barto’s excellent book on RL which is used in the course. Lectures are entertaining but not for everyone, projects are okay but big thumbs down for group projects.\nHigh Dimensional Data Analysis. This was one of the more mathy classes and introduced a lot of techniques I was unfamiliar with for high dimensional data. Functional PCA, \\(p &gt; 2\\) tensor decomposition, group LASSO. Lots of good review of some familiar concepts as well like LASSO, ridge, robust PCA.\nGame AI. A pretty easy class, but the projects were just very fun and the material pretty interesting. Cool to get a look at the ‘Good ol fashioned AI’ still used in a lot of games. Fun to mess around in Unity and C# which I’ve never touched previously.\nGraduate Algorithms. I’ve never taken a formal algorithms course, so this had a lot of new and interesting material for me. Now, there are some…things about this class that I’ll talk about later, but for now I’ll just note that the material is interesting and the TA’s are pretty involved."
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html#stuff-that-is-maybe-not-so-great",
    "href": "posts/omscs-grad/omscs-grad.html#stuff-that-is-maybe-not-so-great",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "Stuff that is maybe not so great",
    "text": "Stuff that is maybe not so great\nRigor. The program is possibly not as rigorous as most in-person masters programs. I’m comparing it to my experience with in-person classes for my undergrad and stats masters, where the assignments and tests just seemed more frustrating/difficult. Now this is possibly just because I’ve become a better student along the way, but that is my feeling. The material can still be difficult, but there is definitely sentiment on the OMSCS reddit that things are not as challenging, or comparable to a tough undergraduate curriculum. For example, there is a general opinion that graduate algorithms is as hard or easier than many undergraduate algorithms classes.\nEngagement. There is some level of community centered around class boards and Slack channels, but obviously it’s a bit more difficult to meet up for lunch. Often the professors are completely MIA, basically outsourcing the administration of the class to an army of TA’s. In my experience the TA’s mostly did an admirable job, however some classes you really are just navigating the material by yourself.\nCheating. The possibility of cheating is omnipresent. There is simply no way to mimic the in-person test taking experience while making sure no one is cheating. A sufficiently motivated cheater will be able to cheat effectively. The program has some ham-fisted methods for trying to prevent cheating. One is Honorlock, essentially a virus you install on your browser that records your screen/audio to prevent low-effort cheating. I guess this is fine since without it a lot of people would just google a lot of answers. They have other opaque methods that take coding submissions and compare them against some database(?) of known code. As you can imagine this will probably produce some false positives."
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html#some-anecdotes-from-my-experience",
    "href": "posts/omscs-grad/omscs-grad.html#some-anecdotes-from-my-experience",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "Some Anecdotes From my Experience",
    "text": "Some Anecdotes From my Experience\nGraduate Algorithms Cheating Accusation Fiasco. As I said, OMSCS uses plagiarism detection tools. In the semester (Fall 2024) I took this class a large number of people were flagged for plagiarism on a couple assignments and sent to OSI, a place where you can make your case against cheating accusations from staff. There was a ton of hubbub around this on Reddit/Slack (Search the OMSCS subreddit for posts tagged 6515 around Fall 2024). Essentially people felt that certain assignments were almost guaranteed to produce false positives, as they are common problems with known solutions. Eventually many were acquitted in OSI, but it seems to have prompted some changes in the grading (homeworks are now 0%, tests 90%).\nOther drama included people being very pissed at some of the attitudes of the TA’s, going to far as to call them out by name on Reddit/class review sites, and tension around harsh grading and mistakes in some of the quizzes. I am sympathetic to both sides (cowardly I know) here, on one hand it really sucks to be stressed about one of the harder classes and possibly be accused of cheating (if you didn’t cheat), but on the other hand the TA’s have to put up with a bunch of nonsense on top of the day-to-day administration of the class which includes forum discussion, grading, regrading, watching probably mind-numbingly boring proctoring footage etc.\nDoing Assignments in Hotels/Airplanes A couple of vacations I booked without thinking lined up with the final exams/projects for a couple classes. While it sucks to have to finish up a final assignment in a hotel, airplane, or just away from home, there’s a weird feeling of accomplishment when it all works out.\nStress for Nothing. In RL I was stressed about my final exam score (74) before learning it was the highest score. In Machine Learning I was stressed I would not get the B and have to retake it, ended up getting an A. In general the grading is forgiving, but may bruise your ego a bit."
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html#final-thoughts",
    "href": "posts/omscs-grad/omscs-grad.html#final-thoughts",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nOverall OMSCS was a good experience for me. I feel like I learned a lot for very cheap, the quality was mostly high, and though my motivation tapered off towards the end, it kept me engaged which was what I wanted. Career-wise do I think it will help me? I think a little bit, but CS degrees are the shovels being sold to would-be prospectors in the current age, and I feel like their value is not what it used to be. My motivation was to just force myself to learn by paying someone and having a bit of structure, I’m not too concerned about the sticker. My read is that if you are using OMSCS as a ticket to a career switch into software engineering/data science it will not be so effective.\nAlso as mentioned in the title my work immediately doubled by pay, fired my work enemies, and showered me with accolades."
  },
  {
    "objectID": "posts/omscs-grad/omscs-grad.html#footnotes",
    "href": "posts/omscs-grad/omscs-grad.html#footnotes",
    "title": "Doubling your pay by doubling your masters degrees",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe just agreed to get married sometimes next year, it was not as formal as it sounds. Update 2025-07-18: Am now married.↩︎"
  },
  {
    "objectID": "posts/ftx-blowup/index.html",
    "href": "posts/ftx-blowup/index.html",
    "title": "FTX Blowup and Massive Egos on Twitter",
    "section": "",
    "text": "The last 24 hours on crypto-Twitter have been a cacophony of rage, disbelief and whatever toxic stew of emotions exist in the hearts of crypto traders as the crypto exchange FTX has imploded after some FUD lead to actually-its-not-just-FUD-induced-selling/panic of the Chuck-E-Cheese tokens that made up the majority of assets on the balance sheet of the closely related trading desk Alameda Research. It was shortly thereafter that FTX itself was revealed to be insolvent, with a hole likely billions deep. More competent people than me will surely be releasing detailed postmortems in the coming days with the technicals of how this happened, though I’m almost sure it will boil down to some very, very brazen lies about what was being done with peoples money. Instead I’d like to talk (at bro-science level) about the pathology of the just spectacular lineup of characters who did totally legitimate business are are now totally not trying to evade authorities.\nI’ve written previously on my lightweight adventures in crypto, and though I’ve been out from a financial standpoint since ETH was $2000 and still had one more dance with $4000 left in it, I follow the various personalities in the crypto space, partly from a morbid interest in the strange specimens that inhabit the space, and partly from a lingering belief that there just might be something to crypto.\n\nSince I began to exit my crypto holdings, I’ve been a pretty big cynic about the space, however out of some (possibly misplaced) desire to maintain a nonzero level of optimism, I look for the honest, –or at least non-cringe– players in the space. The ones who will still be around if the meme-fueled casino ever decides to offer something of value.\nI initially found two candidates for groups of crypto people (sorry cryptographers, they are the crypto people now) that would let me proudly say I never completely lost faith in crypto once we are all paying our taxes in digital currency issued by His Excellency. These groups can be roughly categorized as the degens and the (self-proclaimed) builders.\nIn a result that reveals the true heart of crypto, the degens are the ones that still make me smile about crypto. They are lone-wolf traders, brash, pugnacious, but harmless; refreshingly self-aware and under few illusions (few, not none, as the FTX implosion has shown) about the game they are playing. It is just that to them – a game, and most seem to just be having fun and don’t wish anything bad on anyone. The most they might be blamed for is that they serve as an extra light that lures hapless retail traders into the electrified mesh. Finally, to their credit, I believe that until now they were not fooled by the latter group.\n\nThe second group, consisting of high-profile heads of exchanges, trading desks and other crypto ‘services’, never gave me the warm fuzzy feeling that the degens did, but boy did they try. Their Twitter accounts are (were) non-stop thread-presses, expounding their deep philosophical and technical wisdom. However the last year has exposed many of these characters (through the collapse of their projects, not just batshit tweets) as nothing more than reckless –though sufficiently intelligent– gamblers with massive egos.\n\n\n\nUntold wisdom awaits in their threads.\n\n\nEvery time one of them posted these insufferable threads clearly meant to keep their sense of self-importance fully inflated, I would try –as I did with the whole crypto space– to maintain a bit of belief that I might be being too cynical, that these people really were trying their best to build something good, but were possibly deluded about the fact that they were just in the ponzi-as-a-service industry. I said “hey if Sam Bankman-Fried is just transferring money from gamblers to animal-welfare causes (he wasn’t), then I guess I’m okay with that”.\nNever has my cynicism been more validated, and no doubt more validation is to come as the tide continues to recede. The more shocking thing is that these people, after they have been revealed as intellectual and legal frauds, continue to post Twitter threads about what they have learned and how they wish to impart their newfound wisdom as a result of the mistakes they’ve made rough times they’ve endured.\nSomething that keeps me reading their insane rants is a morbid desire to know whats going on in there. When, after incinerating people’s money, they try to explain their remorse and how they had the best intentions, what is their intent?\n\ngenuinely explain to people how things went bad, but that they didn’t want to hurt anyone\nThey have drunk their own Kool-Aid and are trying to prevent the air from rushing out of their inflated sense of self-worth.\nSetting up for the next con .\nMake tweets that sound good, hope that they appear in court as evidence, and that that might help them?\n\nI know its not healthy to spend too much time contemplating this, but you do sometimes just want to see the mask fall of and have them brokenly admit a-la Crime and Punishment that they are the murderer. The people such as His Excellency Justin Sun, though grotesque, at least have the advantage of being so outrageous that there is never any doubt that they are just pure grifters, obtaining their adrenaline rush from getting over on people and spitting in the face of fate.\nThe worst part is that the FTX blowup seems to have been the last straw for the degens. On Twitter, they seem exhausted, dejected, ready to call it quits. Their humorous banter is no longer there when I’m sipping coffee in the morning. Who now will bring a smile to my face about crypto?"
  },
  {
    "objectID": "posts/diambra-colab/index.html",
    "href": "posts/diambra-colab/index.html",
    "title": "Diambra Agents on EC2 and Google Colab",
    "section": "",
    "text": "SAD TIMES SPOILER: There is no expert agent at the end of this blog post, really its mostly about interfacing between multiple docker containers running on an EC2 instance and Google Colab. See the example notebook here.\nDiambra is a framework for training reinforcement learning agents on classic video games. In my exploration of the project, I found myself needing a gpu to train CNN based policies and value function approximators. Enter Google Colab, where you can get access to an A100 GPU for 10 bucks a month (granted you probably only have ~20 hours of training on an A100 with your monthly credits before needing to top up). I decided to give this a try since Colab is a nice way to share solutions.\nImmediately there was a problem: Diambra is run with Docker containers, and running these containers on Google Colab is, in my investigations, not well supported/hacky. My solution was to attempt to run the containers on a separate machine, accessible by the Colab instance. The Colab instance would then send actions to the Diambra container over HTTPS, and receive observations and rewards in return. As we’ll see, this solution runs into some issues, hopefully solvable by someone reading this!"
  },
  {
    "objectID": "posts/diambra-colab/index.html#single-environment-setup",
    "href": "posts/diambra-colab/index.html#single-environment-setup",
    "title": "Diambra Agents on EC2 and Google Colab",
    "section": "Single Environment Setup",
    "text": "Single Environment Setup\nFirst lets see how to get the simplest setup working: 1 container on a remote machine, accessed from Google Colab. The remote machine I used was an AWS EC2 instance.\n\nSet up an EC2 instance\nWe need an ec2 instance to host our containers. It does not have to be beefy, just have enough memory to run some docker containers. I started with a t2.micro running AWS Linux, but we’ll have to spin up more . For a tutorial on how to create/connect to an ec2 instance via SSH, see this tutorial.\nOnce you are up and running and connected, we need to install docker. See this AWS documentation. In short, we run:\n# install docker\nsudo yum update -y\nsudo yum install -y docker\n\n# start the docker daemon\nsudo service docker start\n\n# add the ec2-user to the docker group so you dont have to do sudo docker commands\nsudo usermod -a -G docker ec2-user\nCheck that docker works with docker run hello-world or docker ps.\n\nAdd a Security Group\nI am about to give you some advice that is probably bad, which is to allow all traffic on ports 32000-32999 on your ec2 instance. Under the ‘Security’ tab of your ec2 instance, there should be at least one listed security group. Click on that security group, go to the groups inbound rules and ‘Edit inbound rules’. You should see a screen with an option to ‘Add rule’. The rule we want to add is a Custom TCP rule on ports 32000 - 32999 as shown below:\n\nDo this same thing for the outbound rules, as we will be passing data to and receiving data from the running containers on the ec2 instance.\n\n\nInstall Diambra\nThis setup is exactly like in the Diambra installation docs and the vanilla ec2 instance should have all you need to install diambra. You will, as with running locally, need a copy of a compatible ROM (I use DOA++ in this example). You can copy this to the ec2 instance with scp or rsync from your local machine:\nscp -i ~/.ssh/my-key.pem /path/to/doapp.zip &lt;ec2-public-ip&gt;:&lt;ec2-roms-path&gt;\n\n\n\nRun a Diambra Container\nAnd then specify the roms path as usual in the ec2 instance with the DIAMBRAROMSPATH environment variable.\nWe can run a single diambra container with:\n[ec2-user]$ diambra arena --path.roms /path/to/roms --env.host 0.0.0.0 up\n🖥️  logged in\nv2.2: Pulling from diambra/engine\nDigest: sha256:77ba99e5d7d099fe2db292fc7b4e43da310e1bbdb0bcc3a157810e1f933ec81d\nStatus: Image is up to date for diambra/engine:v2.2\nStored credentials found.\nAuthorization granted.\nServer listening on 0.0.0.0:50051\n127.0.0.1:32768\nMake note of the port it is running on, this will be in the range 32000-32999.\nWe can see the running container with:\n[ec2-user]$ docker ps\nCONTAINER ID   IMAGE                 COMMAND                  CREATED         STATUS         PORTS                      NAMES\n7bf96960255f   diambra/engine:v2.2   \"/bin/diambraEngineS…\"   8 seconds ago   Up 6 seconds   0.0.0.0:32768-&gt;50051/tcp   busy_villani"
  },
  {
    "objectID": "posts/diambra-colab/index.html#connect-from-colab",
    "href": "posts/diambra-colab/index.html#connect-from-colab",
    "title": "Diambra Agents on EC2 and Google Colab",
    "section": "Connect From Colab",
    "text": "Connect From Colab\nFrom colab, we can connect using the diambra api. We will need the ec2 instances IPv4 address/IPv4 DNS. These can be obtained from the main page of your ec2 instance in the AWS console.\nimport diambra.arena\nfrom diambra.arena import EnvironmentSettings\n\n# \nos.environ['DIAMBRA_ENVS'] = \"your-ec2-ipv4-address:port\"\n\nsettings = EnvironmentSettings(\n    difficulty=2,\n    splash_screen=False\n)\n\nenv = diambra.arena.make(\"doapp\", render_mode='rgb_array', env_settings=settings)\nenv.reset()\n\nact = env.action_space.sample()\nstate, reward, done, truncated, info = env.step(act)\nIf we take a look at the state we see the start screen of a fight inside DOA++:\n\nNow you have the ability to run a training loop or pass your environment object to e.g. stable-baselines."
  },
  {
    "objectID": "posts/diambra-colab/index.html#connect-to-multiple-environments",
    "href": "posts/diambra-colab/index.html#connect-to-multiple-environments",
    "title": "Diambra Agents on EC2 and Google Colab",
    "section": "Connect to Multiple Environments",
    "text": "Connect to Multiple Environments\nJust one problem…there is a bit of latency between sending the action to the ec2 instance, and receiving the observations, rewards, etc. back. It’s not much for a single ping, but we’re talking about training an RL agent, requiring millions of agent-environment interactions, and if you add 30ms to every single interaction you can imagine the training is gonna be very slow.\nNo problem, lets spin up a bunch of containers and call them in parallel! How do we do this? Well, basically we need to create a class that spins up multiple environments and calls them all at once. Thanks to @lwneal on github for this example which was basically the template for what follows. We essentially have a function that executes multiple commands from an iterable and then use that for our environment’s reset() and step() functions. See the colab notebook for the code and below for an example of 12 observations pulled from 12 environments running in an ec2 instance:\n\nGreat! There are still some hurdles to jump over however. Remember these are 12 different environments being sampled from, we cant just mix the observations from all of them. Instead, we have to keep track of things like states, rewards, etc. on a per-environment basis. Something like below:\n# store running episodes for each parallel environment\nobs_list = [[] for _ in range(len(env.envs))]\nact_list = [[] for _ in range(len(env.envs))]\nrew_list = [[] for _ in range(len(env.envs))]\nval_list = [[] for _ in range(len(env.envs))]\nlog_prob_list = [[] for _ in range(len(env.envs))]\ndone_list = [[] for _ in range(len(env.envs))]\ntruncated_list = [[] for _ in range(len(env.envs))]\ndiscounted_rewards = [[] for _ in range(len(env.envs))]\nFor example, in PPO, we have to complete a few episodes, compute discounted rewards and advantages, and then form training tuples. We don’t want to just wait til a few episodes are complete, update the policy/value function, and then continue, since then the episodes that did not get used since they did not complete will not reflect the current policy.\nFurther, this process is…..still kinda slow. Colab is a great tool, but this might not be the use case for it (we dont really need that A100), and the cost of spinning up an ec2 instance with a GPU might be worth having the environment running on the same machine that is doing the training and seriously reduce latency issues. I’ll probably give a go at actually training a fighter all on an ec2 instance since, well, I want to actually get this working and I’ve been making some mistakes in training previous PPO agents (incoming amendments to previous posts)."
  },
  {
    "objectID": "posts/minigpt/minigpt.html",
    "href": "posts/minigpt/minigpt.html",
    "title": "I Follow a GPT Tutorial",
    "section": "",
    "text": "ChatGPT is cool and popular. GPT-3 is the starting point for ChatGPT. Lets write something about me training a little version of GPT-3.\nGPT-3 (Brown et al. 2020) has been popular for a while now, and has strangely become it’s own nested version of deep learning as a whole: Everyone was applying deep learning to everything, now everyone is applying GPT-3 to everything, and —related— everyone is applying transformers to everything (vision, audio, point clouds….bleh). I’m reminded of this ‘The Onion’ short where the Large Hadron Collider scientists get bored and are throwing random things into it: ‘…last week we threw a bird in there…’. 10% of machine learning research now seems to be “what if we trained a transformer like this?!”.\nStrangely, GPT-3’s architecture is not all that complicated, or rather not much more than the original transformer architecture, and reading the paper and having it explained (“its just the decoder part…”) made me think that there must be more to it. Well, of course there is y’know all that engineering to train on 1000+ GPUs and such, but anyhow…\nI wanted to actually run the code to see if there was an ‘ah, its a bit more complicated’ moment. I stumbled across Andrej Karpathy’s tutorial video about training a mini version of GPT-3 so I decided to watch and see if I could replicate the Shakespeare-sounding gobbledeygook coming out of the final model, and get some more practice with weights-and-biases along the way.\nOk, so obviously you should probably just go watch his video?…but…uh…I dunno maybe stick around for my sharp wit? I didn’t follow his build-the-transformer-from-scratch bits, which are good, but I’ve gone through all that already. Really I just wanted to do the things that make GPT-3 what it is:\nThe above three things plus scale are indeed almost all of what makes GPT-3 special. Here’s a link to the colab notebook for training this thing up. For fun I did some hyperparameter tuning as well.\nOk so I’ll briefly cover 1 - 3, but just go watch the video, it’s great.\n(Aside: Andrej Karpathy’s video also convinced me to get Github Copilot, it auto suggested ‘just go watch the video, it’s great’ at the end of the last sentence. I’m not sure if I should be impressed or scared. I’m going to go with impressed. &lt;- These two sentences also generated by copilot. This entire blog is written by copilot. Haha, just kidding, or am I? What is real? This is a blog post about GPT model partially written by a GPT-based model aaaaaaaaaaaaaaaaaaaaaaa!)\n1. Basically, specify some maximum context length \\(N_{max}\\), grab \\(n + 1: n &lt; N_{max}\\) ‘tokens’ (words, characters, subword tokens, etc), and make your input: tokens 1 to \\(n\\), and your targets tokens 2 to \\(n + 1\\). Something like this:\n2. For masked self attention, we just want to pass an attention mask that makes it so every token can only attend to itself and tokens before it. The format is a bit strange, but it’s just a boolean tensor where the upper triangle is True and the lower triangle including diagonal is False (See the docs for further options). Here’s a quick way to make one:\n3. For generating text, we just need to start with some prompt, feed it in and get a prediction of the next token for every input token, then feed in the prompt + the predicted token, and get the next prediction, and so on."
  },
  {
    "objectID": "posts/minigpt/minigpt.html#hyperparameter-tuning",
    "href": "posts/minigpt/minigpt.html#hyperparameter-tuning",
    "title": "I Follow a GPT Tutorial",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nI sweep over hidden embedding dimension, learning rate, and batch size. The results of one of the sweeps can be seen here. There doesn’t seem to be much of a correlation between the hyperparameters and the validation loss. If I was doing another sweep I’d probably try varying dropout or max sequence length to see if I could recreate the validation loss.\n\n\n\n\n\n\nFigure 1: Hyperparameter Combinations and Corresponding Validation Loss\n\n\n\n\nGenerated Text\nI use the model from the run Radiant Rabbit to generate some text. The text is generated as described in 3 above, with a prompt of “LEONTES:”, a max length of 1000, and a block size of 128. Notice below I am passing an increasingly large mask and feeding a maximum of 128 tokens at a time. The model has not seen sequences longer than 128 tokens, and would begin to produce nonsense (or rather, even more nonsensical nonsense) beyond that length.\nidx = torch.tensor(encode(\"LEONTES:\\n\")).unsqueeze(0).to(device)\nblock_size = 128\n\nfor i in range(1000):\n    attn_mask = torch.triu(torch.ones(idx.size(1), idx.size(1)), diagonal=1).bool()\n    attn_mask = attn_mask[:block_size, :block_size].to(device)\n\n    logits = m(idx[:, -block_size:], attn_mask = attn_mask)\n    logits = logits[:, -1, :]\n    probs = F.softmax(logits, dim=-1)\n    idx_next = torch.multinomial(probs, 1)\n\n    idx = torch.cat([idx, idx_next], dim=1)\nI promised some funny text, here’s a snippet of the decoded output:\nLEONTES:\nTrue!\nO worse of this men it.\n\nDUKE VINCENTIO:\nGentle king him!\n\nProvost:\nNo more of ourself in a say 'nointed,' silent visit,\nIn carrion choose with ever of person;\nHe hath had made her heads, that nature's away:\nTherefore, had I in my vain sweet prevent\nTo see her friends did close his minds.\n\nProvost:\nO though this, if I do wish a sword knowledge\nIn wanton common can my blate to some have\nbased with said, but that it bloody billows,\nRatter lieuted with a moler'd and enmity\nMay have utter'd my heart\nBy from the testy-moning hour, whom\nMore comes not thus? beitterly od married.\n\nMAMILLIUS:\nLear out?\n\nLEONTES:\nNay, God help.\nServing Servant:\nHe's sometime now:\nThe judge hate I, being in become moads,\n'gainst enjoying warps it, and venture,\nThese stocks, tears; and it may be feen gone,\nbut the master of his old return in\nAnd bear me those his grace, that knows be call'd\nThe queen amorous of burthen and walking at him.\nDear most respect this prince hour? If then\npenter this member what h\nDANIEL:\nDaniel out?"
  },
  {
    "objectID": "posts/ppo/index.html",
    "href": "posts/ppo/index.html",
    "title": "Lunar Lander with PPO",
    "section": "",
    "text": "So, PPO….it seems like the transformer of reinforcement learning: “What should we do?”, “Eh, shove it through PPO and see what happens”. So again, this is just me replicating PPO and imploring anyone who got lost on the internet and is reading this to just go read the paper instead: Schulman et al. (2017).\nIf you are just here for some code, here is a colab notebook that runs hyperparameter tuning with wandb and ray-tune on my PPO implementation. At the end you can grab a trained model from wandb, run an episode, and download a gif of the episode."
  },
  {
    "objectID": "posts/ppo/index.html#the-objective",
    "href": "posts/ppo/index.html#the-objective",
    "title": "Lunar Lander with PPO",
    "section": "The Objective",
    "text": "The Objective\nMmmm, ok again….the paper….but I’ll go over the main pieces presented therein. PPO is, as it’s name suggests, a policy gradient algorithm, in the sense that it tries to maximize expected performance (total reward) by directly leveraging the gradient of the performance with respect to the policy parameters. That is, we want to maximize \\(J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\) and can derive an expression for the gradient \\(\\nabla_{\\theta} J(\\pi_{\\theta})\\) that we can approximate by sampling episodes and computing gradients. For a good intro to policy optimization, I like this intro by OpenAI.\nPPO is different from ‘vanilla’ policy optimization in that it maximizes some weird surrogate objective function:\n\\[\nL^{CPI}(\\theta) = E_{t \\sim \\pi_{\\theta_{old}}}\\left[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A(s_t, a_t)\\right]\n\\]\nwhere \\(A(s_t, a_t)\\) is the advantage function, which is the difference between the value function at time step \\(t\\) and some bootstrapped version of the value function at time step \\(t\\), which I’ll show later. The subscripts \\(\\theta\\) and \\(\\theta_{old}\\) indicate the parameters that will be updated, and a ‘frozen’ version of the parameters that will be used to compute the ratio of the new policy to the old policy. In words, the part of the objective inside the expectation says if the action we took at time step \\(t\\) was better than we currently expect (positive advantage), increase the probability of that action. If it was worse than we expect (negative advantage) decrease the probability of that action. Each example inside the expectation is sampled according a policy with the old parameters, \\(\\pi_{\\theta_{old}}\\).\nOkay, so if you stare at that loss there, you might notice that the algorithm will probably want to make very large updates to \\(\\theta\\). PPO addresses this by ‘clipping’ the probability ratio to prevent these large updates, like so:\n\\[\nL^{CLIP}(\\theta) = E_{t \\sim \\pi_{\\theta_{old}}}\\left[\\text{min}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A(s_t, a_t), \\text{clip}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)A(s_t, a_t)\\right)\\right]\n\\]\nWhere clip\\((x, a, b)\\) is a function that clips \\(x\\) to be between \\(a\\) and \\(b\\) and \\(\\epsilon\\) is a hyperparameter that controls the size of the clipping.\nOne thing that was weird to me, is that this loss does not have the log probability in the expectation there, as is seen in the derivation of the vanilla policy gradient. Well, actually, it sort of does, but it’s just easier to write it the way they do, see this blog post for an explanation."
  },
  {
    "objectID": "posts/ppo/index.html#computing-as_t-a_t",
    "href": "posts/ppo/index.html#computing-as_t-a_t",
    "title": "Lunar Lander with PPO",
    "section": "Computing \\(A(s_t, a_t)\\)",
    "text": "Computing \\(A(s_t, a_t)\\)\nIn the paper they mention two ways of computing the advantage. One is to simply compute the difference between the value function at time step \\(t\\) and the rewards-to-go from time step \\(t\\) plus the value function at the last time step:\n\\[\n\\hat{A}(s_t, a_t) = -V(s_t) + \\sum_{t'=t}^{T}r_{t'} + V(s_T)\n\\]\nAs mentioned in the paper, we can compute a weighted version of this return Schulman et al. (2018) Sutton and Barto (2020) as so:\n\\[\n\\hat{A}(s_t, a_t) = \\sum_{t'=t}^{T}(\\gamma \\lambda)^{t'-t}\\delta_{t'}\n\\]\nWhere \\(\\delta_{t'} = r_{t'} + \\gamma V(s_{t'+1}) - V(s_{t'})\\) is the TD error at time step \\(t'\\) and \\(\\lambda\\) is a hyperparameter that controls the weighting of the TD error. I show my implementation of this and highlight a mistake I made.\nFirst, lets assume I’ve calculated the rewards-to-go, \\(G_t = \\sum_{t'=t}^T \\gamma^{t'-t}r_t'\\), and the value function, \\(V(s_t)\\), for each time step \\(t\\). First, I compute the deltas:\n# given tensor of values val_tensor and tensor of rewards-to-go dsc_rews\ndeltas = gamma*F.pad(val_tensor[1:], (0, 1), 'constant', 0) + torch.tensor(rew_list) - val_tensor\nFrom this, I can compute the advantages by starting from the last time step, working backwards, and accumulating the weighted deltas:\n# compute advantages from deltas\nadvantages = []\ncur_adv = 0\nfor t in range(len(deltas)):\n    cur_adv = deltas[-(t+1)] + gamma * lam * cur_adv\n    advantages.append(cur_adv)\n\nadvantages = reversed(torch.tensor(advantages, dtype=torch.float32))\nYou can use a similar to collect-and-reverse scheme to compute the rewards to go, or you can be an idiot and forget to do that, like me:\ndsc_rews = []\ncur_rew = 0\n\n# compute discounted rewards\nfor t in range(len(rew_list)):\n    cur_rew = rew_list[-t] + gamma * cur_rew\n    dsc_rews.append(cur_rew)\nBasically my rewards to go were backwards…christ, look, the indexing isn’t even correct. Consider the situation where I have a large negative reward at the end of the episode, the first action will take the full weight for that failure. Unsurprisingly, my LunarLander agent was a bit timid about approaching the surface. However, shockingly, the algorithm still seemed to learn something with this glaring mistake, though it was incredibly inconsistent. My intuition is that successful episodes would still propagate positive reward signal to all state action pairs, even if the actual values were inaccurate, so the agent was still able to attempt to learn good behavior.\nOkay, so now the correct way to compute the rewards to go:\ndsc_rews = []\ncur_rew = 0\n\n# compute discounted rewards\nfor t in range(len(rew_list)):\n    cur_rew = rew_list[-(t+1)] + gamma * cur_rew\n    dsc_rews.append(cur_rew)\n\ndsc_rews = reversed(torch.tensor(dsc_rews, dtype=torch.float32))"
  },
  {
    "objectID": "posts/ppo/index.html#computing-the-loss",
    "href": "posts/ppo/index.html#computing-the-loss",
    "title": "Lunar Lander with PPO",
    "section": "Computing the Loss",
    "text": "Computing the Loss\nOkay so we have the advantages, the rewards-to-go, and some log-probabilities. We need to compute that probability ratio thing, multiply it by the advantage and then take the minimum of that and its clipped version. An important thing to remember is that when forming term \\(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}{A}(s_t, a_t)\\) the only thing that has gradients flow through it is the numerator. The advantage is a static quantity at this point, and the denominator is a frozen version of log-probabilities we saw when collecting experience.\nMy implementation runs the observations back through the actor and critic to get the quantities we need to be differentiated via autograd (everything below is in the colab notebook).\n# cur_batch from a queue of tuples\nobs_batch, act_batch, rew_batch, val_batch, log_prob_batch_prev, dsc_rew_batch, adv_batch = zip(*cur_batch)\n\ndist_batch = actor(torch.tensor(obs_batch, device = device))\n\n# this `requires_grad`, log_prob_batch_prev does not\nlog_prob_batch = dist_batch.log_prob(torch.tensor(act_batch, device=device))\nval_batch = critic(obs_batch.to(device))\nNow I compute the ratio with the differentiable numerator\ndenom = torch.tensor(log_prob_batch_prev, device=device)\nadv_batch = torch.tensor(adv_batch, device=device)\n\nratio = torch.exp(log_prob_batch - denom.to(device))\nAnd compute the minimum of the clipped objective and the unclipped objective, as well as the mean-squared error between the value function estimate and the actual rewards-to-go we observed. The total loss is the negative of the clipped objective plus the value function loss. Note: in the paper they mention an entropy loss to encourage exploration, I show it below, but I don’t use it in my implementation, instead I use a simple exploration scheme where I randomly sample actions at a decreasing probability over training.\nsloss = torch.min(ratio * adv_batch, torch.clamp(ratio, 1.0 - config['eps'], 1.0 + config['eps']) * adv_batch).mean()\nvloss = F.mse_loss(val_batch.squeeze(), torch.tensor(dsc_rew_batch, dtype = torch.float32, device=device))\n\n# possible entropy loss, we would add this to loss\n# entropy = dist_batch.entropy().mean()\n\nloss = -sloss + vloss"
  },
  {
    "objectID": "posts/ppo/index.html#traininghyperparameter-tuning",
    "href": "posts/ppo/index.html#traininghyperparameter-tuning",
    "title": "Lunar Lander with PPO",
    "section": "Training/Hyperparameter Tuning",
    "text": "Training/Hyperparameter Tuning\nI train on the LunarLander environment for 100 rounds of sampling + training, filling a buffer with some number N iterations, and then updating with training batches from that buffer. I also have some fun hyperparameter tuning, varying the following:\nlearning rate (actor and critic), clipping parameter \\(\\epsilon\\), \\(\\lambda\\) parameter in the computation of the advantage, and discount factor \\(\\gamma\\). The results are in this wandb project. Below is the parallel lines plot showing the effect of hyperparameters.\n\n\n\nMany of the training runs stagnate around 0 average reward, others get up to around ~100 average reward. A few seem to be doing okay and then completely collapse…I’m not sure what’s going on here but I’m investigating some methods of alleviating this. Some of the runs maintain a stable reward at over 200, which is what we want to consider it a success. The parameter that training seems most sensitive to is the advantage computation parameter \\(\\lambda\\).\nOkay, below is a gif of an episode from the best training run:\n\nIt has the ‘correct’ behavior of dropping quickly, before engaging the thrusters to land softly. Rather, it does not have some annoyingly not-awful-not-great behaviors such as just hovering in the air, afraid of crashing, or landing and then refusing to turn off thrusters."
  }
]
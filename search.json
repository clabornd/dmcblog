[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Another blog? Why sir?",
    "section": "",
    "text": "I made this blog because I wanted to practice with .qmd documents, as well as have a place to document my learning journey on various data-science/ML topics (I may also post pictures of bread I bake). Also, I needed a way to inflate my ego, and a blog seems like a good way to do that?\n__DISCLAIMER__ This will appear throughout the blog - I am not an expert in anything, I will try to cauch everything in uncertainty. Use any code or follow any conclusions I post at your own risk."
  },
  {
    "objectID": "posts/vision-transformer/vision-transformer.html",
    "href": "posts/vision-transformer/vision-transformer.html",
    "title": "Dumb Mistakes Training Vision Transformer",
    "section": "",
    "text": "Another post about one of the pieces of DALL-E, the vision transformer (Dosovitskiy et al. 2021), which is specifically used as one of the vision encoders in CLIP (Radford et al. 2021), which is itself used to condition the decoding process in DALL-E.\nThe vision transformer is at its base really not all that complicated: An image is divided into patches, and each patch is flattened, sent through an embedding layer, and arranged in a sequence to provide the expected input to a stack of multi-headed-attention + position-wise feed-forward layers identical to the architectures used to process text. Somewhat miraculously, with enough data, the vision transformer learns the spatial nature of the input even without the inductive biases of convolutional networks.\nHere’s a link to the colab notebook for training this thing up.\nI did roughly the following:\n\nEssentially copy the transformer encoder architecture from harvard nlp\nPrepend the patch creation and embedding plus learnable positional embeddings\nAdd the classification token and classification head as described in (Dosovitskiy et al. 2021)\n\n\nComponents added to base transformer architecture\nI implemented an image patcher like so:\nimport torch\nimport einops\n\nclass imagePatches(torch.nn.Module):\n  def __init__(self, patch_size=(8,8), input_channels = 3, stride = 8, embedding_dim=768):\n    super().__init__()\n    self.patch_size = patch_size\n    self.unfold = torch.nn.Unfold(patch_size, stride = stride)\n    self.patch_embedding = torch.nn.Linear(\n        patch_size[0]*patch_size[1]*input_channels, \n        embedding_dim\n    )\n\n  def forward(self, img):\n    patches = self.unfold(img)\n    patches = einops.rearrange(patches, \"b c p -> b p c\")\n\n    embeddings = self.patch_embedding(patches)\n    return embeddings, patches\nThe classification head looked like:\n# ... blah blah module super init\n\ndef forward(self, x:torch.tensor, **kwargs):\n    x = encoder(x, **kwargs)\n    # x = Reduce(\"b p c -> b c\", reduction='mean')(x) #\n    # x = self.layernorm(x)\n    x = self.layernorm(x[:,0])\n    x = self.hidden(x)\n    x = torch.tanh(x)\n    x = self.classification(x)\nI switched back and forth between the mean pooling (commented out) and using only the classification token output before going through the classification layer.\nJust to show it, here’s the class embedding + learnable positional embedding inside the encoder class:\nclass vitEncoder(torch.nn.Module):\n  def __init__(self, n_layers, embedding_dim, image_tokenizer, mha_layer, ff_layer, n_patches):\n    super().__init__()\n    self.image_tokenizer=image_tokenizer\n    self.positional_embedding = torch.nn.Parameter(\n        torch.randn((n_patches + 1, embedding_dim))\n    )\n    self.embedding_dim = embedding_dim\n    self.n_layers = n_layers\n    self.encoder_layers = torch.nn.ModuleList([\n        vitEncoderLayer(copy.deepcopy(mha_layer), copy.deepcopy(ff_layer)) for _ in range(n_layers)\n    ])\n    self.class_embedding = torch.nn.Parameter(torch.randn((1, 1, embedding_dim)))\n    \n  def forward(self, x:torch.tensor, attn_mask:torch.tensor=None):\n    x, _ = self.image_tokenizer(x)\n\n    x = torch.concatenate([\n        einops.repeat(self.class_embedding, \"b t c -> (r b) t c\", r = x.shape[0]), x], \n        axis = 1 # patch sequence axis\n    )\n\n    x = x + self.positional_embedding\n\n    for l in self.encoder_layers:\n      x = l(x, attn_mask)\n\n    return(x)\n\n\nTraining\nFirst attempt:\nEpoch 0, Loss is 2.305\nEpoch 0, Loss is 2.367\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.298\n\n...\n\nEpoch 10, Loss is 2.300\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.312\n… shit. Oh right the positional embeddings.\nEpoch 0, Loss is 2.301\nEpoch 0, Loss is 2.309\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.308\n\n...\n\nEpoch 10, Loss is 2.299\nEpoch 10, Loss is 2.304\nEpoch 10, Loss is 2.311\nEpoch 10, Loss is 2.303\n…damnit. Oh whoops I missed a couple activations.\nEpoch 0, Loss is 2.331\nEpoch 0, Loss is 2.299\nEpoch 0, Loss is 2.297\nEpoch 0, Loss is 2.312\n\n...\n\nEpoch 10, Loss is 2.291\nEpoch 10, Loss is 2.303\nEpoch 10, Loss is 2.312\nEpoch 10, Loss is 2.301\n…shit…\n…\nReminds self this is and EXERCISE and pain is expected. I did eventually get this working, but first a couple bugs I found along the way:\n\nTwo very dumb mistakes implementing the layer normalization (After fixing it I switched to just using torch.nn.LayerNorm)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.w = nn.Parameter(torch.ones(features))\n        self.b = nn.Parameter(torch.zeros(features))\n\n    def forward(self, x, eps=1e-6):\n        return self.w * x.mean(-1, keepdim=True) / (x.std(-1, keepdim=True) + eps) + self.b\nYou see it? Yea I’m not actually mean-centering in the numerator there…\n    ...\n    \n    def forward(self, x, eps=1e-6):\n        self.w * (x - x.mean(-1, keepdim=True)/(x.std(-1, keepdim=True) + eps)) + self.b\nAnd here I’ve just fudged the parentheses…squint a bit and you’ll see.\n\nMultiheadedAttention expects sequence dimension first? Apparently if you dont specify batch_first = True:\n\nattention_layer = torch.nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=n_head, batch_first = True)\nThen the MultiheadAttention treats the first dimension as the sequence dimension of the QKV input…this fails silently since the self-attention matrix multiplication is still valid.\n\nLong story short, I did get the model to train up modestly on MNIST (I’ll try on the other two datasets later). You can see my successful training run against all others here:\nIn the end it was using Adam over SGD in training that got me to see proper training. SGD is known to be very picky about the learning rate. I might try some sweeps over various LR’s and report back.\nAnother (probably related) thing I ran into is that averaging all the outputs of the last layer resulted in some learning with SGD, but taking only the classification token output resulted in zero learning. Probably the learning rate(s) I was using were closer to being appropriate for averaging than with taking just the classification token output.\n\n\n\n\n\nReferences\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], June. http://arxiv.org/abs/2010.11929.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv. https://doi.org/10.48550/arXiv.2103.00020."
  },
  {
    "objectID": "posts/use-shift/index.html",
    "href": "posts/use-shift/index.html",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "",
    "text": "Honestly, what a disaster of a title. I don’t know if either part in isolation would be more likely to get someone to read this, but I just wanted to make a post. Maybe I should have click-baited with something completely unrelated, oh well.\nI’m currently taking Machine Learning for Trading from the Georgia Tech online CS masters program as part of my plan to motivate myself to self-study by paying them money. While much of the ML parts of the class are review for me, it has been fun to learn things about trading, as well as do some numpy/pandas exercises.\\(^{[1]}\\)\nThe class is heavy on vectorizing your code so its fast (speed is money in trading), as well working with time series. I’ll go over two things I’ve found neat/useful. One is vectorizing sampling from a categorical distribution where the rows are logits. The other is using the .shift() method of pandas DataFrames and Series."
  },
  {
    "objectID": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "href": "posts/use-shift/index.html#vectorized-sampling-from-a-categorical-distribution",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "Vectorized sampling from a categorical distribution",
    "text": "Vectorized sampling from a categorical distribution\nOkay, so the setup is you have an array that looks like this:\n\nimport numpy as np\nnp.random.seed(23496)\n\n# The ML people got to me and I now call stuff that sums to 1 'logits'\nlogits = np.random.uniform(size = (1000, 10))\nlogits = logits/logits.sum(axis = 1)[:,None]\n\nlogits[:5,:]\n\narray([[0.13460263, 0.12458665, 0.05453746, 0.11991544, 0.07351353,\n        0.11034637, 0.07374194, 0.11460002, 0.11551094, 0.07864502],\n       [0.18602867, 0.09960763, 0.02422872, 0.10095124, 0.02961313,\n        0.04475981, 0.08855924, 0.11246979, 0.16960986, 0.14417191],\n       [0.0142491 , 0.14630917, 0.11735343, 0.12211442, 0.11230253,\n        0.12474719, 0.13253043, 0.01106296, 0.08627144, 0.13305933],\n       [0.09227899, 0.15207502, 0.07677232, 0.16330634, 0.11855988,\n        0.08710454, 0.05458428, 0.18425363, 0.0224089 , 0.04865609],\n       [0.01826615, 0.1956786 , 0.03484824, 0.12495028, 0.11824123,\n        0.01893324, 0.17954348, 0.15826364, 0.1351583 , 0.01611684]])\n\n\nEach row can be seen as the bin probabilities of a categorical distribution. Now suppose we want to sample from each of those distributions. One way you might do it is by leveraging apply_along_axis:\n\nsamples = np.apply_along_axis(\n    lambda x: np.random.choice(range(len(x)), p=x), \n    1, \n    logits\n)\n\nsamples[:10], samples.shape\n\n(array([3, 1, 6, 7, 8, 9, 2, 4, 5, 3]), (1000,))\n\n\nHm, okay this works, but it is basically running a for loop over the rows of the array. Generally, apply_along_axis is not what you want to be doing if speed is a concern.\nSo how do we vectorize this? The answer I provide here takes advantage of the Gumbel-max trick for sampling from a categorical distribution. Essentially, given probabilities \\(\\pi_i, i \\in {0,1,...,K}, \\sum_i \\pi_i = 1\\) if you add Gumbel distribution noise to the log of the probabilites and then take the max, it is equivalent to sampling from a categorical distribution.\nAgain, take the log of the probabilities, add Gumbel noise, then take the arg-max of the result.\n\nsamples = (\n    np.log(logits) + \\\n    np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1)  \n\nsamples[:10], samples.shape\n\n(array([0, 0, 5, 2, 2, 5, 6, 9, 7, 9]), (1000,))\n\n\nLets test if this is actually faster:\n\n%%timeit\n(np.log(logits) + np.random.gumbel(size = logits.shape)).argmax(axis = 1)  \n\n416 µs ± 501 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\nnp.apply_along_axis(lambda x: np.random.choice(range(len(x)), p=x), 1, logits)  \n\n22.6 ms ± 416 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nYea, so a couple orders of magnitude faster with vectorization. We should probably also check that it produces a similar distribution across many samples (and also put a plot in here to break up the wall of text). I’ll verify by doing barplots for the distribution of 1000 values drawn from 4 of the probability distributions. Brb, going down the stackoverflow wormhole because no one knows how to make plots, no one.\n…\nOk I’m back, here is a way to make grouped barplots with seaborn:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# do 1000 draws from all distributions using gumbel-max\ngumbel_draws = []\n\nfor i in range(1000):\n    samples = (\n        np.log(logits) + np.random.gumbel(size = logits.shape)\n    ).argmax(axis = 1) \n\n    gumbel_draws.append(samples) \n\ngumbel_arr = np.array(gumbel_draws)\n\n# ...and 1000 using apply_along_axis + np.random.choice\napply_func_draws = []\n\nfor i in range(1000):\n    samples = np.apply_along_axis(\n        lambda x: np.random.choice(range(len(x)), p=x), \n        1, \n        logits\n    )\n    apply_func_draws.append(samples)\n\napply_func_arr = np.array(apply_func_draws)\n\nIn the above, if you ran the two for loops separately, you would get a better sense of how much faster the vectorized code is. Now well munge these arrays into dataframes, pivot, and feed them to seaborn.\n\ngumbel_df = pd.DataFrame(gumbel_arr[:,:4])\napply_func_df = pd.DataFrame(apply_func_arr[:,:4])\n\ngumbel_df = pd.melt(gumbel_df, var_name = \"distribution\")\napply_func_df = pd.melt(apply_func_df, var_name = \"distribution\")\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 8))\n\np = sns.countplot(data = gumbel_df, x=\"distribution\", hue=\"value\", ax = axs[0])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[0].set_title(\"Using Gumbel-max\")\n\np = sns.countplot(data = apply_func_df, x=\"distribution\", hue=\"value\", ax = axs[1])\np.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')\naxs[1].set_title(\"Using apply_along_axis + np.random.choice\")\n\nfig.tight_layout()\n\n\n\n\nThe distribution of drawn values should be roughly the same.\n\n\n\n\nEyeballing these, they look similar enough that I feel confident I’ve not messed up somewhere.\nFinally, of note is that a modification of this trick is used a lot in training deep learning models that want to sample from a categorical distribution (Wav2vec(Baevski et al. 2020) and Dall-E(Ramesh et al. 2021) use this). I’ll go over it in another post, but tl;dr, the network learns the probabilities and max is changed to softmax to allow backpropagation."
  },
  {
    "objectID": "posts/use-shift/index.html#pandas.dataframe.shift",
    "href": "posts/use-shift/index.html#pandas.dataframe.shift",
    "title": "Vectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!",
    "section": "pandas.DataFrame.shift()",
    "text": "pandas.DataFrame.shift()\nYou could probably just go read the docs on this function, but I’ll try to explain why its useful. We often had to compute lagged differences or ratios for trading data indexed by date. To start I’ll give some solutions that don’t work or are bad for some reason, but might seem like reasonable starts. Lets make our dataframe with a date index to play around with:\n\nimport pandas as pd\n\nmydf = pd.DataFrame(\n    {\"col1\":np.random.uniform(size=100), \n    \"col2\":np.random.uniform(size=100)}, \n    index = pd.date_range(start = \"11/29/2022\", periods=100)\n)\n\nmydf.head()\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      2022-11-29\n      0.478292\n      0.121631\n    \n    \n      2022-11-30\n      0.664101\n      0.781843\n    \n    \n      2022-12-01\n      0.245395\n      0.005426\n    \n    \n      2022-12-02\n      0.726935\n      0.532795\n    \n    \n      2022-12-03\n      0.658744\n      0.970972\n    \n  \n\n\n\n\nNow, suppose we want to compute the lag 1 difference. Specifically, make a new series \\(s\\) where \\(s[t] = col1[t] - col2[t-1]: t > 0\\), \\(s[0] =\\) NaN. Naive first attempt:\n\nmydf[\"col1\"][1:] - mydf[\"col2\"][:-1]\n\n2022-11-29         NaN\n2022-11-30   -0.117742\n2022-12-01    0.239969\n2022-12-02    0.194140\n2022-12-03   -0.312228\n                ...   \n2023-03-04   -0.214499\n2023-03-05    0.361584\n2023-03-06   -0.199390\n2023-03-07    0.272564\n2023-03-08         NaN\nFreq: D, Length: 100, dtype: float64\n\n\nUh, so what happened here? Well, pandas does subtraction by index, like a join, so we just subtracted the values at the same dates, but the first and last dates were missing from col1 and col2 respectively, so we get NaN at those dates. Clearly this is not what we want.\nAnother option converts to numpy, this is essentially just a way to move to element-wise addition:\n\nlag1_arr = np.array(mydf[\"col1\"][1:]) - np.array(mydf[\"col2\"][:-1])\nlag1_arr[:5], lag1_arr.shape\n\n(array([ 0.54247038, -0.53644839,  0.72150904,  0.12594842, -0.225511  ]),\n (99,))\n\n\nOf course, this is not the same length as the series, so we have to do some finagling to get it to look right.\n\n# prepend a NaN\nlag1_arr = np.insert(lag1_arr, 0, np.nan)\nlag1_arr[:5], lag1_arr.shape\n\n(array([        nan,  0.54247038, -0.53644839,  0.72150904,  0.12594842]),\n (100,))\n\n\nOk, its the same length and has the right values so we can put it back in the dataframe as a column or create a new series (and add the index again)\n\n# make a new series\nlag1_series = pd.Series(lag1_arr, index=mydf.index)\n\n# or make a new column\n# mydf[\"col3\"] = lag1_arr\n\nAlright, but this looks kinda ugly, we can do the same thing much more cleanly with the .shift() method of pandas DataFrames and Series. .shift(N) does what it sounds like, it shifts the values N places forward (or backward for negative values), but keeps the indices of the series/dataframe fixed.\n\nmydf[\"col1\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    0.478292\n2022-12-03    0.664101\n                ...   \n2023-03-04    0.807256\n2023-03-05    0.992331\n2023-03-06    0.974969\n2023-03-07    0.339215\n2023-03-08    0.625530\nFreq: D, Name: col1, Length: 100, dtype: float64\n\n\nWith this we can easily compute the lag 1 difference, keeping the indices and such.\n\n# difference\nmydf[\"col1\"] - mydf[\"col2\"].shift(1)\n\n# lag 3 ratio\nmydf[\"col1\"]/mydf[\"col2\"].shift(3)\n\n2022-11-29         NaN\n2022-11-30         NaN\n2022-12-01         NaN\n2022-12-02    5.976563\n2022-12-03    0.842553\n                ...   \n2023-03-04    0.478158\n2023-03-05    0.635103\n2023-03-06    0.694335\n2023-03-07    0.804248\n2023-03-08    3.747774\nFreq: D, Length: 100, dtype: float64\n\n\nThis lag-N difference or ratio is extremely common and honestly I can’t believe I hadn’t been using .shift() more.\n\n\\(^{[1]}\\)I am not affiliated with GA-Tech beyond the new washing machine and jacuzzi they gave me to advertise their OMSCS program"
  },
  {
    "objectID": "posts/hello-word/index.html",
    "href": "posts/hello-word/index.html",
    "title": "The blog lives. Blog blog blog.",
    "section": "",
    "text": "A test of the new website, deployed on github pages. Errr, I guess this post can be about how I did it? Look, just follow this tutorial okay.\nHere’s some python code I guess?\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter([1,2], [1,2])\nfig.show()"
  },
  {
    "objectID": "posts/variational-autoencoder/index.html",
    "href": "posts/variational-autoencoder/index.html",
    "title": "Yet Another Explainer on Variational Autoencoders",
    "section": "",
    "text": "For a while, I’ve been struggling to understand variational autoencoders (VAE’s) at a satisfactory level. An initial pass produced a bit of understanding, but I got sucked back in when I tried to understand Dalle-E, which led me to try to understand diffusion models, which directed me back to going over the techniques used in variational autoencoders again. Some day I will write a Johnny-come-lately post about the different components of Dall-E, but today is about VAE’s.\nBefore I jump into things, my default disclaimer that I am probably not someone who should be writing authoritative-sounding articles about VAE’s - this is an exercise in understanding through explanation. Also, I think its just generally useful to have as many angles at explaining something available as possible – the sources I used to understand VAE’s had great diversity in the ways they explained recurring topics and how much to delve into certain pieces of the puzzle. Here are some resources that I have found useful and are probably more trustworthy (maybe mine is more humorous?):\n\nPaper by Kingma and Welling\\(^{[1]}\\)\nThis tutorial by some people at UC-Berkeley and Carnegie Mellon\\(^{[2]}\\)\nThis blog post\\(^{[3]}\\)\nThis other blog post\\(^{[4]}\\)\n\nMuch of the material here is going to be a re-hash of what has been presented in these. If you notice something wrong, please submit a pull request or open an issue in the git repo for this blog to correct my mistake.\n\n\nSetup\nAlright, so variational autoencoders are cool, they can produce synthetic samples (images) that are realistic and serve as great material for articles and Reddit posts about how ‘none of these faces are real!’ and how we will soon all live in an identity-less, AI-controlled dystopian nightmare.\n\n\nThis person doesn’t exist\n\nHow do VAE’s help us get there? One thing that [1] and [2] do is to initially put any sort of deep learning architectures aside and just focus on the general variational inference problem. At first I found this very annoying (get to the point!), but now think it is probably useful (yea yea, the people who are way smarter than me were right and I was wrong, who would have thought).\nThe setup is that we have some real observations, \\(X_{obs} = \\{x^{(1)}, x^{(2)}, ... x^{(N)}\\}\\) (When I talk about some arbitrary sample from the observed data, i’ll drop the superscript and just say \\(x\\)) that we assume are generated from some process that goes like:\n\nA random variable \\(z \\in \\mathcal{Z}\\) is drawn from some distribution \\(p(z\\vert\\psi)\\) with parameters \\(\\psi\\).\n\\(x^{(i)} \\in \\mathcal{X}\\) are generated through a conditional distribution \\(p(x\\vert z;\\theta)\\) with parameters \\(\\theta\\)\n\nThe \\(x^{(i)}\\) could be anything from single values to something very high dimensional like an image. We seek to maximize the likelihood of our data under the entire generative process:\n\\[p(x) = \\int p(x\\vert z; \\theta)p(z\\vert\\psi)dz\\]\nHowever there are some issues, the true values of \\(\\theta\\) and \\(\\psi\\) are not necessarily known to us. We also cannot assume that the posterior \\(p(z\\vert x;\\phi)\\) is tractable, which we will see is important later. Here we can note that we have the two pieces that correspond to the ‘encoder’ and ‘decoder’ pieces of the problem. \\(p(x\\vert z;\\theta)\\) is like a decoder, taking the hidden \\(z\\) and turning it into the observed \\(x\\). \\(p(z\\vert x;\\phi)\\) is like a decoder, taking some input \\(x\\) and produces a hidden representation \\(z\\) that was likely given that we observed that \\(x\\). A graph of the whole process including the ‘encoder’ and ‘decoder’ pieces in shown below.\n\n\n\n“Plate diagram” with solid arrows representing \\(p(x\\vert z; \\theta)\\) and dashed arrows representing \\(p(z\\vert x;\\phi)\\). From Kingma and Welling (2014)\n\n\nOk now I’ll try to make the jump to neural networks (you can stop scrolling). This was one of the hardest parts of understanding vae’s for me – it was hard to get used to the idea of networks that produced distributions. The notation of using \\(p(...)\\) to refer to both a distribution under the variational Bayes framework and a neural network that parametrizes that distribution can be a bit confusing, but try to get comfortable switching between the two views. The important part is that \\(p(x\\vert z;\\theta)\\) can be approximated by a neural network. We’ll see soon that \\(p(z\\vert x;\\phi)\\) can be as well, and I’ll explain why we need it to be.\n\nLets start with \\(p(x\\vert z;\\theta)\\). First some assumptions are made:\n\n\\(p(x\\vert z;\\theta)\\) comes from a distribution such that it can be approximated/parametrized by a differentiable function \\(f(z; \\theta)\\)\n\\(z\\) is drawn from some probability distribution, often an isotropic Gaussian \\(p(z) = N(0, I)\\)\n\nThe first assumption is simply so we can perform gradient descent given a sampled \\(z\\) and optimize the likelihood of \\(x\\) given that \\(z\\). Here is where we have our neural network that produces a distribution. One common way is for \\(f(z; \\theta)\\) to take the form of encoding the mean of an isotropic normal: \\[p(x\\vert z; \\theta) = \\mathcal{N}(X\\vert f(z;\\theta), \\sigma^2_x*I)\\] Ok, I usually have to stop here…how does this help us? Well, we now have a network that will output a distribution from which we can calculate a likelihood for any given \\(x\\), and it is differentiable, such that we can edit the parameters \\(\\theta\\) through gradient descent to maximize the likelihood of all \\(x \\in X_{obs}\\). Having this ‘decoder’ network represent a distribution is also necessary when fitting it into the objective function later. From now on when I refer to \\(p(x\\vert z; \\theta)\\), I’ll be simultaneously talking about the distribution, and the network that produces that distribution.\nI think it is worthwhile to consider what maximizing this likelihood looks like in a real example. Suppose the \\(x^{(i)}\\) are greyscale images, what should \\(f(z; \\theta)\\) output to maximize the likelihood of a given \\(x\\)? Intuitively (and mathematically) it should output a mean vector with each element corresponding to a pixel in \\(x\\) and having the same value as that pixel – that is, the multivariate Gaussian it outputs should be directly centered over the multivariate representation of the image. One can also consider other forms of \\(x\\) and output distributions that make sense, such as if \\(x\\) is from a multivariate bernoulli, and \\(f(z; \\theta)\\) would then output the probability vector \\(p\\), maximizing the likelihood by having elements of \\(p\\) as close to 1 as possible for corresponding 1’s in \\(x\\) (and close to zero for zeros).\nThe second assumption is a bit weirder, and took me a while to get comfortable with. Essentially it is very dubious to try to handcraft a distribution for \\(z\\) that represents some informative latent representation of the data. Better to let \\(f(z; \\theta)\\) sort out the random noise and construct informative features through gradient descent. Later, when we get to the encoder/objective, we’ll also see that having \\(z\\) be \\(N(0, I)\\) is convenient when computing a component of the objective.\n\nAt this point, we could go ahead and try to train our decoder to maximize the likelihood of our observed data. We can estimate \\(p(x)\\) by drawing a whole bunch of \\(z\\) and then computing \\(\\frac{1}{N}\\sum_{i=1}^{N} p(x\\vert z_i;\\theta)\\) for each \\(x^{(i)}\\) and maximizing \\(log(p(X_{obs})) = \\sum_i log(p(x^{(i)}))\\) through gradient descent. However as mentioned in [2] we may need an unreasonable number of samples from \\(z\\) to get a good estimate of any \\(p(x)\\). This seems to be a problem with the complexity of the observed data, and how reasonable it is that \\(x\\) arises from \\(N(0, I)\\) random noise being passed through a function approximator:\n\nUs: Hey! we need you to turn this random noise in \\(\\mathbb{R}^2\\) into more of like….a diagonal line.\nModel: Yea sure I can kinda learn to move stuff in the second and fourth quadrants to the first and third quadrants.\n…\nUs: Hi, good job on the diagonal thing, now we need you to make some pictures of numbers from the noise.\nModel: What, like….images? Of digits? Christ man thats really high dimensional, how do I even…I mean i’ll give it a try but this is sort of a stretch.\nUs: Go get em!\n…\nUs: Hi champ, back again. We need you to recreate the diversity of pictures of human faces from the same random noi…\nModel: *explodes*\n\nAnother intuition I have for why this does not work, is that how does \\(f(z; \\theta)\\) decide to what distributions to map certain regions of the latent space \\(\\mathcal{Z}\\)? In the example of generating images of digits, it has to balance maximizing the probability of all digits. However if there is a group of digits (say, all the twos) that are very different from the rest of the digits than the rest of the digits are from each other, then maximizing \\(p(X_{obs})\\) might involve simply not mapping any region of \\(\\mathcal{Z}\\) to a distribution that odd digit is likely under – not a model we want to end up with. It helps then to define, for a given \\(x\\), regions of \\(\\mathcal{Z}\\) that were likely given we observed that \\(x\\).\n\n\nMaking things easier with an encoder\nI mentioned we would need to consider the posterior \\(p(z\\vert x;\\phi)\\) – it directly addresses the problem I just brought up: that we need to know what \\(z\\) values are likely given we observed a particular \\(x\\). We now develop our ‘encoder’, which again will map \\(x\\) to distributions over \\(\\mathcal{Z}\\).\nOne reasonable question is why do we want to map to a distribution? Why don’t we just map to specific points in \\(\\mathcal{Z}\\) that are most likely to produce \\(x\\). Well, remember that we want to be able to generate examples by sampling from \\(\\mathcal{Z}\\), and it is highly unlikely that a regular old encoder will map \\(x\\)’s to \\(z\\)’s in such a way that is ‘smooth’ or ‘regular’ enough such that we can do so.\nConsider some \\(z^{(i)}\\) that is likely to have produced a \\(4\\). The encoder is under no constraint to make some other \\(z^{(j)}\\) that is very close to \\(z^{(i)}\\) also likely to produce a \\(4\\). Similarly, given two regions of \\(\\mathcal{Z}\\) that produce, say, \\(1\\)’s and \\(7\\)’s, as we move from one region to the other, the model is under no obligation to smoothly change from decoding to \\(1\\)’s to decoding to \\(7\\)’s. [4] explains this concept very well. Another way to think about it is that this does not really match our initial definition of the generative process of \\(z\\)’s randomly occurring and producing \\(x\\)’s from some conditional distribution \\(p(x\\vert z)\\). We could make it fit, but \\(p(z)\\) would be some very wacky distribution with completely unknown form. So, it makes sense to encode the \\(x\\) to well behaved distributions over \\(z\\) as well as consider penalizing the encoder for producing sets of these conditional distributions \\(p(z\\vert x)\\) which are far apart, so that we can sample the latent space and reasonably get a high-quality example of an \\(x\\). We’ll see how this is done in the next section.\nTo wrap up this section, \\(p(z\\vert x)\\) will be approximated in a very similar way as \\(p(x\\vert z;\\theta)\\) – it’s parameters are determined by a neural network that takes in \\(x\\) and outputs the parameters of a distribution, in this case, the mean and variance vectors of an isotropic normal distribution:\n\\[q(z\\vert x; \\phi) = \\mathcal{N}(Z \\vert \\mu(x;\\phi), \\mathbf{\\Sigma}(x;\\phi))\\]\nWhere \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) are the mean vector and covariance matrix for the isotropic normal distribution output by our encoder network with parameters \\(\\phi\\) when fed \\(x\\) as input (in practice the network just outputs the diagonal elements for \\(\\mathbf{\\Sigma}(x;\\phi))\\), since the off-diagonals are forced to be zero). When referring to \\(q(z\\vert x; \\phi)\\) I’ll be talking both about the distribution induced by the network that outputs \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) and the network itself.\n\n\nThe objective\nSo, we have an encoder \\(q(z\\vert x; \\phi)\\) and decoder \\(p(x\\vert z;\\theta)\\), now all that is left is to train them. So we have to find an objective that incorporates them, as well as satisfies our goal of maximizing \\(p(X_{obs})\\) under a generative process with a \\(z\\) we can reasonably sample from and produce realistic examples.\nThere is a lot of variation (man…I think I missed the pun on this one earlier) in tutorials about how they arrive at the loss function. I’m working backwards from the definition in Kingma and Welling (2014) for the reason that the starting point is well motivated: We want to maximize \\(p(x)\\) under the generative process; can we find a differentiable form related to \\(p(x)\\) that includes our encoder and decoder structures?\nAnother common approach starts with the motivation of trying to optimize \\(q(z\\vert x; \\phi)\\) to match the intractable posterior \\(p(z\\vert x)\\), but I found it hard to make the logical leaps as to how someone could reasonably start here. That way is maybe algebraically easier to get to what we want, but I like how starting with \\(p(x)\\) and rewriting it feels more intuitively motivated.\nThe way I’ll get our encoder and decoder into the definition of \\(p(x)\\) is by doing a bit of the ol add zero trick:\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(z \\vert x))] + log(p(x))\\]\nYes, I am just taking the expectation of zero in there. I’m taking the log of \\(p(x)\\) because we’ll need log-everything on the right side and maximizing \\(log(p(x))\\) will also maximize \\(p(x)\\). As a bonus, we have the form of our encoder in the equation, great! (GREAT!) Notice that the expectation is over \\(z\\)’s drawn from \\(q(z\\vert x;\\phi)\\), that is, to approximate this expectation we would sample from \\(q(z\\vert x;\\phi)\\) by providing an \\(x\\) and sampling from the conditional distribution. Ok, now we need to get the decoder in there. I’ll move the \\(log(p(x))\\) inside the expectation (legal since it doesn’t depend on \\(z\\)) and then rewrite using the equality \\(log(p(z\\vert x)p(x)) = log(p(x,z)) = log(p(x\\vert z)p(z))\\):\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(x \\vert z;\\theta)) + log(p(z))]\\]\nYay! Theres our decoder! (Dont doubt me, I really am that excited as I write this). Now what we can do is collapse things into Kullback-Leibler divergences - specifically any +/- pair of log probabilities dependent on \\(z\\) can be rewritten as a KL divergence:\n\\[\nlog(p(x)) = KL(q(z\\vert x;\\phi) \\vert\\vert p(z \\vert x)) - KL(q(z\\vert x;\\phi)\\vert\\vert p(z)) + E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\label{eq1}\\tag{eqn-1}\n\\]\nHrm, we have that pesky intractable \\(p(z \\vert x)\\) in there. Thankfully, we can focus on the other terms and simply rewrite/rearrange using the fact that KL-divergence is non-negative:\n\\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\label{eq2}\\tag{eqn-2}\n\\end{align}\n\\]\nThe right hand side is known as the evidence lower bound (ELBO), and various derivations of it and the above inequality can be found all over the internet if you found the above unsettling, disturbing, offensive.\nIt is useful to stare a bit at (\\(\\ref{eq1}\\)) and (\\(\\ref{eq2}\\)). First, if our \\(q(z\\vert x;\\phi)\\) eventually ends up matching \\(p(z \\vert x)\\), then the first KL divergence in (\\(\\ref{eq1}\\)) will be zero, and maximizing the RHS of (\\(\\ref{eq2}\\)) will be like directly maximizing \\(p(x)\\) [2].\nThe other notable thing is that (\\(\\ref{eq2}\\)) has a nice interpretation. \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\) can be though of as the reconstruction loss - how close are our reconstructions to the data. \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\) is like a regularization term telling our encoder: “You must try to be like the prior distribution \\(p(z)\\)”. This regularization term achieves the goal of making sure the conditional distributions across the \\(x\\) are nice and compact around the prior distribution - so if we sample from a \\(\\mathcal{N}(0,I)\\), we are likely to get a \\(z\\) that is likely to produce one of our \\(x\\)’s.\n\n\nMaximizing the objective through gradient descent\nSo, uhm…is (\\(\\ref{eq2}\\)) differentiable? Nope. Tutorial over, you’ve been had, AHAHAHAHA!\nJoking aside it actually isn’t currently amenable to backpropagation but we will get around that in a second. So remember we have two things we need to compute:\n\nThe regularization term \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\)\nThe reconstruction term \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\)\n\nFor 1, when the two distributions in the KL divergence are Gaussian, then there is a nice closed form solution that reduces nicely when, as in our case, the second distribution is \\(\\mathcal{N}(0, I)\\):\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = KL(\\mathcal{N}(Z \\vert \\mu(x;\\phi), \\Sigma(x;\\phi))\\vert\\vert \\mathcal{N}(0,I))\n\\] \\[\n= \\frac{1}{2}(tr(\\Sigma(x;\\phi)) + \\mu(x;\\phi)^T\\mu(x;\\phi) - k - log(det(\\Sigma(x;\\phi))))\n\\]\nWhere remember \\(\\mu(x;\\phi)\\) and \\(\\Sigma(x;\\phi)\\) are our mean vector and covariance matrix computed by our encoder. Nice, this is a value, it is differentiable with respect to the parameters \\(\\phi\\), great.\nFor 2, note that when we compute gradients, we move the gradient inside the expectation, and are just computing a gradient from a single example of \\(z\\), drawn from \\(q(z\\vert x;\\phi)\\) given our input \\(x\\) at train time (and over many gradient computations, we have well approximated maximizing the expectation over \\(z\\sim q(z\\vert x;\\phi)\\)). However we have a bit of a problem. Again notice the expectation is over \\(z\\) sampled from \\(q(z\\vert x;\\phi)\\), therefore maximizing the expression inside the expectation depends not only on updating \\(p(x \\vert z;\\theta)\\) to perform reconstruction well, but on updating \\(q(z\\vert x;\\phi)\\) to produce distributions that output \\(z\\)’s that \\(p(x \\vert z;\\theta)\\) finds easy to decode.\nOk, so no big deal? Just update both networks based on the reconstruction loss? This won’t immediately work because our training looks like:\n\nSend \\(x\\) through \\(q(z\\vert x;\\phi)\\) to get a distribution over \\(z\\)’s\nSample a \\(z\\) from that distribution\nSend \\(z\\) through \\(p(x \\vert z;\\theta)\\) to produce \\(x\\) and compute the reconstruction loss\n\nThe problem is that we cannot backpropagate through step 2 (‘sample from a distribution’ is not a math operation we can take the gradient of). The solution is what is known as the reparametrization trick. Instead of sampling directly from the distribution inferred by the output of our encoder, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and then multiply/add… :\n\\[\\epsilon*\\sqrt{\\Sigma(x;\\phi)} + \\mu(x;\\phi)\\]\n…to mimic sampling from the implied distribution. The difference being that in this case we are just adding and multiplying by constants, things which backpropogation is fine with (even though those constants were obtained from sampling).\nNow we can compute gradients for batches of \\(x\\) and average the gradients in the normal fashion to train the model."
  },
  {
    "objectID": "posts/ftx-blowup/index.html",
    "href": "posts/ftx-blowup/index.html",
    "title": "FTX Blowup and Massive Egos on Twitter",
    "section": "",
    "text": "The last 24 hours on crypto-Twitter have been a cacophony of rage, disbelief and whatever toxic stew of emotions exist in the hearts of crypto traders as the crypto exchange FTX has imploded after some FUD lead to actually-its-not-just-FUD-induced-selling/panic of the Chuck-E-Cheese tokens that made up the majority of assets on the balance sheet of the closely related trading desk Alameda Research. It was shortly thereafter that FTX itself was revealed to be insolvent, with a hole likely billions deep. More competent people than me will surely be releasing detailed postmortems in the coming days with the technicals of how this happened, though I’m almost sure it will boil down to some very, very brazen lies about what was being done with peoples money. Instead I’d like to talk (at bro-science level) about the pathology of the just spectacular lineup of characters who did totally legitimate business are are now totally not trying to evade authorities.\nI’ve written previously on my lightweight adventures in crypto, and though I’ve been out from a financial standpoint since ETH was $2000 and still had one more dance with $4000 left in it, I follow the various personalities in the crypto space, partly from a morbid interest in the strange specimens that inhabit the space, and partly from a lingering belief that there just might be something to crypto.\n\nSince I began to exit my crypto holdings, I’ve been a pretty big cynic about the space, however out of some (possibly misplaced) desire to maintain a nonzero level of optimism, I look for the honest, –or at least non-cringe– players in the space. The ones who will still be around if the meme-fueled casino ever decides to offer something of value.\nI initially found two candidates for groups of crypto people (sorry cryptographers, they are the crypto people now) that would let me proudly say I never completely lost faith in crypto once we are all paying our taxes in digital currency issued by His Excellency. These groups can be roughly categorized as the degens and the (self-proclaimed) builders.\nIn a result that reveals the true heart of crypto, the degens are the ones that still make me smile about crypto. They are lone-wolf traders, brash, pugnacious, but harmless; refreshingly self-aware and under few illusions (few, not none, as the FTX implosion has shown) about the game they are playing. It is just that to them – a game, and most seem to just be having fun and don’t wish anything bad on anyone. The most they might be blamed for is that they serve as an extra light that lures hapless retail traders into the electrified mesh. Finally, to their credit, I believe that until now they were not fooled by the latter group.\n\nThe second group, consisting of high-profile heads of exchanges, trading desks and other crypto ‘services’, never gave me the warm fuzzy feeling that the degens did, but boy did they try. Their Twitter accounts are (were) non-stop thread-presses, expounding their deep philosophical and technical wisdom. However the last year has exposed many of these characters (through the collapse of their projects, not just batshit tweets) as nothing more than reckless –though sufficiently intelligent– gamblers with massive egos.\n\n\n\nUntold wisdom awaits in their threads.\n\n\nEvery time one of them posted these insufferable threads clearly meant to keep their sense of self-importance fully inflated, I would try –as I did with the whole crypto space– to maintain a bit of belief that I might be being too cynical, that these people really were trying their best to build something good, but were possibly deluded about the fact that they were just in the ponzi-as-a-service industry. I said “hey if Sam Bankman-Fried is just transferring money from gamblers to animal-welfare causes (he wasn’t), then I guess I’m okay with that”.\nNever has my cynicism been more validated, and no doubt more validation is to come as the tide continues to recede. The more shocking thing is that these people, after they have been revealed as intellectual and legal frauds, continue to post Twitter threads about what they have learned and how they wish to impart their newfound wisdom as a result of the mistakes they’ve made rough times they’ve endured.\nSomething that keeps me reading their insane rants is a morbid desire to know whats going on in there. When, after incinerating people’s money, they try to explain their remorse and how they had the best intentions, what is their intent?\n\ngenuinely explain to people how things went bad, but that they didn’t want to hurt anyone\nThey have drunk their own Kool-Aid and are trying to prevent the air from rushing out of their inflated sense of self-worth.\nSetting up for the next con .\nMake tweets that sound good, hope that they appear in court as evidence, and that that might help them?\n\nI know its not healthy to spend too much time contemplating this, but you do sometimes just want to see the mask fall of and have them brokenly admit a-la Crime and Punishment that they are the murderer. The people such as His Excellency Justin Sun, though grotesque, at least have the advantage of being so outrageous that there is never any doubt that they are just pure grifters, obtaining their adrenaline rush from getting over on people and spitting in the face of fate.\nThe worst part is that the FTX blowup seems to have been the last straw for the degens. On Twitter, they seem exhausted, dejected, ready to call it quits. Their humorous banter is no longer there when I’m sipping coffee in the morning. Who now will bring a smile to my face about crypto?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Miscellaneous Rambling from Daniel Mabayog Claborne",
    "section": "",
    "text": "Dumb Mistakes Training Vision Transformer\n\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n  \n\n\n\n\nVectorize Your Sampling from a Categorical Distribution Using Gumbel-max! Use pandas.DataFrame.shift() more!\n\n\n\n\n\n\n\nData Science\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n  \n\n\n\n\nYet Another Explainer on Variational Autoencoders\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n  \n\n\n\n\nFTX Blowup and Massive Egos on Twitter\n\n\n\n\n\n\n\nnews\n\n\ncrypto\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe blog lives. Blog blog blog.\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "posts/hello-word/index.html",
    "href": "posts/hello-word/index.html",
    "title": "The blog lives. Blog blog blog.",
    "section": "",
    "text": "Here’s some python code I guess?\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.scatter([1,2], [1,2])\nfig.show()"
  },
  {
    "objectID": "posts/ftx-blowup/index.html",
    "href": "posts/ftx-blowup/index.html",
    "title": "FTX Blowup and Massive Egos on Twitter",
    "section": "",
    "text": "I’ve written previously on my lightweight adventures in crypto, and though I’ve been out from a financial standpoint since ETH was $2000 and still had one more dance with $4000 left in it, I follow the various personalities in the crypto space, partly from a morbid interest in the strange specimens that inhabit the space, and partly from a lingering belief that there just might be something to crypto.\n\nSince I began to exit my crypto holdings, I’ve been a pretty big cynic about the space, however out of some (possibly misplaced) desire to maintain a nonzero level of optimism, I look for the honest, –or at least non-cringe– players in the space. The ones who will still be around if the meme-fueled casino ever decides to offer something of value.\nI initially found two candidates for groups of crypto people (sorry cryptographers, they are the crypto people now) that would let me proudly say I never completely lost faith in crypto once we are all paying our taxes in digital currency issued by His Excellency. These groups can be roughly categorized as the degens and the (self-proclaimed) builders.\nIn a result that reveals the true heart of crypto, the degens are the ones that still make me smile about crypto. They are lone-wolf traders, brash, pugnacious, but harmless; refreshingly self-aware and under few illusions (few, not none, as the FTX implosion has shown) about the game they are playing. It is just that to them – a game, and most seem to just be having fun and don’t wish anything bad on anyone. The most they might be blamed for is that they serve as an extra light that lures hapless retail traders into the electrified mesh. Finally, to their credit, I believe that until now they were not fooled by the latter group.\n\nThe second group, consisting of high-profile heads of exchanges, trading desks and other crypto ‘services’, never gave me the warm fuzzy feeling that the degens did, but boy did they try. Their Twitter accounts are (were) non-stop thread-presses, expounding their deep philosophical and technical wisdom. However the last year has exposed many of these characters (through the collapse of their projects, not just batshit tweets) as nothing more than reckless –though sufficiently intelligent– gamblers with massive egos.\n\n\n\nUntold wisdom awaits in their threads.\n\n\nEvery time one of them posted these insufferable threads clearly meant to keep their sense of self-importance fully inflated, I would try –as I did with the whole crypto space– to maintain a bit of belief that I might be being too cynical, that these people really were trying their best to build something good, but were possibly deluded about the fact that they were just in the ponzi-as-a-service industry. I said “hey if Sam Bankman-Fried is just transferring money from gamblers to animal-welfare causes (he wasn’t), then I guess I’m okay with that”.\nNever has my cynicism been more validated, and no doubt more validation is to come as the tide continues to recede. The more shocking thing is that these people, after they have been revealed as intellectual and legal frauds, continue to post Twitter threads about what they have learned and how they wish to impart their newfound wisdom as a result of the mistakes they’ve made rough times they’ve endured.\nSomething that keeps me reading their insane rants is a morbid desire to know whats going on in there. When, after incinerating people’s money, they try to explain their remorse and how they had the best intentions, what is their intent?\n\ngenuinely explain to people how things went bad, but that they didn’t want to hurt anyone\nThey have drunk their own Kool-Aid and are trying to prevent the air from rushing out of their inflated sense of self-worth.\nSetting up for the next con .\nMake tweets that sound good, hope that they appear in court as evidence, and that that might help them?\n\nI know its not healthy to spend too much time contemplating this, but you do sometimes just want to see the mask fall of and have them brokenly admit a-la Crime and Punishment that they are the murderer. The people such as His Excellency Justin Sun, though grotesque, at least have the advantage of being so outrageous that there is never any doubt that they are just pure grifters, obtaining their adrenaline rush from getting over on people and spitting in the face of fate.\nThe worst part is that the FTX blowup seems to have been the last straw for the degens. On Twitter, they seem exhausted, dejected, ready to call it quits. Their humorous banter is no longer there when I’m sipping coffee in the morning. Who now will bring a smile to my face about crypto?"
  },
  {
    "objectID": "posts/variational-autoencoder/index.html",
    "href": "posts/variational-autoencoder/index.html",
    "title": "Yet Another Explainer on Variation Autoencoders",
    "section": "",
    "text": "Before I jump into things, my default disclaimer that I am probably not someone who should be writing authoritative-sounding articles about VAE’s - this is an exercise in understanding through explanation. Also, I think its just generally useful to have as many angles at explaining something available as possible – the sources I used to understand VAE’s had great diversity in the ways they explained recurring topics and how much to delve into certain pieces of the puzzle. Here are some resources that I have found useful and are probably more trustworthy (maybe mine is more humorous?):\n\nPaper by Kingma and Welling\\(^{[1]}\\)\nThis tutorial by some people at UC-Berkeley and Carnegie Mellon\\(^{[2]}\\)\nThis blog post\\(^{[3]}\\)\nThis other blog post\\(^{[4]}\\)\n\nMuch of the material here is going to be a re-hash of what has been presented in these. If you notice something wrong, please submit a pull request or open an issue in the git repo for this blog to correct my mistake.\n\n\nSetup\nAlright, so variational autoencoders are cool, they can produce synthetic samples (images) that are realistic and serve as great material for articles and Reddit posts about how ‘none of these faces are real!’ and how we will soon all live in an identity-less, AI-controlled dystopian nightmare.\n\n\nThis person doesn’t exist\n\nHow do VAE’s help us get there? One thing that [1] and [2] do is to initially put any sort of deep learning architectures aside and just focus on the general variational inference problem. At first I found this very annoying (get to the point!), but now think it is probably useful (yea yea, the people who are way smarter than me were right and I was wrong, who would have thought).\nThe setup is that we have some real observations, \\(X_{obs} = \\{x^{(1)}, x^{(2)}, ... x^{(N)}\\}\\) (When I talk about some arbitrary sample from the observed data, i’ll drop the superscript and just say \\(x\\)) that we assume are generated from some process that goes like:\n\nA random variable \\(z \\in \\mathcal{Z}\\) is drawn from some distribution \\(p(z\\vert\\psi)\\) with parameters \\(\\psi\\).\n\\(x^{(i)} \\in \\mathcal{X}\\) are generated through a conditional distribution \\(p(x\\vert z;\\theta)\\) with parameters \\(\\theta\\)\n\nThe \\(x^{(i)}\\) could be anything from single values to something very high dimensional like an image. We seek to maximize the likelihood of our data under the entire generative process:\n\\[p(x) = \\int p(x\\vert z; \\theta)p(z\\vert\\psi)dz\\]\nHowever there are some issues, the true values of \\(\\theta\\) and \\(\\psi\\) are not necessarily known to us. We also cannot assume that the posterior \\(p(z\\vert x;\\phi)\\) is tractable, which we will see is important later. Here we can note that we have the two pieces that correspond to the ‘encoder’ and ‘decoder’ pieces of the problem. \\(p(x\\vert z;\\theta)\\) is like a decoder, taking the hidden \\(z\\) and turning it into the observed \\(x\\). \\(p(z\\vert x;\\phi)\\) is like a decoder, taking some input \\(x\\) and produces a hidden representation \\(z\\) that was likely given that we observed that \\(x\\). A graph of the whole process including the ‘encoder’ and ‘decoder’ pieces in shown below.\n\n\n\n“Plate diagram” with solid arrows representing \\(p(x\\vert z; \\theta)\\) and dashed arrows representing \\(p(z\\vert x;\\phi)\\). From Kingma and Welling (2014)\n\n\nI think now is a good time to bring in the neural network piece of the equation (you can stop scrolling). This was one of the hardest parts of understanding vae’s for me – it was hard to get used to the idea of networks that produced distributions. \\(p(x\\vert z;\\theta)\\) can be approximated by a neural network. We’ll see soon that \\(p(z\\vert x;\\phi)\\) can be as well, and I’ll explain why we need it to be.\n\nLets start with \\(p(x\\vert z;\\theta)\\). First some assumptions are made:\n\n\\(p(x\\vert z;\\theta)\\) comes from a distribution such that it can be approximated/parametrized by a differentiable function \\(f(z; \\theta)\\)\n\\(z\\) is drawn from some probability distribution, often an isotropic Gaussian p(z) = \\(N(0, I)\\)\n\nThe first assumption is simply so we can perform gradient descent given a sampled \\(z\\) and optimize the likelihood of \\(x\\) given that \\(z\\). Here is where we have our neural network that produces a distribution. One common way is for \\(f(z; \\theta)\\) to take the form of encoding the mean of an isotropic normal: \\[p(x\\vert z; \\theta) = \\mathcal{N}(X\\vert f(z;\\theta), \\sigma^2_x*I)\\] Ok, I usually have to stop here…how does this help us? Well, we now have a network that will output a distribution from which we can calculate a likelihood for any given \\(x\\), and it is differentiable, such that we can edit the parameters \\(\\theta\\) through gradient descent to maximize the likelihood of all \\(x \\in X_{obs}\\). Having this ‘decoder’ network represent a distribution is also necessary when fitting it into the objective function later. From now on when I refer to \\(p(x\\vert z; \\theta)\\), I’ll be simultaneously talking about the distribution, and the network that produces that distribution.\nI think it is worthwhile to consider what maximizing this likelihood looks like in a real example. Suppose the \\(x^{(i)}\\) are greyscale images, what should \\(f(z; \\theta)\\) output to maximize the likelihood of a given \\(x\\)? Intuitively (and mathematically) it should output a mean vector with each element corresponding to a pixel in \\(x\\) and having the same value as that pixel – that is, the multivariate Gaussian it outputs should be directly centered over the multivariate representation of the image. One can also consider other forms of \\(x\\) and output distributions that make sense, such as if \\(x\\) is from a multivariate bernoulli, and \\(f(z; \\theta)\\) would then output the probability vector \\(p\\), maximizing the likelihood by having elements of \\(p\\) as close to 1 as possible for corresponding 1’s in \\(x\\) (and close to zero for zeros).\nThe second assumption is a bit weirder, and took me a while to get comfortable with. Essentially it is very dubious to try to handcraft a distribution for \\(z\\) that represents some informative latent representation of the data. Better to let \\(f(z; \\theta)\\) sort out the random noise and construct informative features through gradient descent. Later, when we get to the encoder/objective, we’ll also see that having \\(z\\) be \\(N(0, I)\\) is convenient when computing a component of the objective.\n\nAt this point, we could go ahead and try to train our decoder to maximize the likelihood of our observed data. We can estimate \\(p(x)\\) by drawing a whole bunch of \\(z\\) and then computing \\(\\frac{1}{N}\\sum_{i=1}^{N} p(x\\vert z_i;\\theta)\\) for each \\(x^{(i)}\\) and maximizing \\(log(p(X_{obs})) = \\sum_i log(p(x^{(i)}))\\) through gradient descent. However as mentioned in [2] we may need an unreasonable number of samples from \\(z\\) to get a good estimate of any \\(p(x)\\). This seems to be a problem with the complexity of the observed data, and how reasonable it is that \\(x\\) arises from \\(N(0, I)\\) random noise being passed through a function approximator:\n\nUs: Hey! we need you to turn this random noise in \\(\\mathbb{R}^2\\) into more of like….a diagonal line.\nModel: Yea sure I can kinda learn to move stuff in the second and fourth quadrants to the first and third quadrants.\n…\nUs: Hi, good job on the diagonal thing, now we need you to make some pictures of numbers from the noise.\nModel: What, like….images? Of digits? Christ man thats really high dimensional, how do I even…I mean i’ll give it a try but this is sort of a stretch.\nUs: Go get em!\n…\nUs: Hi champ, back again. We need you to recreate the diversity of pictures of human faces from the same random noi…\nModel: *explodes*\n\nAnother intuition I have for why this does not work, is that how does \\(f(z; \\theta)\\) decide to what distributions to map certain regions of the latent space \\(\\mathcal{Z}\\)? In the example of generating images of digits, it has to balance maximizing the probability of all digits. However if there is a group of digits (say, all the twos) that are very different from the rest of the digits than the rest of the digits are from each other, then maximizing \\(p(X_{obs})\\) might involve simply not mapping any region of \\(\\mathcal{Z}\\) to a distribution that odd digit is likely under – not a model we want to end up with. It helps then to define, for a given \\(x\\), regions of \\(\\mathcal{Z}\\) that were likely given we observed that \\(x\\).\n\n\nMaking things easier with an encoder\nI mentioned we would need to consider the posterior \\(p(z\\vert x;\\phi)\\) – it directly addresses the problem I just brought up: that we need to know what \\(z\\) values are likely given we observed a particular \\(x\\). We now develop our ‘encoder’, which again will map \\(x\\) to distributions over \\(\\mathcal{Z}\\).\nOne reasonable question is why do we want to map to a distribution? Why don’t we just map to specific points in \\(\\mathcal{Z}\\) that are most likely to produce \\(x\\). Well, remember that we want to be able to generate examples by sampling from \\(\\mathcal{Z}\\), and it is highly unlikely that a regular old encoder will map \\(x\\)’s to \\(z\\)’s in such a way that is ‘smooth’ or ‘regular’ enough such that we can do so.\nConsider some \\(z^{(i)}\\) that is likely to have produced a \\(4\\). The encoder is under no constraint to make some other \\(z^{(j)}\\) that is very close to \\(z^{(i)}\\) also likely to produce a \\(4\\). Similarly, given two regions of \\(\\mathcal{Z}\\) that produce, say, \\(1\\)’s and \\(7\\)’s, as we move from one region to the other, the model is under no obligation to smoothly change from decoding to \\(1\\)’s to decoding to \\(7\\)’s. [4] explains this concept very well. Another way to think about it is that this does not really match our initial definition of the generative process of \\(z\\)’s randomly occurring and producing \\(x\\)’s from some conditional distribution \\(p(x\\vert z)\\). We could make it fit, but \\(p(z)\\) would be some very wacky distribution with completely unknown form. So, it makes sense to encode the \\(x\\) to well behaved distributions over \\(z\\) as well as consider penalizing the encoder for producing sets of these conditional distributions \\(p(z\\vert x)\\) which are far apart, so that we can sample the latent space and reasonably get a high-quality example of an \\(x\\). We’ll see how this is done in the next section.\nTo wrap up this section, \\(p(z\\vert x)\\) will be approximated in a very similar way as \\(p(x\\vert z;\\theta)\\) – it’s parameters are determined by a neural network that takes in \\(x\\) and outputs the parameters of a distribution, in this case, the mean and variance vectors of an isotropic normal distribution:\n\\[q(z\\vert x; \\phi) = \\mathcal{N}(Z \\vert \\mu(x;\\phi), \\mathbf{\\Sigma}(x;\\phi))\\]\nWhere \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) are the mean vector and covariance matrix for the isotropic normal distribution output by our encoder network with parameters \\(\\phi\\) when fed \\(x\\) as input (in practice the network just outputs the diagonal elements for \\(\\mathbf{\\Sigma}(x;\\phi))\\), since the off-diagonals are forced to be zero). When referring to \\(q(z\\vert x; \\phi)\\) I’ll be talking both about the distribution induced by the network that outputs \\(\\mu(x;\\phi)\\) and \\(\\mathbf{\\Sigma}(x;\\phi)\\) and the network itself.\n\n\nThe objective\nSo, we have an encoder \\(q(z\\vert x; \\phi)\\) and decoder \\(p(x\\vert z;\\theta)\\), now all that is left is to train them. So we have to find an objective that incorporates them, as well as satisfies our goal of maximizing \\(p(X_{obs})\\) under a generative process with a \\(z\\) we can reasonably sample from and produce realistic examples.\nThere is a lot of variation (man…I think I missed the pun on this one earlier) in tutorials about how they arrive at the loss function. I’m working backwards from the definition in Kingma and Welling (2014) for the reason that the starting point is well motivated: We want to maximize \\(p(x)\\) under the generative process; can we find a differentiable form related to \\(p(x)\\) that includes our encoder and decoder structures?\nAnother common approach starts with the motivation of trying to optimize \\(q(z\\vert x; \\phi)\\) to match the intractable posterior \\(p(z\\vert x)\\), but I found it hard to make the logical leaps as to how someone could reasonably start here. That way is maybe algebraically easier to get to what we want, but I like how starting with \\(p(x)\\) and rewriting it feels more intuitively motivated.\nThe way I’ll get our encoder and decoder into the definition of \\(p(x)\\) is by doing a bit of the ol add zero trick:\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(z \\vert x))] + log(p(x))\\]\nYes, I am just taking the expectation of zero in there. I’m taking the log of \\(p(x)\\) because we’ll need log-everything on the right side and maximizing \\(log(p(x))\\) will also maximize \\(p(x)\\). As a bonus, we have the form of our encoder in the equation, great! (GREAT!) Notice that the expectation is over \\(z\\)’s drawn from \\(q(z\\vert x;\\phi)\\), that is, to approximate this expectation we would sample from \\(q(z\\vert x;\\phi)\\) by providing an \\(x\\) and sampling from the conditional distribution. Ok, now we need to get the decoder in there. I’ll move the \\(log(p(x))\\) inside the expectation (legal since it doesn’t depend on \\(z\\)) and then rewrite using the equality \\(log(p(z\\vert x)p(x)) = log(p(x,z)) = log(p(x\\vert z)p(z))\\):\n\\[log(p(x)) = E_{z\\sim q(z\\vert x;\\phi)}[log(q(z\\vert x;\\phi)) - log(p(z \\vert x)) - log(q(z\\vert x;\\phi)) + log(p(x \\vert z;\\theta)) + log(p(z))]\\]\nYay! Theres our decoder! (Dont doubt me, I really am that excited as I write this). Now what we can do is collapse things into Kullback-Leibler divergences - specifically any +/- pair of log probabilities dependent on \\(z\\) can be rewritten as a KL divergence:\n\\[\nlog(p(x)) = KL(q(z\\vert x;\\phi) \\vert\\vert p(z \\vert x)) - KL(q(z\\vert x;\\phi)\\vert\\vert p(z)) + E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\label{eq1}\\tag{eqn-1}\n\\]\nHrm, we have that pesky intractable \\(p(z \\vert x)\\) in there. Thankfully, we can focus on the other terms and simply rewrite/rearrange using the fact that KL-divergence is non-negative:\n\\[\n\\begin{align}\nlog(p(x)) \\geq E_{z\\sim q}[log(p(x \\vert z;\\theta))] - KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\label{eq2}\\tag{eqn-2}\n\\end{align}\n\\]\nThe right hand side is known as the evidence lower bound (ELBO), and various derivations of it and the above inequality can be found all over the internet if you found the above unsettling, disturbing, offensive.\nIt is useful to stare a bit at (\\(\\ref{eq1}\\)) and (\\(\\ref{eq2}\\)). First, if our \\(q(z\\vert x;\\phi)\\) eventually ends up matching \\(p(z \\vert x)\\), then the first KL divergence in (\\(\\ref{eq1}\\)) will be zero, and maximizing the RHS of (\\(\\ref{eq2}\\)) will be like directly maximizing \\(p(x)\\) [2].\nThe other notable thing is that (\\(\\ref{eq2}\\)) has a nice interpretation. \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\) can be though of as the reconstruction loss - how close are our reconstructions to the data. \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\) is like a regularization term telling our encoder: “You must try to be like the prior distribution \\(p(z)\\)”. This regularization term achieves the goal of making sure the conditional distributions across the \\(x\\) are nice and compact around the prior distribution - so if we sample from a \\(\\mathcal{N}(0,I)\\), we are likely to get a \\(z\\) that is likely to produce one of our \\(x\\)’s.\n\n\nMaximizing the objective through gradient descent\nSo, uhm…is (\\(\\ref{eq2}\\)) differentiable? Nope. Tutorial over, you’ve been had, AHAHAHAHA!\nJoking aside it actually isn’t currently amenable to backpropagation but we will get around that in a second. So remember we have two things we need to compute:\n\nThe regularization term \\(KL(q(z\\vert x;\\phi)\\vert\\vert p(z))\\)\nThe reconstruction term \\(E_{z\\sim q(z\\vert x;\\phi)}[log(p(x \\vert z;\\theta))]\\)\n\nFor 1, when the two distributions in the KL divergence are Gaussian, then there is a nice closed form solution that reduces nicely when, as in our case, the second distribution is \\(\\mathcal{N}(0, I)\\):\n\\[\nKL(q(z\\vert x;\\phi)\\vert\\vert p(z)) = KL(\\mathcal{N}(Z \\vert \\mu(x;\\phi), \\Sigma(x;\\phi))\\vert\\vert \\mathcal{N}(0,I))\n\\] \\[\n= \\frac{1}{2}(tr(\\Sigma(x;\\phi)) + \\mu(x;\\phi)^T\\mu(x;\\phi) - k - log(det(\\Sigma(x;\\phi))))\n\\]\nWhere remember \\(\\mu(x;\\phi)\\) and \\(\\Sigma(x;\\phi)\\) are our mean vector and covariance matrix computed by our encoder. Nice, this is a value, it is differentiable with respect to the parameters \\(\\phi\\), great.\nFor 2, note that when we compute gradients, we move the gradient inside the expectation, and are just computing a gradient from a single example of \\(z\\), drawn from \\(q(z\\vert x;\\phi)\\) given our input \\(x\\) at train time (and over many gradient computations, we have well approximated maximizing the expectation over \\(z\\sim q(z\\vert x;\\phi)\\)). However we have a bit of a problem. Again notice the expectation is over \\(z\\) sampled from \\(q(z\\vert x;\\phi)\\), therefore maximizing the expression inside the expectation depends not only on updating \\(p(x \\vert z;\\theta)\\) to perform reconstruction well, but on updating \\(q(z\\vert x;\\phi)\\) to produce distributions that output \\(z\\)’s that \\(p(x \\vert z;\\theta)\\) finds easy to decode.\nOk, so no big deal? Just update both networks based on the reconstruction loss? This won’t immediately work because our training looks like:\n\nSend \\(x\\) through \\(q(z\\vert x;\\phi)\\) to get a distribution over \\(z\\)’s\nSample a \\(z\\) from that distribution\nSend \\(z\\) through \\(p(x \\vert z;\\theta)\\) to produce \\(x\\) and compute the reconstruction loss\n\nThe problem is that we cannot backpropagate through step 2 (‘sample from a distribution’ is not a math operation we can take the gradient of). The solution is what is known as the reparametrization trick. Instead of sampling directly from the distribution inferred by the output of our encoder, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and then multiply/add… :\n\\[\\epsilon*\\sqrt{\\Sigma(x;\\phi)} + \\mu(x;\\phi)\\]\n…to mimic sampling from the implied distribution. The difference being that in this case we are just adding and multiplying by constants, things which backpropogation is fine with (even though those constants were obtained from sampling).\nNow we can compute gradients for batches of \\(x\\) and average the gradients in the normal fashion to train the model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Miscellaneous Rambling from Daniel Mabayog Claborne",
    "section": "",
    "text": "Nov 12, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncrypto\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\nDaniel Claborne\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Another blog? Why sir?",
    "section": "",
    "text": "__DISCLAIMER__ This will appear throughout the blog - I am not an expert in anything, I will try to cauch everything in uncertainty. Use any code or follow any conclusions I post at your own risk."
  }
]